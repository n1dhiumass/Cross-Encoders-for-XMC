20 Sep 2021

Running inference with a trained bi-encoder model on Amazon-131K dataset

	On a small label set
		
		python eval/eval.py --model_config_file ../../results/1_Bi_Enc/debug_eval/m\=bi_enc_l\=hinge_s\=1234_in_batch_negs/model/best_wrt_dev/wrapper_config.json --test_file ../../data/1_LF-Amazon-131K/debug_raw/trn.json --lbl_file ../../data/1_LF-Amazon-131K/debug_raw/lbl_100.json --out_dir ../../results/1_Bi_Enc/debug_eval/m\=bi_enc_l\=hinge_s\=1234_in_batch_negs/eval

	On actual test set

		python eval/eval.py --model_config_file ../../results/1_Bi_Enc/debug_eval/m\=bi_enc_l\=hinge_s\=1234_in_batch_negs/model/best_wrt_dev/wrapper_config.json --test_file ../../data/1_LF-Amazon-131K/raw/tst.json --lbl_file ../../data/1_LF-Amazon-131K/raw/lbl.json --out_dir ../../results/1_Bi_Enc/debug_eval/m\=bi_enc_l\=hinge_s\=1234_in_batch_negs/eval

Dec 
	15 Dec

	    Running BiEncoder Training on ZeShel data with hard negative mining

		    sbatch --partition titanx-long --gres gpu:4 --mem 128GB --job-name el_zeshel_bi_dp_4_bs_128   bin/run.sh python models/train.py --config config/el_zeshel_bi_enc.json --data_parallel=1 --misc hard_negs_w_dp_bs_128 --eval_interval 200 --train_batch_size 128 --neg_strategy hard_negs

			sbatch --partition titanx-long --gres gpu:4 --mem 128GB --job-name el_zeshel_bi_dp_4_bs_32   bin/run.sh python models/train.py --config config/el_zeshel_bi_enc.json --data_parallel=1 --misc hard_negs_w_dp_bs_32 --eval_interval 200 --train_batch_size 32 --neg_strategy hard_negs

			(Batch-size  = 128 gave OOM error even on RTX8000)

			sbatch --partition rtx8000-long --gres gpu:1 --mem 128GB --job-name el_zeshel_bi_dp_1   bin/run.sh python models/train.py --config config/el_zeshel_bi_enc.json --data_parallel= --misc random_negs_wo_dp_bs_32 --eval_interval 200 --train_batch_size 32

			sbatch --partition rtx8000-long --gres gpu:1 --mem 128GB --job-name el_zeshel_bi_dp_1   bin/run.sh python models/train.py --config config/el_zeshel_bi_enc.json --data_parallel= --misc hard_negs_wo_dp_bs_32 --eval_interval 200 --train_batch_size 32 --neg_strategy hard_negs

			

		Running Cross-Encoder Training

			python models/train.py --config config/el_zeshel_cross_enc_debug.json --data_parallel= --misc hard_negs_wo_dp_bs_32 --eval_interval 200 --train_batch_size 32 --neg_strategy hard_negs

			python models/train.py --config config/el_zeshel_cross_enc_debug.json --data_parallel= --misc hard_negs_wo_dp_bs_32 --eval_interval 200 --train_batch_size 8 --neg_strategy hard_neg

			sbatch --partition rtx8000-long --gres gpu:1 --mem 128GB --job-name el_zeshel_cross  bin/run.sh python models/train.py --config config/el_zeshel_cross_enc.json --data_parallel= --misc hard_negs_wo_dp_bs_32 --eval_interval 200 --train_batch_size 32 --neg_strategy hard_negs
			
			sbatch --partition rtx8000-long --gres gpu:1 --mem 128GB --job-name el_zeshel_cross  bin/run.sh python models/train.py --config config/el_zeshel_cross_enc.json --data_parallel= --misc hard_negs_wo_dp_bs_32_w_hard_bienc --eval_interval 200 --train_batch_size 32 --neg_strategy hard_negs --neg_mine_bienc_model_config ../../results/4_Zeshel/d=ent_link/m=bi_enc_l=ce_s=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json


	17 Dec
		
		Eval biencoder model trained on Zeshel with hard negatives
			sbatch -p gpu --gres gpu:1 --mem 64GB --exclude gpu-0-0 bin/run.sh python eval/run_biencoder_eval_zeshel.py --n_ment -1 --top_k 100 --batch_size 100 --res_dir ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/eval --misc epoch_9 --model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json --data_name american_football
			sbatch -p gpu --gres gpu:1 --mem 64GB --exclude gpu-0-0 bin/run.sh python eval/run_biencoder_eval_zeshel.py --n_ment -1 --top_k 100 --batch_size 100 --res_dir ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/eval --misc epoch_9 --model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json --data_name doctor_who
			sbatch -p gpu --gres gpu:1 --mem 64GB --exclude gpu-0-0 bin/run.sh python eval/run_biencoder_eval_zeshel.py --n_ment -1 --top_k 100 --batch_size 100 --res_dir ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/eval --misc epoch_9 --model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json --data_name fallout
			sbatch -p gpu --gres gpu:1 --mem 64GB --exclude gpu-0-0 bin/run.sh python eval/run_biencoder_eval_zeshel.py --n_ment -1 --top_k 100 --batch_size 100 --res_dir ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/eval --misc epoch_9 --model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json --data_name final_fantasy
			sbatch -p gpu --gres gpu:1 --mem 64GB --exclude gpu-0-0 bin/run.sh python eval/run_biencoder_eval_zeshel.py --n_ment -1 --top_k 100 --batch_size 100 --res_dir ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/eval --misc epoch_9 --model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json --data_name military
			sbatch -p gpu --gres gpu:1 --mem 64GB --exclude gpu-0-0 bin/run.sh python eval/run_biencoder_eval_zeshel.py --n_ment -1 --top_k 100 --batch_size 100 --res_dir ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/eval --misc epoch_9 --model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json --data_name pro_wrestling
			sbatch -p gpu --gres gpu:1 --mem 64GB --exclude gpu-0-0 bin/run.sh python eval/run_biencoder_eval_zeshel.py --n_ment -1 --top_k 100 --batch_size 100 --res_dir ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/eval --misc epoch_9 --model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json --data_name starwars
			sbatch -p gpu --gres gpu:1 --mem 64GB --exclude gpu-0-0 bin/run.sh python eval/run_biencoder_eval_zeshel.py --n_ment -1 --top_k 100 --batch_size 100 --res_dir ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/eval --misc epoch_9 --model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json --data_name world_of_warcraft
			sbatch -p gpu --gres gpu:1 --mem 64GB --exclude gpu-0-0 bin/run.sh python eval/run_biencoder_eval_zeshel.py --n_ment -1 --top_k 100 --batch_size 100 --res_dir ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/eval --misc epoch_9 --model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json --data_name forgotten_realms
			sbatch -p gpu --gres gpu:1 --mem 64GB --exclude gpu-0-0 bin/run.sh python eval/run_biencoder_eval_zeshel.py --n_ment -1 --top_k 100 --batch_size 100 --res_dir ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/eval --misc epoch_9 --model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json --data_name lego
			sbatch -p gpu --gres gpu:1 --mem 64GB --exclude gpu-0-0 bin/run.sh python eval/run_biencoder_eval_zeshel.py --n_ment -1 --top_k 100 --batch_size 100 --res_dir ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/eval --misc epoch_9 --model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json --data_name star_trek
			sbatch -p gpu --gres gpu:1 --mem 64GB --exclude gpu-0-0 bin/run.sh python eval/run_biencoder_eval_zeshel.py --n_ment -1 --top_k 100 --batch_size 100 --res_dir ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/eval --misc epoch_9 --model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json --data_name yugioh
			sbatch -p gpu --gres gpu:1 --mem 64GB --exclude gpu-0-0 bin/run.sh python eval/run_biencoder_eval_zeshel.py --n_ment -1 --top_k 100 --batch_size 100 --res_dir ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/eval --misc epoch_9 --model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json --data_name coronation_street
			sbatch -p gpu --gres gpu:1 --mem 64GB --exclude gpu-0-0 bin/run.sh python eval/run_biencoder_eval_zeshel.py --n_ment -1 --top_k 100 --batch_size 100 --res_dir ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/eval --misc epoch_9 --model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json --data_name elder_scrolls
			sbatch -p gpu --gres gpu:1 --mem 64GB --exclude gpu-0-0 bin/run.sh python eval/run_biencoder_eval_zeshel.py --n_ment -1 --top_k 100 --batch_size 100 --res_dir ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/eval --misc epoch_9 --model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json --data_name ice_hockey
			sbatch -p gpu --gres gpu:1 --mem 64GB --exclude gpu-0-0 bin/run.sh python eval/run_biencoder_eval_zeshel.py --n_ment -1 --top_k 100 --batch_size 100 --res_dir ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/eval --misc epoch_9 --model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json --data_name muppets

		Eval biencoder model trained on Zeshel with random negatives
			sbatch -p gpu --gres gpu:1 --mem 64GB --exclude gpu-0-0 bin/run.sh python eval/run_biencoder_eval_zeshel.py --n_ment -1 --top_k 100 --batch_size 100 --res_dir ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_random_negs_wo_dp_bs_32/eval --misc epoch_9 --model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_random_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json --data_name american_football
			sbatch -p gpu --gres gpu:1 --mem 64GB --exclude gpu-0-0 bin/run.sh python eval/run_biencoder_eval_zeshel.py --n_ment -1 --top_k 100 --batch_size 100 --res_dir ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_random_negs_wo_dp_bs_32/eval --misc epoch_9 --model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_random_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json --data_name doctor_who
			sbatch -p gpu --gres gpu:1 --mem 64GB --exclude gpu-0-0 bin/run.sh python eval/run_biencoder_eval_zeshel.py --n_ment -1 --top_k 100 --batch_size 100 --res_dir ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_random_negs_wo_dp_bs_32/eval --misc epoch_9 --model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_random_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json --data_name fallout
			sbatch -p gpu --gres gpu:1 --mem 64GB --exclude gpu-0-0 bin/run.sh python eval/run_biencoder_eval_zeshel.py --n_ment -1 --top_k 100 --batch_size 100 --res_dir ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_random_negs_wo_dp_bs_32/eval --misc epoch_9 --model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_random_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json --data_name final_fantasy
			sbatch -p gpu --gres gpu:1 --mem 64GB --exclude gpu-0-0 bin/run.sh python eval/run_biencoder_eval_zeshel.py --n_ment -1 --top_k 100 --batch_size 100 --res_dir ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_random_negs_wo_dp_bs_32/eval --misc epoch_9 --model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_random_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json --data_name military
			sbatch -p gpu --gres gpu:1 --mem 64GB --exclude gpu-0-0 bin/run.sh python eval/run_biencoder_eval_zeshel.py --n_ment -1 --top_k 100 --batch_size 100 --res_dir ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_random_negs_wo_dp_bs_32/eval --misc epoch_9 --model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_random_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json --data_name pro_wrestling
			sbatch -p gpu --gres gpu:1 --mem 64GB --exclude gpu-0-0 bin/run.sh python eval/run_biencoder_eval_zeshel.py --n_ment -1 --top_k 100 --batch_size 100 --res_dir ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_random_negs_wo_dp_bs_32/eval --misc epoch_9 --model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_random_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json --data_name starwars
			sbatch -p gpu --gres gpu:1 --mem 64GB --exclude gpu-0-0 bin/run.sh python eval/run_biencoder_eval_zeshel.py --n_ment -1 --top_k 100 --batch_size 100 --res_dir ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_random_negs_wo_dp_bs_32/eval --misc epoch_9 --model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_random_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json --data_name world_of_warcraft
			sbatch -p gpu --gres gpu:1 --mem 64GB --exclude gpu-0-0 bin/run.sh python eval/run_biencoder_eval_zeshel.py --n_ment -1 --top_k 100 --batch_size 100 --res_dir ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_random_negs_wo_dp_bs_32/eval --misc epoch_9 --model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_random_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json --data_name forgotten_realms
			sbatch -p gpu --gres gpu:1 --mem 64GB --exclude gpu-0-0 bin/run.sh python eval/run_biencoder_eval_zeshel.py --n_ment -1 --top_k 100 --batch_size 100 --res_dir ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_random_negs_wo_dp_bs_32/eval --misc epoch_9 --model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_random_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json --data_name lego
			sbatch -p gpu --gres gpu:1 --mem 64GB --exclude gpu-0-0 bin/run.sh python eval/run_biencoder_eval_zeshel.py --n_ment -1 --top_k 100 --batch_size 100 --res_dir ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_random_negs_wo_dp_bs_32/eval --misc epoch_9 --model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_random_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json --data_name star_trek
			sbatch -p gpu --gres gpu:1 --mem 64GB --exclude gpu-0-0 bin/run.sh python eval/run_biencoder_eval_zeshel.py --n_ment -1 --top_k 100 --batch_size 100 --res_dir ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_random_negs_wo_dp_bs_32/eval --misc epoch_9 --model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_random_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json --data_name yugioh
			sbatch -p gpu --gres gpu:1 --mem 64GB --exclude gpu-0-0 bin/run.sh python eval/run_biencoder_eval_zeshel.py --n_ment -1 --top_k 100 --batch_size 100 --res_dir ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_random_negs_wo_dp_bs_32/eval --misc epoch_9 --model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_random_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json --data_name coronation_street
			sbatch -p gpu --gres gpu:1 --mem 64GB --exclude gpu-0-0 bin/run.sh python eval/run_biencoder_eval_zeshel.py --n_ment -1 --top_k 100 --batch_size 100 --res_dir ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_random_negs_wo_dp_bs_32/eval --misc epoch_9 --model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_random_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json --data_name elder_scrolls
			sbatch -p gpu --gres gpu:1 --mem 64GB --exclude gpu-0-0 bin/run.sh python eval/run_biencoder_eval_zeshel.py --n_ment -1 --top_k 100 --batch_size 100 --res_dir ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_random_negs_wo_dp_bs_32/eval --misc epoch_9 --model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_random_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json --data_name ice_hockey
			sbatch -p gpu --gres gpu:1 --mem 64GB --exclude gpu-0-0 bin/run.sh python eval/run_biencoder_eval_zeshel.py --n_ment -1 --top_k 100 --batch_size 100 --res_dir ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_random_negs_wo_dp_bs_32/eval --misc epoch_9 --model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_random_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json --data_name muppets


	31 Dec

		Eval crossencoder model trained with hard negatives from biencoder trained with random negatives

			sbatch -p gpu --gres gpu:1 --mem 64GB --exclude gpu-0-0 bin/run.sh python eval/run_cross_encoder_w_binenc_retriever_zeshel.py --data_name $data --n_ment -1 --top_k 100 --batch_size 100 --res_dir ../../results/4_Zeshel/d\=ent_link/m\=cross_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/eval --bi_model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json  --cross_model_config ../../results/4_Zeshel/d\=ent_link/m\=cross_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev/wrapper_config.json 


Jan 2022
	1 Jan 22
		
		Train cross-encoder model with larger negatives = 64 to match state of the art models

			sbatch --partition rtx8000-long --gres gpu:1 --mem 64GB --job-name el_zeshel_cross_63  bin/run.sh python models/train.py --config config/el_zeshel_cross_enc.json --data_parallel= --misc hard_negs_63_bs_8_w_hard_bienc --eval_interval 200 --train_batch_size 8 --neg_strategy hard_negs --neg_mine_bienc_model_config ../../results/4_Zeshel/d=ent_link/m=bi_enc_l=ce_s=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json --num_negs 63


		Computing entire mention-entity score matrix for ZeShel using latest trained cross-encoder model

			sbatch --partition titanx-long --gres gpu:1 --mem 64GB --job-name m2e_$data   bin/run.sh python utils/run_cross_encoder_for_ment_ent_matrix_zeshel.py --cross_model_config ../../results/4_Zeshel/d\=ent_link/m=cross_enc_l=ce_s=1234_hard_negs_wo_dp_bs_16_w_hard_bienc/model/best_wrt_dev/wrapper_config.json --res_dir ../../results/4_Zeshel/d\=ent_link/m=cross_enc_l=ce_s=1234_hard_negs_wo_dp_bs_16_w_hard_bienc/score_mats --n_ment 100 --n_ent -1 --batch_size 100 --data_name $data

			
	2 Jan 2022

		Comparing hard negatives wrt cross-encoder and biencoder model

			python eval/compare_biencoder_vs_crossencoder_zeshel.py --top_k 1000 --bi_model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json --res_dir ../../results/4_Zeshel/d\=ent_link/m\=cross_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_16_w_hard_bienc/score_mats --data_name lego


		Comparing exact cross-encoder and nsw crossencoder model

			python eval/nsw_eval_zeshel.py --embed_type tfidf --bi_model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json --res_dir ../../results/4_Zeshel/d\=ent_link/m\=cross_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_16_w_hard_bienc/score_mats_wrt_final_model --data_name american_football

			python eval/nsw_eval_zeshel.py --embed_type tfidf --bi_model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json --res_dir ../../results/4_Zeshel/d\=ent_link/m\=cross_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_16_w_hard_bienc/score_mats_wrt_final_model --data_name lego

			python eval/nsw_eval_zeshel.py --embed_type bienc --bi_model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json --res_dir ../../results/4_Zeshel/d\=ent_link/m\=cross_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_16_w_hard_bienc/score_mats_wrt_final_model --data_name lego


	5 Jan 2022
		
		Train cross-encoder model with larger negatives = 100

			sbatch -p gpu -G 1 --mem 64GB --job-name el_100 bin/run.sh python models/train.py --config config/el_zeshel_cross_enc.json --data_parallel= --misc hard_negs_100_bs_1_w_hard_bienc --eval_interval 200 --train_batch_size 1 --grad_acc_steps 1 --neg_strategy hard_negs --neg_mine_bienc_model_config ../../results/4_Zeshel/d=ent_link/m=bi_enc_l=ce_s=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json --num_negs 100 
			sbatch -p gpu -G 1 --mem 64GB --job-name el_100_10_8 bin/run.sh python models/train.py --config config/el_zeshel_cross_enc.json --data_parallel= --misc hard_negs_10_of_100_bs_1_w_hard_bienc --eval_interval 200 --train_batch_size 1 --grad_acc_steps 1 --neg_strategy hard_negs --neg_mine_bienc_model_config ../../results/4_Zeshel/d=ent_link/m=bi_enc_l=ce_s=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json --num_negs 100 --loss_type topk_ce --topk_ce 10

			sbatch -p rtx8000-long --gres gpu:1 --mem 64GB --job-name el_63_10_8 bin/run.sh python models/train.py --config config/el_zeshel_cross_enc.json --data_parallel= --misc hard_negs_10_of_63_bs_8_w_hard_bienc --eval_interval 200 --train_batch_size 8 --neg_strategy hard_negs --neg_mine_bienc_model_config ../../results/4_Zeshel/d=ent_link/m=bi_enc_l=ce_s=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json --num_negs 63 --loss_type topk_ce --topk_ce 10

			sbatch -p rtx8000-long --gres gpu:1 --mem 64GB --job-name el_100 bin/run.sh python models/train.py --config config/el_zeshel_cross_enc.json --data_parallel= --misc hard_negs_100_bs_4_w_hard_bienc --eval_interval 200 --train_batch_size 4 --neg_strategy hard_negs --neg_mine_bienc_model_config ../../results/4_Zeshel/d=ent_link/m=bi_enc_l=ce_s=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json --num_negs 100 --eval_batch_size 32


	6 Jan 2022

		Run gradient based inference

			python utils/run_gradient_based_search_w_cross_enc.py --quant_method bienc --lr 0.1 --n_ment 20 --bi_model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json --cross_model_config ../../results/4_Zeshel/d\=ent_link/m\=cross_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_16_w_hard_bienc/model/best_wrt_dev/wrapper_config.json --data_name lego

			 e

		Training an XMC Model again

	8 Jan 2022
		
		Run score matrix compuatation for each layer
			sbatch --partition gpu --gres gpu:1 --mem 64GB --job-name 1K_m2e_$data  --exclude gpu-0-0 bin/run.sh python utils/run_cross_encoder_for_ment_ent_matrix_zeshel.py --cross_model_config ../../results/4_Zeshel/d=ent_link/m=cross_enc_l=ce_s=1234_hard_negs_wo_dp_bs_16_w_hard_bienc/model/best_wrt_dev/wrapper_config.json --res_dir ../../results/4_Zeshel/d=ent_link/m=cross_enc_l=ce_s=1234_hard_negs_wo_dp_bs_16_w_hard_bienc/score_mats --n_ment 100 --n_ent -1 --batch_size 800 --data_name $data --layers final

	    	sbatch --partition gpu --gres gpu:1 --mem 64GB --job-name 1K_m2e_$data  --exclude gpu-0-0 bin/run.sh python utils/run_cross_encoder_for_ment_ent_matrix_zeshel.py --cross_model_config ../../results/4_Zeshel/d=ent_link/m=cross_enc_l=ce_s=1234_hard_negs_wo_dp_bs_16_w_hard_bienc/model/curr_epoch/wrapper_config.json --res_dir ../../results/4_Zeshel/d=ent_link/m=cross_enc_l=ce_s=1234_hard_negs_wo_dp_bs_16_w_hard_bienc/score_mats_wrt_final_model --n_ment 100 --n_ent -1 --batch_size 800 --data_name $data --layers final


	9 Jan 2022
		
		Comparing scores from different crossencoder layer


			python eval/explore_crossencoder_working.py --data_name yugioh --cross_model_config ../../results/4_Zeshel/d\=ent_link/m\=cross_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_16_w_hard_bienc/model/best_wrt_dev/wrapper_config.json --score_file ../../results/4_Zeshel/d\=ent_link/m\=cross_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_16_w_hard_bienc/score_mats_wrt_final_model/yugioh/ment_to_ent_scores_n_m_7_n_e_5_all_layers_Truedebug_layer_wise_score.pkl 


	10 jan 
		
		Re-runnning some jobs to compute cost-vs-performance tradeoffs for cross-encoder model

			fallout
			python eval/nsw_eval_zeshel.py --embed_type bienc --bi_model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json --res_dir ../../results/4_Zeshel/d\=ent_link/m\=cross_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_16_w_hard_bienc/score_mats --data_name american_football

			python eval/nsw_eval_zeshel.py --embed_type bienc --bi_model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json --res_dir ../../results/4_Zeshel/d\=ent_link/m\=cross_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_16_w_hard_bienc/score_mats --data_name fallout


	11 Jan
		
		Eval top-100 and top-10_out_of_top-63 negative crossencoder model

			for data in forgotten_realms lego star_trek yugioh american_football doctor_who fallout final_fantasy military pro_wrestling starwars world_of_warcraft coronation_street elder_scrolls ice_hockey muppets


			sbatch -p gpu --gres gpu:1 --mem 64GB --exclude gpu-0-0 bin/run.sh python eval/run_cross_encoder_w_binenc_retriever_zeshel.py --data_name $data --n_ment -1 --top_k 100 --batch_size 1 --bi_model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json --res_dir ../../results/4_Zeshel/d\=ent_link/m=cross_enc_l=ce_s=1234_hard_negs_100_bs_4_w_hard_bienc/eval  --cross_model_config ../../results/4_Zeshel/d\=ent_link/m=cross_enc_l=ce_s=1234_hard_negs_100_bs_4_w_hard_bienc/model/best_wrt_dev/wrapper_config.json


			sbatch -p gpu --gres gpu:1 --mem 64GB --exclude gpu-0-0 bin/run.sh python eval/run_cross_encoder_w_binenc_retriever_zeshel.py --data_name $data --n_ment -1 --top_k 100 --batch_size 1 --bi_model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json --res_dir ../../results/4_Zeshel/d\=ent_link/m=cross_enc_l=topk_ce_s=1234_hard_negs_10_of_63_bs_8_w_hard_bienc/eval  --cross_model_config ../../results/4_Zeshel/d\=ent_link/m=cross_enc_l=topk_ce_s=1234_hard_negs_10_of_63_bs_8_w_hard_bienc/model/best_wrt_dev/wrapper_config.json
	    

	    	sbatch -p gpu --gres gpu:1 --mem 64GB --exclude gpu-0-0 bin/run.sh python eval/run_cross_encoder_w_binenc_retriever_zeshel.py --data_name $data --n_ment -1 --top_k 100 --batch_size 1 --bi_model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json --res_dir ../../results/4_Zeshel/d\=ent_link/m\=cross_enc_l\=ce_s\=1234_hard_negs_63_bs_8_w_hard_bienc/eval --cross_model_config ../../results/4_Zeshel/d\=ent_link/m\=cross_enc_l\=ce_s\=1234_hard_negs_63_bs_8_w_hard_bienc/model/best_wrt_dev/wrapper_config.json



	    Train cross-encoder model on smaller train datasets

	    	sbatch -p rtx8000-long --gres gpu:1 --mem 64GB --job-name el_100 bin/run.sh python models/train.py --config config/el_zeshel_cross_enc_small_train.json --neg_strategy hard_negs --neg_mine_bienc_model_config ../../results/4_Zeshel/d=ent_link/m=bi_enc_l=ce_s=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json --num_negs 63 --train_batch_size 8  --misc hard_negs_63_bs_8_w_hard_bienc_small_train 

	    Train a biencoder with larger negatives (= 100) and also on smaller data

	    	sbatch -p rtx8000-long --gres gpu:1 --mem 64GB --job-name el_bi_all_100 bin/run.sh python models/train.py --config config/el_zeshel_bi_enc.json --neg_strategy hard_negs --train_batch_size 8 --misc hard_negs_63_bs_8_4_epochs --num_negs 63 --num_epoch 4

	    	sbatch -p rtx8000-long --gres gpu:1 --mem 64GB --job-name el_bi_100 bin/run.sh python models/train.py --config config/el_zeshel_bi_enc_small_train.json --neg_strategy hard_negs --train_batch_size 8 --misc hard_negs_63_bs_8_small_train_4_epochs --num_negs 63 --num_epoch 4

	    Now eval

	    	sbatch -p gpu --gres gpu:1 --mem 64GB --exclude gpu-0-0 bin/run.sh python eval/run_cross_encoder_w_binenc_retriever_zeshel.py --data_name $data --n_ment -1 --top_k 100 --batch_size 1 --bi_model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json --res_dir ../../results/4_Zeshel/d\=ent_link/m=cross_enc_l=ce_s=1234_hard_negs_63_bs_8_w_hard_bienc_small_train/eval  --cross_model_config ../../results/4_Zeshel/d\=ent_link/m=cross_enc_l=ce_s=1234_hard_negs_63_bs_8_w_hard_bienc_small_train/model/best_wrt_dev/wrapper_config.json

			sbatch -p gpu --gres gpu:1 --mem 64GB --exclude gpu-0-0 bin/run.sh python eval/run_biencoder_eval_zeshel.py --data_name $data --n_ment -1 --top_k 100 --batch_size 1 --model_config ../../results/4_Zeshel/d\=ent_link/m=bi_enc_l=ce_s=1234_hard_negs_63_bs_8_small_train_4_epochs/model/best_wrt_dev/wrapper_config.json --res_dir ../../results/4_Zeshel/d\=ent_link/m=bi_enc_l=ce_s=1234_hard_negs_63_bs_8_small_train_4_epochs/eval  




	Jan 14 
		
		Analyzing NSW graph and creating trainign data
		
			python eval/analyze_nsw_graph.py --data_name lego --res_dir ../../results/4_Zeshel/d=ent_link/m=cross_enc_l=ce_s=1234_hard_negs_wo_dp_bs_16_w_hard_bienc/score_mats --bi_model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json --embed_type tfidf


		Training another cross-encoder model on small subset of zeshel with larger learning rate

			sbatch -p rtx8000-long --gres gpu:1 --mem 64GB --job-name el_ce_100 bin/run.sh python models/train.py --config config/el_zeshel_cross_enc_small_train.json --neg_strategy hard_negs --neg_mine_bienc_model_config ../../results/4_Zeshel/d=ent_link/m=bi_enc_l=ce_s=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json --num_negs 63 --train_batch_size 8  --misc hard_negs_63_bs_8_w_hard_bienc_small_train_lr_5e5 --learning_rate 0.00005 

			


	Jan 15

		Running NSW eval with varying number of max_node_degree


			sbatch -p gpu --gres gpu:1 --mem 64GB --exclude gpu-0-0 bin/run.sh  python eval/nsw_eval_zeshel.py --embed_type tfidf --bi_model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json --res_dir ../../results/4_Zeshel/d\=ent_link/m\=cross_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_16_w_hard_bienc/score_mats --data_name lego

			sbatch -p gpu --gres gpu:1 --mem 64GB --exclude gpu-0-0 bin/run.sh  python eval/nsw_eval_zeshel.py --embed_type bienc --bi_model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json --res_dir ../../results/4_Zeshel/d\=ent_link/m\=cross_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_16_w_hard_bienc/score_mats --data_name lego

			python eval/analyze_nsw_graph.py --data_name lego --res_dir ../../results/4_Zeshel/d=ent_link/m=cross_enc_l=ce_s=1234_hard_negs_wo_dp_bs_16_w_hard_bienc/score_mats --bi_model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json --embed_type tfidf





	Jan 17 and 19
		
		Running Cross-Encoder Training with NSW Eval on Zeshel Small Training data

			sbatch -p rtx8000-long --gres gpu:1 --mem 64GB --job-name nsw_10_4_bienc bin/run.sh python models/train.py --config config/el_zeshel_cross_enc_small_train.json --exp_id 5_CrossEnc --neg_strategy nsw_graph --nsw_max_nbrs 10 --nsw_num_paths 4 --num_negs_per_node 8 --nsw_embed_type bienc --train_batch_size 48 --misc nsw_negs_10_nbrs_4_paths_8_negs_w_hard_bienc_small_train 


			sbatch -p rtx8000-long --gres gpu:1 --mem 64GB --job-name nsw_10_4_tfidf bin/run.sh python models/train.py --config config/el_zeshel_cross_enc_small_train.json --exp_id 5_CrossEnc --neg_strategy nsw_graph --neg_mine_bienc_model_config None --nsw_max_nbrs 10 --nsw_num_paths 4 --num_negs_per_node 8 --nsw_embed_type tfidf --train_batch_size 48 --misc nsw_negs_10_nbrs_4_paths_8_negs_w_tfidf_small_train 


			--

			sbatch -p rtx8000-long --gres gpu:1 --mem 64GB --job-name nsw_10_4_bienc bin/run.sh python models/train.py --config config/el_zeshel_cross_enc_small_train.json --exp_id 5_CrossEnc --neg_strategy nsw_graph --nsw_max_nbrs 10 --nsw_num_paths 4 --num_negs_per_node 8 --nsw_embed_type bienc --train_batch_size 40 --misc nsw_negs_10_nbrs_4_paths_8_negs_w_hard_bienc_small_train 


			sbatch -p rtx8000-long --gres gpu:1 --mem 64GB --job-name nsw_10_4_tfidf bin/run.sh python models/train.py --config config/el_zeshel_cross_enc_small_train.json --exp_id 5_CrossEnc --neg_strategy nsw_graph --neg_mine_bienc_model_config None --nsw_max_nbrs 10 --nsw_num_paths 4 --num_negs_per_node 8 --nsw_embed_type tfidf --train_batch_size 40 --misc nsw_negs_10_nbrs_4_paths_8_negs_w_tfidf_small_train 



	Jan 21

		Re-running some previous bienc and crossenc jobs to make sure that pytorch lightning did not result in any significant changes

			Crossencoder model - running
				sbatch -p rtx8000-long --gres gpu:1 --mem 64GB --job-name pl_repr bin/run.sh python models/train.py --config config/el_zeshel_cross_enc_small_train.json --neg_strategy hard_negs --neg_mine_bienc_model_config ../../results/4_Zeshel/d=ent_link/m=bi_enc_l=ce_s=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json --num_negs 63 --train_batch_size 8 --misc hard_negs_63_bs_8_w_hard_bienc_small_train_reproduce --num_epoch 4

			Biencoder model - to run after fixing data reload functionality with pytorch lightning
				sbatch -p rtx8000-long --gres gpu:1 --mem 64GB --job-name nsw_10_4_tfidf bin/run.sh python models/train.py --config config/el_zeshel_bi_enc_small_train.json --neg_strategy hard_negs --train_batch_size 8 --misc hard_negs_63_bs_8_small_train_4_epochs_reproduce --num_negs 63 --num_epoch 4



	Jan 26

		Running matrix computation for cross-enc trained using NSW (for best model wrt dev)

			python utils/run_cross_encoder_for_ment_ent_matrix_zeshel.py \
			--cross_model_config ../../results/5_CrossEnc/d=ent_link/m=cross_enc_l=ce_s=1234_nsw_negs_10_nbrs_4_paths_8_negs_w_tfidf_small_train/model/best_wrt_dev/wrapper_config.json \
			--res_dir ../../results/5_CrossEnc/d=ent_link/m=cross_enc_l=ce_s=1234_nsw_negs_10_nbrs_4_paths_8_negs_w_tfidf_small_train/score_mats \
			--n_ment 100 \
			--n_ent -1 \
			--batch_size 100 \
			--data_name $data \
			--layers final




	Jan 27

		Running NSW eval now using cross-enc trained using NSW

			sbatch -p gpu --gres gpu:1 --mem 64GB --exclude gpu-0-0 bin/run.sh \
			python eval/nsw_eval_zeshel.py \
			--embed_type bienc \
			--bi_model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json \
			--res_dir ../../results/5_CrossEnc/d\=ent_link/m=cross_enc_l=ce_s=1234_nsw_negs_10_nbrs_4_paths_8_negs_w_tfidf_small_train/score_mats \
			--data_name lego


	Jan 29
		
		Comparing pytorch ligtning and my previous code to figure out differences


			python models/train.py --config config/el_zeshel_cross_enc_debug.json --train_batch_size 4 --misc debug_pl_model_ckpt --num_gpus 1 --grad_acc_steps 2 --print_interval 1 --eval_interval 1 



		Eval pytorch lightning trained model small zeshel train set

			sbatch -p gpu --gres gpu:1 --mem 64GB --exclude gpu-0-0 bin/run.sh \
			python eval/run_cross_encoder_w_binenc_retriever_zeshel.py \
			--data_name lego \
			--n_ment -1 \
			--top_k 100 \
			--batch_size 1 \
			--bi_model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json \
			--res_dir ../../results/4_Zeshel/d\=ent_link/m=cross_enc_l=ce_s=1234_hard_negs_63_bs_8_w_hard_bienc_small_train_reproduce/eval \
			--cross_model_ckpt ../../results/4_Zeshel/d\=ent_link/m=cross_enc_l=ce_s=1234_hard_negs_63_bs_8_w_hard_bienc_small_train_reproduce/model/last.ckpt



	Jan 30
		
		Running matrix computation for cross-enc trained using NSW (for final model)
		
			sbatch -p gpu --gres gpu:1 --mem 64GB --exclude gpu-0-0 bin/run.sh \
			python utils/run_cross_encoder_for_ment_ent_matrix_zeshel.py \
			--cross_model_config ../../results/5_CrossEnc/d=ent_link/m=cross_enc_l=ce_s=1234_nsw_negs_10_nbrs_4_paths_8_negs_w_tfidf_small_train/model/curr_epoch/wrapper_config.json \
			--res_dir ../../results/5_CrossEnc/d=ent_link/m=cross_enc_l=ce_s=1234_nsw_negs_10_nbrs_4_paths_8_negs_w_tfidf_small_train/score_mats_wrt_final_model \
			--n_ment 100 \
			--n_ent -1 \
			--batch_size 100 \
			--data_name $data \
			--layers final



		Running NSW eval now using cross-enc trained using NSW -- using final model for eval

			sbatch -p gpu --gres gpu:1 --mem 64GB --exclude gpu-0-0 bin/run.sh \
			python eval/nsw_eval_zeshel.py \
			--embed_type bienc \
			--bi_model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json \
			--res_dir ../../results/5_CrossEnc/d\=ent_link/m=cross_enc_l=ce_s=1234_nsw_negs_10_nbrs_4_paths_8_negs_w_tfidf_small_train/score_mats_wrt_final_model \
			--data_name lego

			sbatch -p gpu --gres gpu:1 --mem 64GB --exclude gpu-0-0 bin/run.sh \
			python eval/nsw_eval_zeshel.py \
			--embed_type tfidf \
			--bi_model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json \
			--res_dir ../../results/5_CrossEnc/d\=ent_link/m=cross_enc_l=ce_s=1234_nsw_negs_10_nbrs_4_paths_8_negs_w_tfidf_small_train/score_mats_wrt_final_model \
			--data_name lego

		Run when american_football computation ends

			sbatch -p gpu --gres gpu:1 --mem 64GB --exclude gpu-0-0 bin/run.sh \
			python eval/nsw_eval_zeshel.py \
			--embed_type bienc \
			--bi_model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json \
			--res_dir ../../results/5_CrossEnc/d\=ent_link/m=cross_enc_l=ce_s=1234_nsw_negs_10_nbrs_4_paths_8_negs_w_tfidf_small_train/score_mats_wrt_final_model \
			--data_name american_football

			sbatch -p gpu --gres gpu:1 --mem 64GB --exclude gpu-0-0 bin/run.sh \
			python eval/nsw_eval_zeshel.py \
			--embed_type tfidf \
			--bi_model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json \
			--res_dir ../../results/5_CrossEnc/d\=ent_link/m=cross_enc_l=ce_s=1234_nsw_negs_10_nbrs_4_paths_8_negs_w_tfidf_small_train/score_mats_wrt_final_model \
			--data_name american_football



		Eval pytorch lightning trained model small zeshel train set

			sbatch -p gpu --gres gpu:1 --mem 64GB --exclude gpu-0-0 bin/run.sh \
			python eval/run_cross_encoder_w_binenc_retriever_zeshel.py \
			--data_name $data \
			--n_ment -1 \
			--top_k 100 \
			--batch_size 1 \
			--bi_model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json \
			--res_dir ../../results/4_Zeshel/d\=ent_link/m=cross_enc_l=ce_s=1234_hard_negs_63_bs_8_w_hard_bienc_small_train_reproduce_try_2/eval \
			--cross_model_ckpt ../../results/4_Zeshel/d\=ent_link/m=cross_enc_l=ce_s=1234_hard_negs_63_bs_8_w_hard_bienc_small_train_reproduce_try_2/model/model-4006.0-1.40.ckpt


		Train a cross-encoder model using NSW graph by finetuning previously trained model using biencoder hard negatives

			sbatch -p rtx8000-long --gres gpu:1 --mem 64GB --job-name nsw_10_4_tfidf bin/run.sh python models/train.py \
			--config config/el_zeshel_cross_enc_small_train.json \
			--exp_id 5_CrossEnc \
			--neg_strategy nsw_graph \
			--nsw_max_nbrs 10 \
			--nsw_num_paths 4 \
			--num_negs_per_node 8 \
			--nsw_embed_type tfidf \
			--train_batch_size 48 \
			--path_to_model ../../results/4_Zeshel/d\=ent_link/m=cross_enc_l=ce_s=1234_hard_negs_63_bs_8_w_hard_bienc/model/best_wrt_dev/model.torch \
			--misc nsw_negs_10_nbrs_4_paths_8_negs_w_tfidf_small_train_finetune_31


			sbatch -p rtx8000-long --gres gpu:1 --mem 64GB --job-name nsw_10_4_bienc bin/run.sh python models/train.py \
			--config config/el_zeshel_cross_enc_small_train.json \
			--exp_id 5_CrossEnc \
			--neg_strategy nsw_graph \
			--nsw_max_nbrs 10 \
			--nsw_num_paths 4 \
			--num_negs_per_node 8 \
			--nsw_embed_type bienc \
			--train_batch_size 64 \
			--path_to_model ../../results/4_Zeshel/d\=ent_link/m=cross_enc_l=ce_s=1234_hard_negs_63_bs_8_w_hard_bienc/model/best_wrt_dev/model.torch \
			--misc nsw_negs_10_nbrs_4_paths_8_negs_w_hard_bienc_small_train_finetune_31


	Jan 31


		Running matrix computation for cross-enc trained using NSW (for model finetuned after bienc training)
			
			Finetuned on bienc nsw graph
				sbatch -p gpu --gres gpu:1 --mem 64GB --exclude gpu-0-0 bin/run.sh \
				python utils/run_cross_encoder_for_ment_ent_matrix_zeshel.py \
				--cross_model_ckpt ../../results/5_CrossEnc/d=ent_link/m=cross_enc_l=ce_s=1234_nsw_negs_10_nbrs_4_paths_8_negs_w_hard_bienc_small_train_finetune_31/model/model-599.0-2.18.ckpt \
				--res_dir ../../results/5_CrossEnc/d=ent_link/m=cross_enc_l=ce_s=1234_nsw_negs_10_nbrs_4_paths_8_negs_w_hard_bienc_small_train_finetune_31/score_mats_599 \
				--n_ment 100 \
				--n_ent -1 \
				--batch_size 100 \
				--data_name $data \
				--layers final

			Finetuned on tfidf nsw graph
				sbatch -p gpu --gres gpu:1 --mem 64GB --exclude gpu-0-0 bin/run.sh \
				python utils/run_cross_encoder_for_ment_ent_matrix_zeshel.py \
				--cross_model_ckpt ../../results/5_CrossEnc/d=ent_link/m=cross_enc_l=ce_s=1234_nsw_negs_10_nbrs_4_paths_8_negs_w_tfidf_small_train_finetune_31/model/model-399.0-2.08.ckpt \
				--res_dir ../../results/5_CrossEnc/d=ent_link/m=cross_enc_l=ce_s=1234_nsw_negs_10_nbrs_4_paths_8_negs_w_tfidf_small_train_finetune_31/score_mats_399 \
				--n_ment 100 \
				--n_ent -1 \
				--batch_size 100 \
				--data_name $data \
				--layers final

		Running cross-encoder eval with bienc retrieval

			Finetuned on bienc nsw graph
				sbatch -p gpu --gres gpu:1 --mem 64GB --exclude gpu-0-0 bin/run.sh python eval/run_cross_encoder_w_binenc_retriever_zeshel.py \
				--data_name $data \
				--n_ment -1 \
				--top_k 100 \
				--batch_size 100 \
				--res_dir ../../results/5_CrossEnc/d=ent_link/m=cross_enc_l=ce_s=1234_nsw_negs_10_nbrs_4_paths_8_negs_w_hard_bienc_small_train_finetune_31/eval \
				--bi_model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json  \
				--cross_model_ckpt ../../results/5_CrossEnc/d=ent_link/m=cross_enc_l=ce_s=1234_nsw_negs_10_nbrs_4_paths_8_negs_w_hard_bienc_small_train_finetune_31/model/model-599.0-2.18.ckpt

			Finetuned on tfidf nsw graph
				sbatch -p gpu --gres gpu:1 --mem 64GB --exclude gpu-0-0 bin/run.sh python eval/run_cross_encoder_w_binenc_retriever_zeshel.py \
				--data_name $data \
				--n_ment -1 \
				--top_k 100 \
				--batch_size 100 \
				--res_dir ../../results/5_CrossEnc/d=ent_link/m=cross_enc_l=ce_s=1234_nsw_negs_10_nbrs_4_paths_8_negs_w_tfidf_small_train_finetune_31/eval \
				--bi_model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json  \
				--cross_model_ckpt ../../results/5_CrossEnc/d=ent_link/m=cross_enc_l=ce_s=1234_nsw_negs_10_nbrs_4_paths_8_negs_w_tfidf_small_train_finetune_31/model/model-399.0-2.08.ckpt


			Train a cross-encoder model using NSW graph using margin based loss

				sbatch -p rtx8000-long --gres gpu:1 --mem 64GB --job-name tfidf_margin_nsw_10_4 bin/run.sh python models/train.py \
				--config config/el_zeshel_cross_enc_small_train.json \
				--exp_id 5_CrossEnc \
				--neg_strategy nsw_graph \
				--nsw_max_nbrs 10 \
				--nsw_num_paths 4 \
				--num_negs_per_node 8 \
				--nsw_embed_type tfidf \
				--train_batch_size 48 \
				--loss_type margin \
				--hinge_margin 1.0 \
				--misc nsw_negs_10_nbrs_4_paths_8_negs_w_tfidf_small_train


				sbatch -p rtx8000-long --gres gpu:1 --mem 64GB --job-name bienc_margin_nsw_10_4 bin/run.sh python models/train.py \
				--config config/el_zeshel_cross_enc_small_train.json \
				--exp_id 5_CrossEnc \
				--neg_strategy nsw_graph \
				--nsw_max_nbrs 10 \
				--nsw_num_paths 4 \
				--num_negs_per_node 8 \
				--nsw_embed_type bienc \
				--train_batch_size 64 \
				--loss_type margin \
				--hinge_margin 1.0 \
				--misc nsw_negs_10_nbrs_4_paths_8_negs_w_hard_bienc_small_train


Feb 2022
	1 Feb

		Debugging some logging issues

		python models/train.py \
		--config config/el_zeshel_cross_enc_debug.json \
		--exp_id _0_Debug \
		--neg_strategy random \
		--train_batch_size 4 \
		--loss_type margin \
		--misc debug_margin

	2 Feb
		
		Train a cross-encoder using tfidf hard negs

			sbatch -p rtx8000-long --gres gpu:1 --mem 64GB --job-name tfidf_negs bin/run.sh python models/train.py \
			--config config/el_zeshel_cross_enc_small_train.json \
			--neg_strategy hard_negs_tfidf \
			--num_negs 63 \
			--train_batch_size 8 \
			--misc hard_negs_63_bs_8_w_tfidf_small_train \
			--num_epochs 10

		Testing multi-gpu training

			python models/train.py \
			--config config/el_zeshel_cross_enc_debug.json \
			--exp_id _0_Debug \
			--train_batch_size 4 \
			--loss_type ce \
			--neg_strategy random \
			--num_gpus 4 --strategy ddp   \
			--misc debug_multi_gpu_4 \
			--neg_mine_bienc_model_config "" 

			python models/train.py \
			--config config/el_zeshel_cross_enc_debug.json \
			--exp_id _0_Debug \
			--train_batch_size 4 \
			--loss_type ce \
			--neg_strategy random \
			--misc debug_multi_gpu_0 \
			--neg_mine_bienc_model_config "" 


	3 Feb
		
		Train model with NSW negs - trained to rank neighbors 

			sbatch -p rtx8000-long --gres gpu:1 --mem 64GB --job-name nsw_rank_ce \
			bin/run.sh python models/train.py \
			--config config/el_zeshel_cross_enc_small_train.json \
			--exp_id 5_CrossEnc \
			--num_negs 63 \
			--train_batch_size 8 \
			--neg_strategy nsw_graph_rank \
			--dist_cutoff 3 \
			--nsw_embed_type tfidf \
			--loss_type rank_ce \
			--misc 10_maxnbrs_3_distcut_63_negs_w_tfidf_small_train

			sbatch -p rtx8000-long --gres gpu:1 --mem 64GB --job-name nsw_rank_margin \
			bin/run.sh python models/train.py \
			--config config/el_zeshel_cross_enc_small_train.json \
			--exp_id 5_CrossEnc \
			--num_negs 63 \
			--train_batch_size 8 \
			--neg_strategy nsw_graph_rank \
			--dist_cutoff 3 \
			--nsw_embed_type tfidf \
			--loss_type rank_margin \
			--misc 10_maxnbrs_3_distcut_63_negs_w_tfidf_small_train


		#    Running matrix computation for cross-enc trained using NSW with margin based loss
	        sbatch -p gpu --gres gpu:1 --mem 64GB --job-name ${data}_1 --exclude gpu-0-0 bin/run.sh \
	        python utils/run_cross_encoder_for_ment_ent_matrix_zeshel.py \
	        --n_ment 100 \
	        --n_ent -1 \
	        --batch_size 100 \
	        --data_name $data \
	        --layers final \
	        --cross_model_ckpt ../../results/5_CrossEnc/d=ent_link/m=cross_enc_l=margin_s=1234_nsw_negs_10_nbrs_4_paths_8_negs_w_tfidf_small_train/model/model-2599.0-0.71.ckpt \
	        --res_dir          ../../results/5_CrossEnc/d=ent_link/m=cross_enc_l=margin_s=1234_nsw_negs_10_nbrs_4_paths_8_negs_w_tfidf_small_train/score_mats_2599


	        sbatch -p gpu --gres gpu:1 --mem 64GB --job-name ${data}_2 --exclude gpu-0-0 bin/run.sh \
	        python utils/run_cross_encoder_for_ment_ent_matrix_zeshel.py \
	        --n_ment 100 \
	        --n_ent -1 \
	        --batch_size 100 \
	        --data_name $data \
	        --layers final \
	        --cross_model_ckpt ../../results/5_CrossEnc/d=ent_link/m=cross_enc_l=margin_s=1234_nsw_negs_10_nbrs_4_paths_8_negs_w_tfidf_small_train/model/last.ckpt \
	        --res_dir          ../../results/5_CrossEnc/d=ent_link/m=cross_enc_l=margin_s=1234_nsw_negs_10_nbrs_4_paths_8_negs_w_tfidf_small_train/score_mats_last


	        sbatch -p gpu --gres gpu:1 --mem 64GB --job-name ${data}_3 --exclude gpu-0-0 bin/run.sh \
	        python utils/run_cross_encoder_for_ment_ent_matrix_zeshel.py \
	        --n_ment 100 \
	        --n_ent -1 \
	        --batch_size 100 \
	        --data_name $data \
	        --layers final \
	        --cross_model_ckpt ../../results/5_CrossEnc/d=ent_link/m=cross_enc_l=margin_s=1234_nsw_negs_10_nbrs_4_paths_8_negs_w_hard_bienc_small_train/model/last.ckpt \
	        --res_dir          ../../results/5_CrossEnc/d=ent_link/m=cross_enc_l=margin_s=1234_nsw_negs_10_nbrs_4_paths_8_negs_w_hard_bienc_small_train/score_mats_last


	4 Feb
		
		sbatch -p gpu --gres gpu:1 --mem 64GB --exclude gpu-0-0 bin/run.sh \
			python eval/nsw_eval_zeshel.py \
			--embed_type tfidf \
			--bi_model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json \
			--res_dir ../../results/5_CrossEnc/d\=ent_link/m=cross_enc_l=margin_s=1234_nsw_negs_10_nbrs_4_paths_8_negs_w_tfidf_small_train/score_mats_2599 \
			--data_name $data

			sbatch -p gpu --gres gpu:1 --mem 64GB --exclude gpu-0-0 bin/run.sh \
			python eval/nsw_eval_zeshel.py \
			--embed_type bienc \
			--bi_model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json \
			--res_dir ../../results/5_CrossEnc/d\=ent_link/m=cross_enc_l=margin_s=1234_nsw_negs_10_nbrs_4_paths_8_negs_w_tfidf_small_train/score_mats_2599 \
			--data_name $data


		Running some NSW evals

			lego pro_wrestling both
				sbatch -p gpu --gres gpu:1 --mem 64GB --exclude gpu-0-0 bin/run.sh \
				python eval/nsw_eval_zeshel.py \
				--embed_type $embed \
				--bi_model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json \
				--res_dir ../../results/5_CrossEnc/d\=ent_link/m=cross_enc_l=ce_s=1234_nsw_negs_10_nbrs_4_paths_8_negs_w_tfidf_small_train_finetune_31/score_mats_399 \
				--data_name $data


				sbatch -p gpu --gres gpu:1 --mem 64GB --exclude gpu-0-0 bin/run.sh \
				python eval/nsw_eval_zeshel.py \
				--embed_type $embed \
				--bi_model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json \
				--res_dir ../../results/4_Zeshel/d\=ent_link/m=cross_enc_l=ce_s=1234_hard_negs_wo_dp_bs_16_w_hard_bienc/score_mats_wrt_final_model \
				--data_name $data
		
			lego only
				sbatch -p gpu --gres gpu:1 --mem 64GB --exclude gpu-0-0 bin/run.sh \
				python eval/nsw_eval_zeshel.py \
				--embed_type $embed \
				--bi_model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json \
				--res_dir ../../results/5_CrossEnc/d\=ent_link/m=cross_enc_l=ce_s=1234_nsw_negs_10_nbrs_4_paths_8_negs_w_tfidf_small_train/score_mats_wrt_final_model \
				--data_name $data		

			pro_wrestling only
				sbatch -p gpu --gres gpu:1 --mem 64GB --exclude gpu-0-0 bin/run.sh \
				python eval/nsw_eval_zeshel.py \
				--embed_type $embed \
				--bi_model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json \
				--res_dir ../../results/5_CrossEnc/d\=ent_link/m=cross_enc_l=ce_s=1234_nsw_negs_10_nbrs_4_paths_8_negs_w_tfidf_small_train/score_mats \
				--data_name $data		


				sbatch -p gpu --gres gpu:1 --mem 64GB --exclude gpu-0-0 bin/run.sh \
				python eval/nsw_eval_zeshel.py \
				--embed_type $embed \
				--bi_model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json \
				--res_dir ../../results/4_Zeshel/d\=ent_link/pro m=cross_enc_l=ce_s=1234_hard_negs_wo_dp_bs_16_w_hard_bienc/score_mats \
				--data_name $data
			

			lego pro_wrestling both
				sbatch -p gpu --gres gpu:1 --mem 64GB --exclude gpu-0-0 bin/run.sh \
				python eval/nsw_eval_zeshel.py \
				--embed_type $embed \
				--bi_model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json \
				--res_dir ../../results/5_CrossEnc/d\=ent_link/m=cross_enc_l=rank_ce_neg=nsw_graph_rank_s=1234_10_maxnbrs_3_distcut_63_negs_w_tfidf_small_train/score_mats_1999 \
				--data_name $data


				sbatch -p gpu --gres gpu:1 --mem 64GB --exclude gpu-0-0 bin/run.sh \
				python eval/nsw_eval_zeshel.py \
				--embed_type $embed \
				--bi_model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json \
				--res_dir ../../results/4_Zeshel/d\=ent_link/m=cross_enc_l=rank_margin_neg=nsw_graph_rank_s=1234_10_maxnbrs_3_distcut_63_negs_w_tfidf_small_train/score_mats_2806 \
				--data_name $data
		

	5 Feb


		Score matrix calculations

		m=cross_enc_l=rank_margin_neg=nsw_graph_rank_s=1234_10_maxnbrs_3_distcut_63_negs_w_tfidf_small_train
			score mat computation

		m=cross_enc_l=rank_ce_neg=nsw_graph_rank_s=1234_10_maxnbrs_3_distcut_63_negs_w_tfidf_small_train
			score mat computation

		m=cross_enc_l=margin_s=1234_nsw_negs_10_nbrs_4_paths_8_negs_w_tfidf_small_train
			all set 

		m=cross_enc_l=ce_s=1234_nsw_negs_10_nbrs_4_paths_8_negs_w_tfidf_small_train_finetune_31
			all set


		m=cross_enc_l=ce_s=1234_nsw_negs_10_nbrs_4_paths_8_negs_w_tfidf_small_train
			score_mats_wrt_final_model
				pro	

		m=cross_enc_l=ce_s=1234_hard_negs_wo_dp_bs_16_w_hard_bienc
			score_mats_wrt_final_model
				lego nsw eval 
			score_mats
				pro nsw eval

		m=cross_enc_l=ce_s=1234_hard_negs_63_bs_8_w_hard_bienc_small_train
			score_mats_wrt_final_model
				lego pro nsw eval 
			score_mats
				lego pro nsw eval


	7 Feb
		

		Tfidf trained crossencoder with tfidf based retrieval

			sbatch -p gpu --gres gpu:1 --mem 64GB --exclude gpu-0-0 bin/run.sh python eval/run_cross_encoder_w_binenc_retriever_zeshel.py \
			--data_name $data \
			--n_ment -1 \
			--top_k 100 \
			--batch_size 1 \
			--res_dir ../../results/4_Zeshel/d=ent_link/m=cross_enc_l=ce_s=1234_hard_negs_63_bs_8_w_tfidf_small_train/eval \
			--bi_model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json  \
			--cross_model_ckpt ../../results/4_Zeshel/d=ent_link/m=cross_enc_l=ce_s=1234_hard_negs_63_bs_8_w_tfidf_small_train/model/model-1999.0-1.00.ckpt \
			--misc tfidf_retrieval


				sbatch -p gpu --gres gpu:1 --mem 64GB --exclude gpu-0-0 bin/run.sh python eval/run_cross_encoder_w_binenc_retriever_zeshel.py \
				--data_name $data \
				--n_ment -1 \
				--top_k 100 \
				--batch_size 1 \
				--bi_model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json  \
				--res_dir ../../results/4_Zeshel/d=ent_link/m=cross_enc_l=ce_s=1234_hard_negs_63_bs_8_w_hard_bienc_small_train/eval \
				--cross_model_config ../../results/4_Zeshel/d=ent_link/m=cross_enc_l=ce_s=1234_hard_negs_63_bs_8_w_hard_bienc_small_train/model/best_wrt_dev/wrapper_config.json \
				--misc tfidf_retrieval


	8 Feb
		
		Train model with NSW negs - trained to rank neighbors - running on entire dataset this time

			sbatch -p rtx8000-long --gres gpu:1 --mem 64GB --job-name nsw_rank_ce \
			bin/run.sh python models/train.py \
			--config config/el_zeshel_cross_enc.json \
			--exp_id 5_CrossEnc \
			--num_negs 63 \
			--train_batch_size 8 \
			--neg_strategy nsw_graph_rank \
			--dist_cutoff 3 \
			--nsw_embed_type tfidf \
			--loss_type rank_ce \
			--misc 10_maxnbrs_3_distcut_63_negs_w_tfidf

			sbatch -p rtx8000-long --gres gpu:1 --mem 64GB --job-name nsw_rank_margin \
			bin/run.sh python models/train.py \
			--config config/el_zeshel_cross_enc.json \
			--exp_id 5_CrossEnc \
			--num_negs 63 \
			--train_batch_size 8 \
			--neg_strategy nsw_graph_rank \
			--dist_cutoff 3 \
			--nsw_embed_type tfidf \
			--loss_type rank_margin \
			--misc 10_maxnbrs_3_distcut_63_negs_w_tfidf
			 

	11 Feb

		Train model with NSW negs - trained to rank neighbors - running on entire dataset this time - on BiEncoder graph

			sbatch -p rtx8000-long --gres gpu:1 --mem 64GB --job-name bi_nsw_rank_ce \
			bin/run.sh python models/train.py \
			--config config/el_zeshel_cross_enc.json \
			--exp_id 5_CrossEnc \
			--num_negs 63 \
			--train_batch_size 8 \
			--neg_strategy nsw_graph_rank \
			--nsw_max_nbrs 10 \
			--dist_cutoff 3 \
			--nsw_embed_type bienc \
			--loss_type rank_ce \
			--misc 10_maxnbrs_3_distcut_63_negs_w_bienc

			sbatch -p rtx8000-long --gres gpu:1 --mem 64GB --job-name bi_nsw_rank_margin \
			bin/run.sh python models/train.py \
			--config config/el_zeshel_cross_enc.json \
			--exp_id 5_CrossEnc \
			--num_negs 63 \
			--train_batch_size 8 \
			--neg_strategy nsw_graph_rank \
			--nsw_max_nbrs 10 \
			--dist_cutoff 3 \
			--nsw_embed_type bienc \
			--loss_type rank_margin \
			--misc 10_maxnbrs_3_distcut_63_negs_w_bienc


	15 Feb
		
		Train hard_negs ranked using nsw graph loss in debug mode
			python models/train.py \
			--config config/el_zeshel_cross_enc_debug.json \
			--misc hard_negs_w_rank \
			--neg_strategy hard_negs_w_rank \
			--nsw_max_nbrs 10 \
			--nsw_embed_type tfidf \
			--loss_type rank_margin  \
			--train_batch_size 4

		Train hard_negs ranked using BiEncoder nsw graph loss - 
			using margin based ranking loss
				sbatch -p rtx8000-long --gres gpu:1 --mem 64GB --job-name hard_bi_nsw_rank_margin \
				bin/run.sh python models/train.py \
				--config config/el_zeshel_cross_enc.json \
				--exp_id 5_CrossEnc \
				--num_negs 63 \
				--train_batch_size 8 \
				--neg_strategy hard_negs_w_rank \
				--nsw_max_nbrs 10 \
				--dist_cutoff 3 \
				--nsw_embed_type bienc \
				--loss_type rank_margin \
				--misc 10_maxnbrs_63_negs_w_bienc
					

			(To-Run) using ranking cross-entropy loss
				sbatch -p rtx8000-long --gres gpu:1 --mem 64GB --job-name hard_bi_nsw_rank_ce \
				bin/run.sh python models/train.py \
				--config config/el_zeshel_cross_enc.json \
				--exp_id 5_CrossEnc \
				--num_negs 63 \
				--train_batch_size 8 \
				--neg_strategy hard_negs_w_rank \
				--nsw_max_nbrs 10 \
				--dist_cutoff 3 \
				--nsw_embed_type bienc \
				--loss_type rank_margin \
				--misc 10_maxnbrs_63_negs_w_biencs
					

		Train model with NSW negs - trained to rank neighbors - running on entire dataset this time - on BiEncoder graph 
			(Training again as wrong biencoder model path used in 11 Feb exps)
			
			using margin loss
				sbatch -p rtx8000-long --gres gpu:1 --mem 64GB --job-name bi_nsw_rank_margin \
				bin/run.sh python models/train.py \
				--config config/el_zeshel_cross_enc.json \
				--exp_id 5_CrossEnc \
				--num_negs 63 \
				--train_batch_size 8 \
				--neg_strategy nsw_graph_rank \
				--nsw_max_nbrs 10 \
				--dist_cutoff 3 \
				--nsw_embed_type bienc \
				--loss_type rank_margin \
				--misc 10_maxnbrs_3_distcut_63_negs_w_bienc

			(To-Run)  using cross-entropy loss
				sbatch -p rtx8000-long --gres gpu:1 --mem 64GB --job-name bi_nsw_rank_ce \
				bin/run.sh python models/train.py \
				--config config/el_zeshel_cross_enc.json \
				--exp_id 5_CrossEnc \
				--num_negs 63 \
				--train_batch_size 8 \
				--neg_strategy nsw_graph_rank \
				--nsw_max_nbrs 10 \
				--dist_cutoff 3 \
				--nsw_embed_type bienc \
				--loss_type rank_ce \
				--misc 10_maxnbrs_3_distcut_63_negs_w_bienc

	 17 Feb

	 	Running matrix eval for all models
	 		Files in 5_CrossEnc
			 	File ignored
					m=cross_enc_l=rank_ce_neg=nsw_graph_rank_s=1234_10_maxnbrs_3_distcut_63_negs_w_rand_bienc
					m=cross_enc_l=ce_s=1234_nsw_negs_10_nbrs_4_paths_8_negs_w_tfidf_small_train_finetune_31

				Run final model matrix computation after 2 epochs and then run NSW Eval
					m=cross_enc_l=rank_margin_neg=nsw_graph_rank_s=1234_10_maxnbrs_3_distcut_63_negs_w_bienc
					m=cross_enc_l=rank_margin_neg=hard_negs_w_rank_s=1234_10_maxnbrs_63_negs_w_bienc

				m=cross_enc_l=rank_margin_neg=nsw_graph_rank_s=1234_10_maxnbrs_3_distcut_63_negs_w_tfidf_small_train
				m=cross_enc_l=rank_margin_neg=nsw_graph_rank_s=1234_10_maxnbrs_3_distcut_63_negs_w_tfidf
				m=cross_enc_l=rank_margin_neg=nsw_graph_rank_s=1234_10_maxnbrs_3_distcut_63_negs_w_rand_bienc
				m=cross_enc_l=rank_margin_neg=nsw_graph_rank_s=1234_10_maxnbrs_3_distcut_63_negs_w_bienc
				m=cross_enc_l=rank_margin_neg=hard_negs_w_rank_s=1234_10_maxnbrs_63_negs_w_bienc
				m=cross_enc_l=rank_ce_neg=nsw_graph_rank_s=1234_10_maxnbrs_3_distcut_63_negs_w_tfidf_small_train
				m=cross_enc_l=rank_ce_neg=nsw_graph_rank_s=1234_10_maxnbrs_3_distcut_63_negs_w_tfidf
				m=cross_enc_l=margin_s=1234_nsw_negs_10_nbrs_4_paths_8_negs_w_tfidf_small_train
				m=cross_enc_l=ce_s=1234_nsw_negs_10_nbrs_4_paths_8_negs_w_tfidf_small_train_finetune_31
				m=cross_enc_l=ce_s=1234_nsw_negs_10_nbrs_4_paths_8_negs_w_tfidf_small_train

			Files in 4_Zeshel
				m=cross_enc_l=ce_s=1234_hard_negs_wo_dp_bs_16_w_hard_bienc
				m=cross_enc_l=ce_s=1234_hard_negs_63_bs_8_w_hard_bienc_small_train

				--> Matrix comp running
				m=cross_enc_l=ce_s=1234_hard_negs_63_bs_8_w_tfidf_small_train
				m=cross_enc_l=ce_s=1234_hard_negs_63_bs_8_w_hard_bienc


	19 Feb

		Training biencoder model with hard negatives in pytorch lightning framework - 
			to test if reloading data and keeping track of best model after every data load is working as expected

			sbatch -p rtx8000-long --gres gpu:1 --mem 64GB --job-name bi_hard_negs bin/run.sh \
			python models/train.py \
			--config config/el_zeshel_bi_enc_small_train.json \
			--exp_id 4_Zeshel \
			--neg_strategy bienc_hard_negs \
			--train_batch_size 8 \
			--misc hard_negs_63_bs_8_small_train_4_epochs_reproduce \
			--num_negs 63 \
			--num_epoch 4 \
			--reload_dataloaders_every_n_epochs 1


		Training with re-ranking of biencoder hard negatives using crossencoder on small data subset

			sbatch -p rtx8000-long --gres gpu:1 --mem 64GB --job-name bienc_hard_negs_w_rerank \
			bin/run.sh python models/train.py \
			--config config/el_zeshel_cross_enc_small_train.json \
			--exp_id 5_CrossEnc \
			--init_num_negs 256 \
			--num_negs 63 \
			--train_batch_size 8 \
			--neg_strategy bienc_hard_negs_w_rerank \
			--loss_type ce \
			--reload_dataloaders_every_n_epochs 1 \
			--misc 63_from_256_bienc_negs_small_train

		Debugging
		python models/train.py \
		--config config/el_zeshel_bi_enc_debug.json \
		--exp_id _0_Debug \
		--eval_interval 2 \
		--train_batch_size 4 \
		--reload_dataloaders_every_n_epochs 1  \
		--neg_strategy bienc_hard_negs \
		--eval_batch_size 4 \
		--num_negs 8 \
		--misc save_per_epoch_w_tune


	23 Feb
		Training with re-ranking of biencoder hard negatives using crossencoder on entire data (Init using model from 4_Zeshel/lemon-tree-31)

			sbatch -p rtx8000-long --gres gpu:1 --mem 64GB --job-name bienc_hard_negs_w_rerank \
			bin/run.sh python models/train.py \
			--config config/el_zeshel_cross_enc.json \
			--exp_id 5_CrossEnc \
			--init_num_negs 256 \
			--num_negs 63 \
			--train_batch_size 8 \
			--neg_strategy bienc_hard_negs_w_rerank \
			--loss_type ce \
			--reload_dataloaders_every_n_epochs 1 \
			--misc 63_from_256_bienc_negs \
			--path_to_model ../../results/4_Zeshel/d=ent_link/m=cross_enc_l=ce_s=1234_hard_negs_63_bs_8_w_hard_bienc/model/best_wrt_dev/model.torch \
			--eval_batch_size 32

		Running biencoder eval for model trained with hard negatives using pytorch lightning framework
			python eval/run_biencoder_eval_zeshel.py \
			--data_name $data \
			--n_ment -1 \
			--top_k 100 \
			--batch_size 100 \
			--model_ckpt ../../results/4_Zeshel/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_hard_negs_63_bs_8_small_train_4_epochs_reproduce/model/model-0-1999.0-0.66.ckpt \
			--res_dir ../../results/4_Zeshel/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_hard_negs_63_bs_8_small_train_4_epochs_reproduce/eval \
			--misc c1999


	24 Feb

		Training with neg mining using NSW search with cross-encoder model on small dataset

			sbatch -p rtx8000-long --gres gpu:1 --mem 64GB --job-name bienc_nsw_negs \
			bin/run.sh python models/train.py \
			--config config/el_zeshel_cross_enc_small_train.json \
			--exp_id 5_CrossEnc \
			--loss_type ce \
			--num_negs 63 \
			--train_batch_size 8 \
			--neg_strategy bienc_nsw_search \
			--nsw_comp_budget 250 \
			--nsw_beamsize 2 \
			--nsw_max_nbrs 10  \
			--reload_dataloaders_every_n_epochs 1 \
			--misc 63_negs_2_bs_10_max_nbrs_250_budget \
			--path_to_model ../../results/5_CrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_w_rerank_s=1234_63_from_256_bienc_negs_small_train_from_scratch/model/model-0-1199.0-1.44.ckpt


	25 Feb
		Training with re-ranking of biencoder hard negatives using crossencoder on small data subset - training from scratch try 2

			sbatch -p rtx8000-long --gres gpu:1 --mem 64GB --job-name bienc_hard_negs_w_rerank_2 \
			bin/run.sh python models/train.py \
			--config config/el_zeshel_cross_enc_small_train.json \
			--exp_id 5_CrossEnc \
			--init_num_negs 256 \
			--num_negs 63 \
			--train_batch_size 8 \
			--neg_strategy bienc_hard_negs_w_rerank \
			--loss_type ce \
			--reload_dataloaders_every_n_epochs 1 \
			--misc 63_from_256_bienc_negs_small_train_from_scratch_try_2

		Training with neg mining using NSW search with cross-encoder model on small dataset - with larger budget for NSW search

			sbatch -p rtx8000-long --gres gpu:1 --mem 64GB --job-name bienc_500_nsw_negs \
			bin/run.sh python models/train.py \
			--config config/el_zeshel_cross_enc_small_train.json \
			--exp_id 5_CrossEnc \
			--loss_type ce \
			--num_negs 63 \
			--train_batch_size 8 \
			--neg_strategy bienc_nsw_search \
			--nsw_comp_budget 500 \
			--nsw_beamsize 5 \
			--nsw_max_nbrs 10  \
			--reload_dataloaders_every_n_epochs 1 \
			--misc 63_negs_5_bs_10_max_nbrs_500_budget \
			--path_to_model ../../results/5_CrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_w_rerank_s=1234_63_from_256_bienc_negs_small_train_from_scratch/model/model-0-1199.0-1.44.ckpt \
			--eval_batch_size 32 


	26 Feb

		Debugging NSW search

			sbatch -p rtx8000-long --gres gpu:1 --mem 64GB --job-name bienc_nsw_negs \
			bin/run.sh python models/train.py \
			--config config/el_zeshel_cross_enc_debug.json \
			--exp_id _0_Debug \
			--loss_type ce \
			--num_negs 10 \
			--train_batch_size 1 \
			--grad_acc_steps 1 \
			--neg_strategy tfidf_nsw_search \
			--nsw_comp_budget 250 \
			--nsw_beamsize 2 \
			--nsw_max_nbrs 10  \
			--reload_dataloaders_every_n_epochs 1 \
			--misc 63_negs_2_bs_10_max_nbrs_250_budget \
			--path_to_model ../../results/5_CrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_w_rerank_s=1234_63_from_256_bienc_negs_small_train_from_scratch/model/model-0-1199.0-1.44.ckpt


		Training with neg mining using NSW search with cross-encoder model on small dataset

			sbatch -p rtx8000-long --gres gpu:1 --mem 64GB --job-name bienc_nsw_negs \
			bin/run.sh python models/train.py \
			--config config/el_zeshel_cross_enc.json \
			--exp_id 5_CrossEnc \
			--loss_type ce \
			--num_negs 63 \
			--train_batch_size 8 \
			--neg_strategy bienc_nsw_search \
			--nsw_comp_budget 250 \
			--nsw_beamsize 2 \
			--nsw_max_nbrs 10  \
			--reload_dataloaders_every_n_epochs 1 \
			--misc 63_negs_2_bs_10_max_nbrs_250_budget_all_data \
			--path_to_model ../../results/4_Zeshel/d=ent_link/m=cross_enc_l=ce_s=1234_hard_negs_63_bs_8_w_hard_bienc/model/best_wrt_dev/model.torch \
			--eval_batch_size 32


	27 Feb

		Running nbrhood ranking eval
			python eval/run_exact_cross_encoder.py \
			--data_name yugioh \
			--embed_type tfidf \
	       	--bi_model_config  ../../results/4_Zeshel/d=ent_link/m=bi_enc_l=ce_s=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json  \
			--res_dir ../../results/4_Zeshel/d=ent_link/m=cross_enc_l=ce_s=1234_hard_negs_63_bs_8_w_hard_bienc/score_mats


		Training with re-ranking of biencoder hard negatives using crossencoder on small data subset - training from scratch try 3
			sbatch -p rtx8000-long --gres gpu:1 --mem 64GB --job-name bienc_hard_negs_w_rerank_3 \
			bin/run.sh python models/train.py \
			--config config/el_zeshel_cross_enc_small_train.json \
			--exp_id 5_CrossEnc \
			--init_num_negs 256 \
			--num_negs 63 \
			--train_batch_size 8 \
			--neg_strategy bienc_hard_negs_w_rerank \
			--loss_type ce \
			--reload_dataloaders_every_n_epochs 1 \
			--misc 63_from_256_bienc_negs_small_train_from_scratch_try_3 \
			--eval_batch_size 32







	Architecture and hyperparameter choices
	BLINK
		- Bi-Encoder:
			- 10 hard negs w/ in-batch negs = 10 + 127 = 137
			- Input (context) tokens - 128 for best model as per Appendix, 32 as per Sec 4.1.
				- Yes, they use 128. See discussion for crossencoder for more details.
			- Candidate tokens - 128.
			- Batch size 128
			- 30mins per epoch -- seems surprisingly low
				- They use 8GPUs with 32GB memory each and a smaller number of negs hence the speedup.
			- Epochs 5
			- Lr 2x10e-5
			- Uses learning rate scheduler with warmup=0.1
		- Cross-Encoder: 
			- 64 hard negatives from a BiEncoder
			- Input (context) tokens - 128 as per Appendix
				- As per blink/crossencoder/data_process.py - they only use 32 tokens for input as default param and do not override it when this function (with 32 as default value) is called
					- This is however only used at test time in blink/main_dense.py file. 
				- During training they load a pre-computed tensors and it is not clear what dimensions do they have.
					- It looks like they save *.t7 file in blink/biencoder/eval_biencoder.py. When tokenizing input mention and context in this
					file they do accept max_context_len as a parameter with default value of 128.
					- Default value of max_context_len parameter in blink/common/params.py is set to 128.
				**- At least in BLINK/examples/zeshel/README.md where they have instructions for running BLINK exps,
					they use max_context_len=128
				- Create BLINK zeshel data file in examples/create_BLINK_zeshel_data.py used max_tokens of 128 for encoding context. 
					- However, this tokenization is using whitespance and is not wordpiece tokenization. This is just to pack mentions
					and entities together with their context text. No actual tokenization happens here. 
			- Candidate tokens : 128
			- Batch size 1
			- 2.5 hrs per epoch for bert base
			- 8.5 hrs per epoch for bert large
			- Epochs 2
			- Uses learning rate scheduler with warmup=0.1



	NCE Paper
		- Bi-Encoder:
			- 64 hard negs
			- Input (context) tokens - 128 
			- Candidate tokens - 128
			- Batch size 4 (16 ? for SOM retriever in example command)
			- Grad acc steps 2 (16 ? for SOM retriever in example command)
			- Training time
				- 12 hrs on NVIDIA A100 GPU - uses multi-gpu training (as per readme example command)
			- Epochs 4		
			- Lr 1e-5
			- grad clip 1 (default)
			- Uses learning rate scheduler - yes
				- warmup proportion 0.1 (Default value)

		- Cross-Encoder: 
			- 64 hard negatives from a BiEncoder
			- Input (context) tokens - 128
			- Candidate tokens : 128
			- Batch size 2
			- Grad acc steps 2 (as per example command)
			- Training time 
				- 12?? hrs per epoch for bert base
				- Uses only 1 GPU in example command
			- Epochs 3
			- Lr 2e-5
			- grad clip 1 (default)
			- Uses learning rate scheduler - yes
				- warmup proportion 0.2


	My implementation:
		- Bi-Encoder:
			- 63 hard negs
			- ** Input (context) tokens - 32
				- CHANGE THIS TO 128
			- Candidate tokens - 128.
			- Batch size 8
			- Epochs 5
			- Lr 2x10e-5
		- Cross-Encoder: 
			- 64 hard negatives from a BiEncoder
			- ** Input (context) tokens - 32
				- CHANGE THIS TO 128
			- Candidate tokens : 128
			- Batch size 1
			- Epochs 5
			- Grad Clip parameter??


	TODOS

		- Add support for saving and loading training data from disk for more fault resilient training
		
		- See if I can do away with learning rate scheduler and get decent models?
			- See need for warmup in learning rate scheduler - this is the default one in BERT?? 
			There seems to be some evidence that smaller learning rate is usefull in the beginnign - can read more about it

		- What embedding is used for representing mention/entity/pair - CLS token? Avg of CLS tokens? CLS through a linear layer? 



		- Did BLINK or NCE paper use multi-GPU training?
			- BLINK could support batch-size of 128 for biencoder training on zeshel but I can only allow batch-size=8 with grad_acc_steps 4
				- Maybe this because they only used 10 hard negs and used 127 in-batch negs?
				- They use 8 GPUs for training - Note: the following command requires to run on 8 GPUs with 32G memory. (from BLINK/examples/zeshel/README.md)
			- NCE?
				- NVIDIA A1000 GPUs - apparently better than RTX800 in design for AI model training, similar memory to rtx8000 (40GB)


		- See micro vs macro eval difference when reporting results


March 2022

	3 March (re-lauching on 1 March)
		
		Training bienc w/ 63 bienc hard negs - training model to use in future reproducible experiments
			sbatch -p rtx8000-long --gres gpu:1 --mem 64GB --job-name bienc_1 \
			bin/run.sh python  models/train.py \
			--config config/el_zeshel_bi_enc.json \
			--exp_id 6_ReprCrossEnc \
			--neg_strategy bienc_hard_negs \
			--num_negs 63 \
			--train_batch_size 8 \
			--reload_dataloaders_every_n_epochs 1 \
			--num_epochs 4 \
			--warmup_proportion 0.1 \
			--misc 63_hard_negs_4_epochs_wp_0.1

			sbatch -p rtx8000-long --gres gpu:1 --mem 64GB --job-name bienc_2 \
			bin/run.sh python  models/train.py \
			--config config/el_zeshel_bi_enc.json \
			--exp_id 6_ReprCrossEnc \
			--neg_strategy bienc_hard_negs \
			--num_negs 63 \
			--train_batch_size 8 \
			--reload_dataloaders_every_n_epochs 1 \
			--num_epochs 4 \
			--warmup_proportion 0.01 \
			--misc 63_hard_negs_4_epochs_wp_0.01

			sbatch -p rtx8000-long --gres gpu:1 --mem 64GB --job-name bienc_3 \
			bin/run.sh python  models/train.py \
			--config config/el_zeshel_bi_enc.json \
			--exp_id 6_ReprCrossEnc \
			--neg_strategy bienc_hard_negs \
			--num_negs 63 \
			--train_batch_size 8 \
			--reload_dataloaders_every_n_epochs 1 \
			--num_epochs 4 \
			--warmup_proportion 0.001 \
			--misc 63_hard_negs_4_epochs_wp_0.001

			Distributed training 
			sbatch -p rtx8000-long --gres gpu:2 --mem 64GB --job-name bienc_4 \
			bin/run.sh python  models/train.py \
			--config config/el_zeshel_bi_enc.json \
			--exp_id _0_Debug \
			--neg_strategy bienc_hard_negs \
			--num_negs 63 \
			--train_batch_size 16 \
			--eval_batch_size 128 \
			--reload_dataloaders_every_n_epochs 1 \
			--num_epochs 4 \
			--warmup_proportion 0.001 \
			--strategy ddp \
			--num_gpus 2 \
			--misc 63_hard_negs_4_epochs_wp_0.1_w_ddp


	4 March
		
		Debugging multi-gpu training 

			sbatch -p rtx8000-long --gres gpu:2 --mem 64GB --job-name bienc_4 \
			srun -p gpu --gres gpu:2 --mem 64GB --job-name bienc_4 \
			--pty bin/run.sh python  models/train.py \
			--config config/el_zeshel_bi_enc_debug.json \
			--exp_id _0_Debug \
			--neg_strategy bienc_hard_negs \
			--num_negs 8 \
			--train_batch_size 1 \
			--grad_acc_steps 1 \
			--reload_dataloaders_every_n_epochs 1 \
			--misc w_fix_debug_8_hard_negs_4_epochs_0.001_warmup \
			--num_epochs 4 \
			--warmup_proportion 0.5 \
			--eval_interval 10 \
			--eval_batch_size 1 \
			--num_gpus 2 \
			--strategy ddp \
			--path_to_model ../../results/6_ReprCrossEnc/d\=ent_link/m\=bi_enc_l\=ce_neg\=bienc_hard_negs_s\=1234_63_hard_negs_4_epochs_wp_0.1_w_ddp/model/0-last.ckpt 


		Testing model eval bug effect
			srun -p gpu --gres gpu:1 --mem 64GB --job-name eval_1_debug \
			--pty bin/run.sh python  models/train.py \
			--config config/el_zeshel_bi_enc_debug.json \
			--exp_id _0_Debug \
			--neg_strategy bienc_hard_negs \
			--num_negs 8 \
			--train_batch_size 1 \
			--grad_acc_steps 1 \
			--reload_dataloaders_every_n_epochs 1 \
			--misc debug_eval_bug_w_bug \
			--num_epochs 4 \
			--warmup_proportion 0.1 \
			--eval_interval 10 \
			--eval_batch_size 1 

			srun -p gpu --gres gpu:1 --mem 64GB --job-name eval_2_debug \
			--pty bin/run.sh python  models/train.py \
			--config config/el_zeshel_bi_enc_debug.json \
			--exp_id _0_Debug \
			--neg_strategy bienc_hard_negs \
			--num_negs 8 \
			--train_batch_size 1 \
			--grad_acc_steps 1 \
			--reload_dataloaders_every_n_epochs 1 \
			--misc wdebug_eval_bug_wo_bug \
			--num_epochs 4 \
			--warmup_proportion 0.1 \
			--eval_interval 10 \
			--eval_batch_size 1 


	6 March

		Training cross-enc w/ 63 bienc hard negs with reproducible biencoder exp-6-soft-fog-20

			Hard negs w/ BiEncoder
				sbatch -p rtx8000-long --gres gpu:2 --mem 128GB --job-name crossenc_w_hard_bienc_training \
				bin/run.sh python models/train.py \
				--config config/el_zeshel_cross_enc.json \
				--exp_id 6_ReprCrossEnc \
				--neg_strategy bienc_hard_negs \
				--num_negs 63 \
				--train_batch_size 4 \
				--neg_mine_bienc_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
				--reload_dataloaders_every_n_epochs 0 \
				--eval_batch_size 32 \
				--warmup_proportion 0.01 \
				--strategy ddp \
				--num_gpus 2 \
				--misc 63_hard_negs_w_bienc_w_ddp

			Hard negs w/ using NSW search over biencoder graph w/ max_nbrs 10, beamsize = 2, budget = 250
				sbatch -p rtx8000-long --gres gpu:2 --mem 128GB --job-name crossenc_w_hard_bienc_training \
				bin/run.sh python models/train.py \
				--config config/el_zeshel_cross_enc.json \
				--exp_id 6_ReprCrossEnc \
				--num_negs 63 \
				--neg_strategy bienc_nsw_search \
				--nsw_comp_budget 250 \
				--nsw_beamsize 2 \
				--nsw_max_nbrs 10 \
				--reload_dataloaders_every_n_epochs 1 \
				--train_batch_size 4 \
				--neg_mine_bienc_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
				--eval_batch_size 32 \
				--warmup_proportion 0.01 \
				--strategy ddp \
				--num_gpus 2 \
				--misc 63_negs_2_bs_10_max_nbrs_250_budget_w_ddp

			Hard negs w/ using NSW search over biencoder graph w/ max_nbrs 10, beamsize = 5, budget = 500
				sbatch -p rtx8000-long --gres gpu:2 --mem 128GB --job-name crossenc_w_nsw_5_10_500 \
				bin/run.sh python models/train.py \
				--config config/el_zeshel_cross_enc.json \
				--exp_id 6_ReprCrossEnc \
				--num_negs 63 \
				--neg_strategy bienc_nsw_search \
				--nsw_comp_budget 500 \
				--nsw_beamsize 5 \
				--nsw_max_nbrs 10 \
				--reload_dataloaders_every_n_epochs 1 \
				--train_batch_size 4 \
				--neg_mine_bienc_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
				--eval_batch_size 32 \
				--warmup_proportion 0.01 \
				--strategy ddp \
				--num_gpus 2 \
				--misc 63_negs_5_bs_10_max_nbrs_500_budget_w_ddp

			Hard negs w/ by retrieving using crossencoder and then re-ranking using crossencoder
				sbatch -p rtx8000-long --gres gpu:2 --mem 128GB --job-name bienc_hard_negs_w_rerank \
				bin/run.sh python models/train.py \
				--config config/el_zeshel_cross_enc.json \
				--exp_id 6_ReprCrossEnc \
				--init_num_negs 256 \
				--num_negs 63 \
				--neg_strategy bienc_hard_negs_w_rerank \
				--neg_mine_bienc_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
				--loss_type ce \
				--reload_dataloaders_every_n_epochs 1 \
				--train_batch_size 4 \
				--eval_batch_size 32 \
				--warmup_proportion 0.01 \
				--strategy ddp \
				--num_gpus 2 \
				--misc 63_from_256_negs_w_ddp 

		For debugging on blake - ALSO TEST WITH 2 GPUS
			srun -p gpu --gres gpu:2 --mem 64GB --job-name crossenc_w_hard_bienc_training --exclude gpu-0-0 --pty \
			bin/run.sh python models/train.py \
			--config config/el_zeshel_cross_enc_debug.json \
			--exp_id _0_Debug \
			--neg_strategy bienc_hard_negs \
			--num_negs 8 \
			--train_batch_size 1 \
			--grad_acc_steps 1 \
			--neg_mine_bienc_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
			--reload_dataloaders_every_n_epochs 0 \
			--eval_batch_size 32 \
			--eval_interval 5 \
			--warmup_proportion 0.01 \
			--strategy ddp \
			--num_gpus 2 \
			--misc 63_hard_negs_w_bienc_w_ddp
						



			srun -p gpu --gres gpu:2 --mem 64GB --job-name crossenc_w_nsw_search_training --exclude gpu-0-0 --pty \
			bin/run.sh python models/train.py \
			--config config/el_zeshel_cross_enc_debug.json \
			--exp_id _0_Debug \
			--num_negs 8 \
			--neg_strategy bienc_nsw_search \
			--nsw_comp_budget 250 \
			--nsw_beamsize 2 \
			--nsw_max_nbrs 10 \
			--reload_dataloaders_every_n_epochs 1 \
			--train_batch_size 1 \
			--grad_acc_steps 1 \
			--neg_mine_bienc_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
			--eval_batch_size 32 \
			--eval_interval 5 \
			--warmup_proportion 0.01 \
			--strategy ddp \
			--num_gpus 2 \
			--misc 63_negs_2_bs_10_max_nbrs_250_budget


	8 March

		
		Debug use of checkpoint for resuming training for some model

			One issue is how to update config file of model after loading from checkpoint
			Also evaluate this in distributed training setting

			First train with biencoder hard negs for 5 epochs and save model at end of each epoch
				python models/train.py \
				--config config/el_zeshel_cross_enc_debug.json \
				--exp_id _0_Debug \
				--neg_strategy bienc_hard_negs \
				--num_negs 8 \
				--train_batch_size 1 \
				--grad_acc_steps 1 \
				--neg_mine_bienc_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
				--reload_dataloaders_every_n_epochs 0 \
				--eval_batch_size 32 \
				--eval_interval 10 \
				--warmup_proportion 0.01 \
				--misc debug_ckpt_1


			Then resume training for first, second or third checkpoint and see if we get exactly the same model 

				python models/train.py \
				--config config/el_zeshel_cross_enc_debug.json \
				--exp_id _0_Debug \
				--neg_strategy bienc_hard_negs \
				--num_negs 8 \
				--train_batch_size 1 \
				--grad_acc_steps 1 \
				--neg_mine_bienc_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
				--reload_dataloaders_every_n_epochs 0 \
				--eval_batch_size 32 \
				--eval_interval 10 \
				--warmup_proportion 0.01 \
				--ckpt_path  
				TODO: Add checkpoint path in code and here. 
				--misc debug_ckpt_1


			Train using NSW negatives mined adaptively using crossencoder for 10 epochs

				python models/train.py \
				--config config/el_zeshel_cross_enc_debug.json \
				--exp_id _0_Debug \
				--neg_strategy bienc_nsw_search \
				--nsw_comp_budget 250 \
				--nsw_beamsize 2 \
				--nsw_max_nbrs 10 \
				--reload_dataloaders_every_n_epochs 1 \
				--num_negs 8 \
				--train_batch_size 1 \
				--grad_acc_steps 1 \
				--neg_mine_bienc_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
				--reload_dataloaders_every_n_epochs 0 \
				--eval_batch_size 32 \
				--eval_interval 10 \
				--warmup_proportion 0.01 \
				--misc debug_ckpt_3




			See if we are able to use first checkpoint as input to init model and 
			get exactly same result as tranign from scratch with NSW negs



	10 March
		
		Adding accuracy calculations for dev set

			python models/train.py \
			--config config/el_zeshel_cross_enc_debug.json \
			--exp_id _0_Debug \
			--neg_strategy bienc_hard_negs \
			--num_negs 8 \
			--train_batch_size 1 \
			--grad_acc_steps 1 \
			--neg_mine_bienc_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
			--reload_dataloaders_every_n_epochs 0 \
			--eval_batch_size 32 \
			--eval_interval 10 \
			--warmup_proportion 0.01 \
			--misc debug_val_loss


	13 March

		Running crossencoder with biencoder negatives for some hyperparameter combinations

			sbatch -p rtx8000-long --gres gpu:2 --mem 128GB --job-name crossenc_w_hard_bienc_training \
			bin/run.sh python models/train.py \
			--config config/el_zeshel_cross_enc.json \
			--exp_id 6_ReprCrossEnc \
			--pooling_type cls_w_lin \
			--neg_strategy bienc_hard_negs \
			--num_negs 63 \
			--train_batch_size 4 \
			--neg_mine_bienc_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
			--reload_dataloaders_every_n_epochs 0 \
			--eval_batch_size 32 \
			--warmup_proportion 0.01 \
			--strategy ddp \
			--num_gpus 2 \
			--misc 63_hard_negs_w_bienc_w_ddp_cls_w_lin_pooling


			sbatch -p rtx8000-long --gres gpu:2 --mem 128GB --job-name crossenc_w_hard_bienc_training \
			bin/run.sh python models/train.py \
			--config config/el_zeshel_cross_enc.json \
			--exp_id 6_ReprCrossEnc \
			--neg_strategy bienc_hard_negs \
			--num_negs 63 \
			--train_batch_size 4 \
			--neg_mine_bienc_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
			--reload_dataloaders_every_n_epochs 0 \
			--eval_batch_size 32 \
			--ckpt_metric mrr \
			--warmup_proportion 0.01 \
			--strategy ddp \
			--num_gpus 2 \
			--misc 63_hard_negs_w_bienc_w_ddp_best_wrt_dev_mrr


			sbatch -p rtx8000-long --gres gpu:2 --mem 128GB --job-name norm_init_c_w_b_training \
			bin/run.sh python models/train.py \
			--config config/el_zeshel_cross_enc.json \
			--exp_id 6_ReprCrossEnc \
			--neg_strategy bienc_hard_negs \
			--num_negs 63 \
			--train_batch_size 4 \
			--neg_mine_bienc_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
			--reload_dataloaders_every_n_epochs 0 \
			--eval_batch_size 32 \
			--ckpt_metric mrr \
			--warmup_proportion 0.01 \
			--strategy ddp \
			--num_gpus 2 \
			--misc 63_hard_negs_w_bienc_w_ddp_w_best_wrt_dev_mrr_normal_wgt_init

			sbatch -p rtx8000-long --gres gpu:2 --mem 128GB --job-name mrr_c_w_b_training \
			bin/run.sh python models/train.py \
			--config config/el_zeshel_cross_enc.json \
			--exp_id 6_ReprCrossEnc \
			--pooling_type cls_w_lin \
			--neg_strategy bienc_hard_negs \
			--num_negs 63 \
			--train_batch_size 4 \
			--neg_mine_bienc_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
			--reload_dataloaders_every_n_epochs 0 \
			--eval_batch_size 32 \
			--ckpt_metric mrr \
			--warmup_proportion 0.01 \
			--strategy ddp \
			--num_gpus 2 \
			--misc 63_hard_negs_w_bienc_w_ddp_w_best_wrt_dev_mrr_cls_w_lin


			sbatch -p rtx8000-long --gres gpu:2 --mem 128GB --job-name mrr_c_w_b_training \
			bin/run.sh python models/train.py \
			--config config/el_zeshel_bi_enc.json \
			--exp_id 6_ReprCrossEnc \
			--pooling_type cls_w_lin \
			--neg_strategy bienc_hard_negs \
			--num_negs 63 \
			--train_batch_size 4 \
			--neg_mine_bienc_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
			--reload_dataloaders_every_n_epochs 0 \
			--eval_batch_size 32 \
			--ckpt_metric mrr \
			--warmup_proportion 0.01 \
			--strategy ddp \
			--num_gpus 2 \
			--misc 63_hard_negs_w_bienc_w_ddp_w_best_wrt_dev_mrr_cls_w_lin

		Bi-encoder with cls_w_lin pooling and choosing best model wrt dev mrr
			sbatch -p rtx8000-long --gres gpu:2 --mem 128GB --job-name bi_c_w_lin_training \
			bin/run.sh python models/train.py \
			--config config/el_zeshel_bi_enc.json \
			--exp_id 6_ReprCrossEnc \
			--pooling_type cls_w_lin \
			--neg_strategy bienc_hard_negs \
			--num_negs 63 \
			--train_batch_size 8 \
			--eval_batch_size 64 \
			--reload_dataloaders_every_n_epochs 1 \
			--num_epochs 4 \
			--warmup_proportion 0.01 \
			--strategy ddp \
			--num_gpus 2 \
			--misc 63_hard_negs_4_epochs_wp_0.01_w_ddp_w_cls_w_lin

		Running NSW graph analysis

			python eval/analyze_nsw_graph.py \
			--bi_model_file ../../results/6_ReprCrossEnc/d\=ent_link/m\=bi_enc_l\=ce_neg\=bienc_hard_negs_s\=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
			--data_name lego \
			--embed_type tfidf \
			--res_dir ../../results/4_Zeshel/hnsw_debug/score_mats
			

	15 March
		Attempting Traing models on 1080ti or 2080tis with more GPUs - using splitting of num negs in each training instance

				python models/train.py \
				--config config/el_zeshel_cross_enc_small_train.json \
				--exp_id _0_Debug \
				--pooling_type cls \
				--neg_strategy bienc_hard_negs \
				--num_negs 63 \
				--train_batch_size 4 \
				--neg_mine_bienc_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
				--reload_dataloaders_every_n_epochs 0 \
				--eval_batch_size 32 \
				--warmup_proportion 0.01 \
				--strategy ddp \
				--num_gpus 8 \
				--misc _debug_other_gypsum_queues



				python models/train.py \
				--config config/el_zeshel_cross_enc_debug.json \
				--exp_id _0_Debug \
				--pooling_type cls \
				--neg_strategy bienc_hard_negs \
				--num_negs 64 \
				--train_batch_size 4 \
				--neg_mine_bienc_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
				--reload_dataloaders_every_n_epochs 0 \
				--eval_batch_size 32 \
				--warmup_proportion 0.01 \
				--misc _debug_neg_split


		Training with NSW losses with better hyper-params - cls_w_lin pooling and best_wrt_mrr for checkpointing

			Hard negs w/ using NSW search over biencoder graph w/ max_nbrs 10, beamsize = 2, budget = 250
				sbatch -p rtx8000-long --gres gpu:2 --mem 128GB --job-name ce_w_nsw_2_cwl_mrr \
				bin/run.sh python models/train.py \
				--config config/el_zeshel_cross_enc.json \
				--exp_id 6_ReprCrossEnc \
				--pooling_type cls_w_lin \
				--num_negs 63 \
				--neg_strategy bienc_nsw_search \
				--nsw_comp_budget 250 \
				--nsw_beamsize 2 \
				--nsw_max_nbrs 10 \
				--reload_dataloaders_every_n_epochs 1 \
				--train_batch_size 4 \
				--neg_mine_bienc_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
				--eval_batch_size 32 \
				--ckpt_metric mrr \
				--warmup_proportion 0.01 \
				--strategy ddp \
				--num_gpus 2 \
				--misc 63_negs_2_bs_10_max_nbrs_250_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin

			Hard negs w/ using NSW search over biencoder graph w/ max_nbrs 10, beamsize = 5, budget = 500
				# -SET BATCH SPLITTING AND NUMGPUS accordingly sbatch -p m40-long --gres gpu:4 --mem 250GB  --job-name ce_w_nsw_5_cwl_mrr \
				sbatch -p rtx8000-long --gres gpu:2 --mem 128GB --job-name rtx_ce_w_nsw_5_cwl_mrr \
				bin/run.sh \
				python models/train.py \
				--config config/el_zeshel_cross_enc.json \
				--exp_id 6_ReprCrossEnc \
				--pooling_type cls_w_lin \
				--num_negs 64 \
				--neg_strategy bienc_nsw_search \
				--nsw_comp_budget 500 \
				--nsw_beamsize 5 \
				--nsw_max_nbrs 10 \
				--reload_dataloaders_every_n_epochs 1 \
				--train_batch_size 4 \
				--neg_mine_bienc_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
				--eval_batch_size 30 \
				--ckpt_metric mrr \
				--warmup_proportion 0.01 \
				--strategy ddp \
				--num_gpus 2 \
				--eval_interval 600 \
				--misc 64_negs_5_bs_10_max_nbrs_500_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx

			Hard negs w/ using re-ranking of 256 BiEncoder negs
				# sbatch -p rtx8000-long --gres gpu:4 --mem 128GB --job-name ce_w_nsw_5_cwl_mrr \
				# bin/run.sh \
				sbatch -p m40-long --gres gpu:4 --mem 250GB  --job-name ce_w_nsw_5_cwl_mrr \
				bin/run.sh \
				python models/train.py \
				--config config/el_zeshel_cross_enc.json \
				--exp_id 6_ReprCrossEnc \
				--pooling_type cls_w_lin \
				--init_num_negs 256 \
				--num_negs 64 \
				--num_neg_splits 2 \
				--neg_strategy bienc_hard_negs_w_rerank \
				--reload_dataloaders_every_n_epochs 1 \
				--train_batch_size 4 \
				--neg_mine_bienc_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
				--eval_batch_size 30 \
				--ckpt_metric mrr \
				--warmup_proportion 0.01 \
				--strategy ddp \
				--num_gpus 4 \
				--eval_interval 600 \
				--misc 63_from_256_negs_w_ddp_w_best_wrt_dev_mrr_cls_w_lin


	16/17 March
		
		Running inference with data from MuVER paper [https://github.com/Alibaba-NLP/MuVER]

				sbatch -p gpu --gres gpu:1 --mem 64GB  --job-name muver_test --exclude gpu-0-0 \
				run.sh \
				python muver/multi_view/train.py  \
				--pretrained_model bert-base-uncased  \
				--dataset_path ../../data/muver \
				--bi_ckpt_path ../../data/muver/MuVER_best_zeshel.bin \
				--max_cand_len 40 \
				--max_seq_len 128 \
				--do_test \
				--test_mode test \
				--eval_batch_size 16 \
				--accumulate_score

				# sbatch -p gpu --gres gpu:1 --mem 128GB  --job-name muver_train --exclude gpu-0-0 \
				# sbatch -p rtx8000-long --gres gpu:1 --mem 64GB  --job-name muver_train --exclude gpu-0-0 \
				srun -p gpu --gres gpu:2 --mem 100GB  --job-name muver_train --exclude gpu-0-0 --pty \
				run.sh \
				python muver/multi_view/train.py  \
				--pretrained_model bert-base-uncased  \
				--dataset_path ../../data/muver \
				--bi_ckpt_path ../../data/muver/MuVER_best_zeshel.bin \
				--max_cand_len 40 \
				--max_seq_len 128 \
				--do_test \
				--test_mode train \
				--eval_batch_size 16 \
				--accumulate_score

				sbatch -p gpu --gres gpu:1 --mem 128GB  --job-name muver_valid --exclude gpu-0-0 \
				run.sh \
				python muver/multi_view/train.py  \
				--pretrained_model bert-base-uncased  \
				--dataset_path ../../data/muver \
				--bi_ckpt_path ../../data/muver/MuVER_best_zeshel.bin \
				--max_cand_len 40 \
				--max_seq_len 128 \
				--do_test \
				--test_mode valid \
				--eval_batch_size 16 \
				--accumulate_score


	18 March 2022

		Training on crossencoder with binecoder negs on smaller datasets (with better hyperparameters)
			sbatch -p rtx8000-long --gres gpu:2 --mem 128GB --job-name small_ce_train \
			bin/run.sh python models/train.py \
			--config config/el_zeshel_cross_enc_small_train.json \
			--exp_id 6_ReprCrossEnc \
			--pooling_type cls_w_lin \
			--neg_strategy bienc_hard_negs \
			--num_negs 63 \
			--train_batch_size 4 \
			--neg_mine_bienc_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
			--reload_dataloaders_every_n_epochs 0 \
			--eval_batch_size 32 \
			--ckpt_metric mrr \
			--warmup_proportion 0.01 \
			--strategy ddp \
			--num_gpus 2 \
			--eval_interval 0.2 \
			--misc 63_hard_negs_w_bienc_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_small_train

		Saving data for distillation experiments - testing
			python eval/run_cross_encoder_w_nsw_eval.py \
			--data_name lego \
			--n_ment 4 \
			--cross_model_file ../../results/6_ReprCrossEnc/_distill_debug/0-last.ckpt \
			--res_dir ../../results/6_ReprCrossEnc/_distill_debug/eval \
			--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
			--misc test \
			--embed_type bienc \
			--max_nbrs 10 \
			--beamsize 5 \
			--top_k 100 \
			--comp_budget 250

			python eval/run_cross_encoder_w_nsw_eval.py \
			--data_name coronation_street \
			--n_ment -1 \
			--cross_model_file ../../results/6_ReprCrossEnc/_distill_debug/0-last.ckpt \
			--res_dir ../../results/6_ReprCrossEnc/_distill_debug/eval \
			--bi_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
			--embed_type tfidf \
			--max_nbrs 10 \
			--beamsize 5 \
			--top_k 100 \
			--comp_budget 500


		Savign data for distillation experiments from NSW-5-10-500 trained 
			python eval/run_cross_encoder_w_nsw_eval.py \
			--data_name $data \
			--n_ment -1 \
			--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_nsw_search_s=1234_64_negs_5_bs_10_max_nbrs_500_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/model/0-last.ckpt \
			--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_nsw_search_s=1234_64_negs_5_bs_10_max_nbrs_500_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/eval \
			--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
			--embed_type bienc \
			--max_nbrs 10 \
			--beamsize 5 \
			--top_k 100 \
			--comp_budget 500 

		Running distillation experiment with NSW 5-10-500 crossencode model

			Debug
				srun -p gpu --gres gpu:2 --mem 64GB --job-name distill --pty \
				bin/run.sh python models/train.py \
				--config config/el_zeshel_bi_enc_small_train.json \
				--exp_id _0_Debug \
				--neg_strategy distill \
				--distill_n_labels 8 \
				--train_batch_size 4 \
				--path_to_model ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
				--ent_w_score_file_template ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_nsw_search_s=1234_64_negs_5_bs_10_max_nbrs_500_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/eval/{}/m=-1_k=100_nsw_bienc_10_5_500_/crossenc_topk_preds_w_nsw.txt \
				--reload_dataloaders_every_n_epochs 0 \
				--eval_batch_size 32 \
				--warmup_proportion 0.01 \
				--strategy ddp \
				--num_gpus 2 \
				--eval_interval 0.2 \
				--misc distill_w_64_negs_wrt_cross_id_6_82_0
			
			Train on small subset of training data using best biencoder model as ckpt
				sbatch -p rtx8000-long --gres gpu:2 --mem 128GB --job-name distill \
				bin/run.sh python models/train.py \
				--config config/el_zeshel_bi_enc_small_train.json \
				--exp_id 7_EntModel \
				--neg_strategy distill \
				--distill_n_labels 64 \
				--train_batch_size 8 \
				--path_to_model ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
				--ent_w_score_file_template ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_nsw_search_s=1234_64_negs_5_bs_10_max_nbrs_500_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/eval/{}/m=-1_k=100_nsw_bienc_10_5_500_/crossenc_topk_preds_w_nsw.txt \
				--reload_dataloaders_every_n_epochs 0 \
				--eval_batch_size 32 \
				--warmup_proportion 0.01 \
				--strategy ddp \
				--num_gpus 2 \
				--eval_interval 0.2 \
				--misc distill_w_64_negs_wrt_cross_id_6_82_0

			Train on small subset of training data from scratch
				sbatch -p rtx8000-long --gres gpu:2 --mem 128GB --job-name distill_2 \
				bin/run.sh python models/train.py \
				--config config/el_zeshel_bi_enc_small_train.json \
				--exp_id 7_EntModel \
				--neg_strategy distill \
				--distill_n_labels 64 \
				--train_batch_size 8 \
				--ent_w_score_file_template ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_nsw_search_s=1234_64_negs_5_bs_10_max_nbrs_500_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/eval/{}/m=-1_k=100_nsw_bienc_10_5_500_/crossenc_topk_preds_w_nsw.txt \
				--reload_dataloaders_every_n_epochs 0 \
				--eval_batch_size 32 \
				--warmup_proportion 0.01 \
				--strategy ddp \
				--num_gpus 2 \
				--eval_interval 0.2 \
				--misc distill_w_64_negs_wrt_cross_id_6_82_0_from_scratch

	19 March 2022

		Running distillation experiment with NSW 5-10-500 crossencoder model

			Train on all of training data using best biencoder model as ckpt
				sbatch -p rtx8000-long --gres gpu:2 --mem 128GB --job-name distill \
				bin/run.sh python models/train.py \
				--config config/el_zeshel_bi_enc.json \
				--exp_id 7_EntModel \
				--neg_strategy distill \
				--distill_n_labels 64 \
				--train_batch_size 8 \
				--path_to_model ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
				--ent_w_score_file_template ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_nsw_search_s=1234_64_negs_5_bs_10_max_nbrs_500_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/eval/{}/m=-1_k=100_nsw_bienc_10_5_500_/crossenc_topk_preds_w_nsw.txt \
				--reload_dataloaders_every_n_epochs 0 \
				--eval_batch_size 32 \
				--warmup_proportion 0.01 \
				--strategy ddp \
				--num_gpus 2 \
				--eval_interval 0.2 \
				--misc distill_w_64_negs_wrt_cross_id_6_82_0_all_data

			Train on all of training data from scratch
				sbatch -p rtx8000-long --gres gpu:2 --mem 128GB --job-name distill_2 \
				bin/run.sh python models/train.py \
				--config config/el_zeshel_bi_enc.json \
				--exp_id 7_EntModel \
				--neg_strategy distill \
				--distill_n_labels 64 \
				--train_batch_size 8 \
				--ent_w_score_file_template ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_nsw_search_s=1234_64_negs_5_bs_10_max_nbrs_500_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/eval/{}/m=-1_k=100_nsw_bienc_10_5_500_/crossenc_topk_preds_w_nsw.txt \
				--reload_dataloaders_every_n_epochs 0 \
				--eval_batch_size 32 \
				--warmup_proportion 0.01 \
				--strategy ddp \
				--num_gpus 2 \
				--eval_interval 0.2 \
				--misc distill_w_64_negs_wrt_cross_id_6_82_0_from_scratch_all_data

	22 March 2022 (Re-launched on 23 Feb due to use of incorrect ent_w_score_file_template in first attempt)

			Running distillation experiment with NSW 5-10-500 crossencoder model - Iteration 2 of training BiEncoder using crossencoder negatives
			This time, hard negs for cross-encoder were mined using NSW graph built using distilled biencoder model

				Training on small data 
					sbatch -p rtx8000-long --gres gpu:2 --mem 128GB --job-name distill_2 \
					bin/run.sh python models/train.py \
					--config config/el_zeshel_bi_enc_small_train.json \
					--exp_id 7_EntModel \
					--neg_strategy distill \
					--distill_n_labels 64 \
					--train_batch_size 8 \
					--path_to_model    ../../results/7_EntModel/d=ent_link/m=bi_enc_l=ce_neg=distill_s=1234_distill_w_64_negs_wrt_cross_id_6_82_0_all_data/model/model-3-12318.0-1.92.ckpt  \
					--ent_w_score_file_template ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_nsw_search_s=1234_64_negs_5_bs_10_max_nbrs_500_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/eval_w_exp_id_7_5/{}/m=-1_k=100_nsw_bienc_10_5_500_/crossenc_topk_preds_w_nsw.txt \
					--reload_dataloaders_every_n_epochs 0 \
					--eval_batch_size 32 \
					--warmup_proportion 0.01 \
					--strategy ddp \
					--num_gpus 2 \
					--eval_interval 0.2 \
					--misc distill_w_64_negs_wrt_cross_id_6_82_0_nsw_w_7_5_round_2

				Training on all data
					sbatch -p rtx8000-long --gres gpu:2 --mem 128GB --job-name distill_2 \
					bin/run.sh python models/train.py \
					--config config/el_zeshel_bi_enc.json \
					--exp_id 7_EntModel \
					--neg_strategy distill \
					--distill_n_labels 64 \
					--train_batch_size 8 \
					--path_to_model    ../../results/7_EntModel/d=ent_link/m=bi_enc_l=ce_neg=distill_s=1234_distill_w_64_negs_wrt_cross_id_6_82_0_all_data/model/model-3-12318.0-1.92.ckpt  \
					--ent_w_score_file_template ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_nsw_search_s=1234_64_negs_5_bs_10_max_nbrs_500_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/eval_w_exp_id_7_5/{}/m=-1_k=100_nsw_bienc_10_5_500_/crossenc_topk_preds_w_nsw.txt \
					--reload_dataloaders_every_n_epochs 0 \
					--eval_batch_size 32 \
					--warmup_proportion 0.01 \
					--strategy ddp \
					--num_gpus 2 \
					--eval_interval 0.2 \
					--misc distill_w_64_negs_wrt_cross_id_6_82_0_nsw_w_7_5_all_data_round_2


	23 March 2022

		Training bienc hard negs w/ reranking objective on RTX-8000s

				sbatch -p rtx8000-long --gres gpu:2 --mem 128GB --job-name rerank \
				bin/run.sh python models/train.py \
				--config config/el_zeshel_cross_enc.json \
				--exp_id 6_ReprCrossEnc \
				--pooling_type cls_w_lin \
				--init_num_negs 256 \
				--num_negs 64 \
				--neg_strategy bienc_hard_negs_w_rerank \
				--reload_dataloaders_every_n_epochs 1 \
				--train_batch_size 4 \
				--neg_mine_bienc_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
				--eval_batch_size 30 \
				--ckpt_metric mrr \
				--warmup_proportion 0.01 \
				--strategy ddp \
				--num_gpus 2 \
				--eval_interval 600 \
				--misc 63_from_256_negs_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx


		Training entity model using target label scores

			Debug
				python models/train.py  \
				--config config/el_zeshel_bi_enc_debug.json  \
				--neg_strategy ent_distill  \
				--ent_w_score_file_template ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_nsw_search_s=1234_64_negs_5_bs_10_max_nbrs_500_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/eval/{}/m=-1_k=100_nsw_bienc_10_5_500_/crossenc_topk_preds_w_nsw.txt \
				--train_batch_size 4 \
				--distill_n_labels 4 \
				--ent_distill_pair_method consec \
				--misc ent_model_distill 

		Distillation of entity model only

			sbatch -p rtx8000-long --gres gpu:2 --mem 128GB --job-name ent_distill \
			bin/run.sh python models/train.py \
			--config config/el_zeshel_bi_enc_small_train.json \
			--exp_id 7_EntModel \
			--neg_strategy ent_distill  \
			--ent_distill_pair_method consec \
			--distill_n_labels 64 \
			--train_batch_size 8 \
			--path_to_model ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
			--ent_w_score_file_template ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_nsw_search_s=1234_64_negs_5_bs_10_max_nbrs_500_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/eval/{}/m=-1_k=100_nsw_bienc_10_5_500_/crossenc_topk_preds_w_nsw.txt \
			--reload_dataloaders_every_n_epochs 0 \
			--eval_batch_size 32 \
			--warmup_proportion 0.01 \
			--strategy ddp \
			--num_gpus 2 \
			--eval_interval 0.2 \
			--misc ent_distill_w_64_negs_wrt_cross_id_6_82_0

			sbatch -p rtx8000-long --gres gpu:2 --mem 128GB --job-name ent_distill_all_data \
			bin/run.sh python models/train.py \
			--config config/el_zeshel_bi_enc.json \
			--exp_id 7_EntModel \
			--neg_strategy ent_distill  \
			--ent_distill_pair_method consec \
			--distill_n_labels 64 \
			--train_batch_size 8 \
			--path_to_model ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
			--ent_w_score_file_template ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_nsw_search_s=1234_64_negs_5_bs_10_max_nbrs_500_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/eval/{}/m=-1_k=100_nsw_bienc_10_5_500_/crossenc_topk_preds_w_nsw.txt \
			--reload_dataloaders_every_n_epochs 0 \
			--eval_batch_size 32 \
			--warmup_proportion 0.01 \
			--strategy ddp \
			--num_gpus 2 \
			--eval_interval 0.2 \
			--misc ent_distill_w_64_negs_wrt_cross_id_6_82_0_all_data


			sbatch -p rtx8000-long --gres gpu:2 --mem 128GB --job-name ent_distill \
			bin/run.sh python models/train.py \
			--config config/el_zeshel_bi_enc_small_train.json \
			--exp_id 7_EntModel \
			--neg_strategy ent_distill  \
			--ent_distill_pair_method all_pairs \
			--distill_n_labels 64 \
			--train_batch_size 8 \
			--path_to_model ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
			--ent_w_score_file_template ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_nsw_search_s=1234_64_negs_5_bs_10_max_nbrs_500_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/eval/{}/m=-1_k=100_nsw_bienc_10_5_500_/crossenc_topk_preds_w_nsw.txt \
			--reload_dataloaders_every_n_epochs 0 \
			--eval_batch_size 32 \
			--warmup_proportion 0.01 \
			--strategy ddp \
			--num_gpus 2 \
			--eval_interval 0.2 \
			--misc ent_distill_w_64_negs_wrt_cross_id_6_82_0_all_pairs

	24 March

		Trying out mention-entity score based embedding for entities for building NSW graphs 


	25 March

		Training w/ ranking style loss using knn ranks as training signal
			IMPORTANT - Loss is rank_ce and not just ce, also checkpoint metric is only loss
			Debug
				python models/train.py \
				--config config/el_zeshel_cross_enc_debug.json \
				--num_negs 4 \
				--neg_strategy bienc_hard_negs_w_knn_rank \
				--reload_dataloaders_every_n_epochs 0 \
				--train_batch_size 1 \
				--grad_acc_steps 1 \
				--eval_batch_size 30 \
				--misc debug_hard_negs_w_rank_2 \
				--dist_to_prob_method negate_softmax \
				--loss rank_ce \
				--num_negs 16
			
			Training with neg strategy = bienc_hard_negs_w_knn_rank

				--dist_to_prob_method negate_linear

					sbatch -p rtx8000-long --gres gpu:2 --mem 128GB --job-name neg_w_rank_lin \
					bin/run.sh python models/train.py \
					--config config/el_zeshel_cross_enc.json \
					--exp_id 6_ReprCrossEnc \
					--pooling_type cls_w_lin \
					--loss rank_ce \
					--neg_strategy bienc_hard_negs_w_knn_rank \
					--num_negs 63 \
					--dist_to_prob_method negate_linear \
					--train_batch_size 4 \
					--neg_mine_bienc_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
					--reload_dataloaders_every_n_epochs 0 \
					--eval_batch_size 32 \
					--ckpt_metric loss \
					--warmup_proportion 0.01 \
					--strategy ddp \
					--num_gpus 2 \
					--eval_interval 0.2 \
					--misc 63_negs_w_ddp_w_cls_w_lin_d2p_neg_lin

				[To-Run]--dist_to_prob_method negate_softmax

					sbatch -p rtx8000-long --gres gpu:2 --mem 128GB --job-name neg_w_rank_smax \
					bin/run.sh python models/train.py \
					--config config/el_zeshel_cross_enc.json \
					--exp_id 6_ReprCrossEnc \
					--pooling_type cls_w_lin \
					--loss rank_ce \
					--neg_strategy bienc_hard_negs_w_knn_rank \
					--num_negs 63 \
					--dist_to_prob_method negate_softmax \
					--train_batch_size 4 \
					--neg_mine_bienc_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
					--reload_dataloaders_every_n_epochs 0 \
					--eval_batch_size 32 \
					--ckpt_metric loss \
					--warmup_proportion 0.01 \
					--strategy ddp \
					--num_gpus 2 \
					--eval_interval 0.2 \
					--misc 63_negs_w_ddp_w_cls_w_lin_d2p_neg_smax



		Training with precomputed negs

			Debug
				python models/train.py \
				--config config/el_zeshel_cross_enc_debug.json \
				--num_negs 4 \
				--neg_strategy precomp \
				--reload_dataloaders_every_n_epochs 0 \
				--train_batch_size 1 \
				--grad_acc_steps 1 \
				--eval_batch_size 2 \
				--ent_w_score_file_template ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_nsw_search_s=1234_64_negs_5_bs_10_max_nbrs_500_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/eval/{}/m=-1_k=100_nsw_bienc_10_5_500_/crossenc_topk_preds_w_nsw.txt \
				--loss ce \
				--misc precomp_negs


			Training cross-encoder from 0-last ckpt of 6-82 with negs mined using NSW 
			    -Built on biencoder from 6-20 (standard biencoder used for training crossencoder)
					sbatch -p rtx8000-long --gres gpu:2 --mem 128GB --job-name ce_nsw_7_5_precomp \
					bin/run.sh \
					python models/train.py \
					--config config/el_zeshel_cross_enc.json \
					--exp_id 6_ReprCrossEnc \
					--pooling_type cls_w_lin \
					--num_negs 64 \
					--neg_strategy precomp \
					--reload_dataloaders_every_n_epochs 0 \
					--train_batch_size 4 \
					--ent_w_score_file_template ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_nsw_search_s=1234_64_negs_5_bs_10_max_nbrs_500_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/eval_w_exp_id_7_5/{}/m=-1_k=100_nsw_bienc_10_5_500_/crossenc_topk_preds_w_nsw.txt \
					--path_to_model ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_nsw_search_s=1234_64_negs_5_bs_10_max_nbrs_500_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/model/0-last.ckpt \
					--eval_batch_size 30 \
					--ckpt_metric mrr \
					--warmup_proportion 0.01 \
					--strategy ddp \
					--num_gpus 2 \
					--eval_interval 0.2 \
					--misc 64_negs_from_nsw_w_7_5_bienc

				-Built on distlled biencoder from 7-5. (This is different from using negs from NSW negs built using 6-20 biencoder)

					sbatch -p rtx8000-long --gres gpu:2 --mem 128GB --job-name ce_nsw_6_20_precomp \
					bin/run.sh \
					python models/train.py \
					--config config/el_zeshel_cross_enc.json \
					--exp_id 6_ReprCrossEnc \
					--pooling_type cls_w_lin \
					--num_negs 64 \
					--neg_strategy precomp \
					--reload_dataloaders_every_n_epochs 0 \
					--train_batch_size 4 \
					--ent_w_score_file_template ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_nsw_search_s=1234_64_negs_5_bs_10_max_nbrs_500_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/eval/{}/m=-1_k=100_nsw_bienc_10_5_500_/crossenc_topk_preds_w_nsw.txt \
					--path_to_model ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_nsw_search_s=1234_64_negs_5_bs_10_max_nbrs_500_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/model/0-last.ckpt \
					--eval_batch_size 30 \
					--ckpt_metric mrr \
					--warmup_proportion 0.01 \
					--strategy ddp \
					--num_gpus 2 \
					--eval_interval 0.2 \
					--misc 64_negs_from_nsw_w_6_20_bienc

	26 March
		Tried statistical tests


	27 March

		Distillation of entity model only - without initial softmax over scores before computing diff

			sbatch -p rtx8000-long --gres gpu:2 --mem 128GB --job-name ent_distill \
			bin/run.sh python models/train.py \
			--config config/el_zeshel_bi_enc_small_train.json \
			--exp_id 7_EntModel \
			--neg_strategy ent_distill  \
			--ent_distill_pair_method consec \
			--distill_n_labels 64 \
			--train_batch_size 8 \
			--path_to_model ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
			--ent_w_score_file_template ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_nsw_search_s=1234_64_negs_5_bs_10_max_nbrs_500_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/eval/{}/m=-1_k=100_nsw_bienc_10_5_500_/crossenc_topk_preds_w_nsw.txt \
			--reload_dataloaders_every_n_epochs 0 \
			--eval_batch_size 32 \
			--warmup_proportion 0.01 \
			--strategy ddp \
			--num_gpus 2 \
			--eval_interval 0.2 \
			--misc ent_distill_w_64_negs_wrt_cross_id_6_82_0_wo_smax_before_diff


			sbatch -p rtx8000-long --gres gpu:2 --mem 128GB --job-name ent_distill \
			bin/run.sh python models/train.py \
			--config config/el_zeshel_bi_enc_small_train.json \
			--exp_id _0_Debug \
			--neg_strategy ent_distill  \
			--ent_distill_pair_method consec \
			--distill_n_labels 4 \
			--train_batch_size 8 \
			--path_to_model ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
			--ent_w_score_file_template ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_nsw_search_s=1234_64_negs_5_bs_10_max_nbrs_500_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/eval/{}/m=-1_k=100_nsw_bienc_10_5_500_/crossenc_topk_preds_w_nsw.txt \
			--reload_dataloaders_every_n_epochs 0 \
			--eval_batch_size 32 \
			--warmup_proportion 0.01 \
			--eval_interval 0.2 \
			--misc ent_distill_w_64_negs_wrt_cross_id_6_82_0_wo_smax_before_diff_rtx

		Debugging model interpretability

			python eval/captum_interpret.py \
			--data_name lego \
			--cross_model_file ../../results/6_ReprCrossEnc/_model_interpret/0-last.ckpt \
			--res_dir ../../results/6_ReprCrossEnc/_model_interpret \
			--k 3



	28 March

		Running inference with data from MuVER paper [https://github.com/Alibaba-NLP/MuVER] - with view merging

				sbatch -p rtx8000-long --gres gpu:1 --mem 64GB  --job-name muver_test \
				run.sh \
				python muver/multi_view/train.py  \
				--pretrained_model bert-base-uncased  \
				--dataset_path ../../data/muver \
				--bi_ckpt_path ../../data/muver/MuVER_best_zeshel.bin \
				--max_cand_len 40 \
				--max_seq_len 128 \
				--do_test \
				--test_mode test \
				--eval_batch_size 16 \
				--accumulate_score \
				--view_expansion  \
				--merge_layers 4  

					# sbatch -p gpu --gres gpu:1 --mem 128GB  --job-name muver_train --exclude gpu-0-0 \
					
					# srun -p gpu --gres gpu:2 --mem 100GB  --job-name muver_train --exclude gpu-0-0 --pty \
				sbatch -p rtx8000-long --gres gpu:1 --mem 64GB  --job-name muver_train \
				run.sh \
				python muver/multi_view/train.py  \
				--pretrained_model bert-base-uncased  \
				--dataset_path ../../data/muver \
				--bi_ckpt_path ../../data/muver/MuVER_best_zeshel.bin \
				--max_cand_len 40 \
				--max_seq_len 128 \
				--do_test \
				--test_mode train \
				--eval_batch_size 16 \
				--accumulate_score \
				--view_expansion  \
				--merge_layers 4  

				sbatch -p rtx8000-long --gres gpu:1 --mem 64GB  --job-name muver_valid \
				run.sh \
				python muver/multi_view/train.py  \
				--pretrained_model bert-base-uncased  \
				--dataset_path ../../data/muver \
				--bi_ckpt_path ../../data/muver/MuVER_best_zeshel.bin \
				--max_cand_len 40 \
				--max_seq_len 128 \
				--do_test \
				--test_mode valid \
				--eval_batch_size 16 \
				--accumulate_score \
				--view_expansion  \
				--merge_layers 4  
				    


	29 March

		Training cross-encoder from 0-last ckpt of 6-82 with negs mined using NSW 
			    -Built on distlled biencoder from 7-5. (This is different from using negs from NSW negs built using 6-20 biencoder)
					
					sbatch -p rtx8000-long --gres gpu:2 --mem 128GB --job-name ce_nsw_7_5_precomp \
					bin/run.sh \
					python models/train.py \
					--config config/el_zeshel_cross_enc.json \
					--exp_id 6_ReprCrossEnc \
					--pooling_type cls_w_lin \
					--num_negs 64 \
					--neg_strategy precomp \
					--reload_dataloaders_every_n_epochs 0 \
					--train_batch_size 4 \
					--ent_w_score_file_template ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_nsw_search_s=1234_64_negs_5_bs_10_max_nbrs_500_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/eval_w_exp_id_7_5/{}/m=-1_k=100_nsw_bienc_10_5_500_/crossenc_topk_preds_w_nsw.txt \
					--neg_mine_bienc_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
					--eval_batch_size 30 \
					--ckpt_metric mrr \
					--warmup_proportion 0.01 \
					--strategy ddp \
					--num_gpus 2 \
					--eval_interval 0.2 \
					--misc 64_negs_from_nsw_w_7_5_bienc_from_scratch


					sbatch -p rtx8000-long --gres gpu:2 --mem 128GB --job-name ce_ckpt_nsw_7_5_precomp \
					bin/run.sh \
					python models/train.py \
					--config config/el_zeshel_cross_enc.json \
					--exp_id 6_ReprCrossEnc \
					--pooling_type cls_w_lin \
					--num_negs 64 \
					--neg_strategy precomp \
					--reload_dataloaders_every_n_epochs 0 \
					--train_batch_size 4 \
					--ent_w_score_file_template ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_nsw_search_s=1234_64_negs_5_bs_10_max_nbrs_500_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/eval_w_exp_id_7_5/{}/m=-1_k=100_nsw_bienc_10_5_500_/crossenc_topk_preds_w_nsw.txt \
					--ckpt_path ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_nsw_search_s=1234_64_negs_5_bs_10_max_nbrs_500_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/model/0-last.ckpt \
					--eval_batch_size 30 \
					--ckpt_metric mrr \
					--warmup_proportion 0.01 \
					--strategy ddp \
					--num_gpus 2 \
					--eval_interval 0.2 \
					--misc 64_negs_from_nsw_w_7_5_bienc_from_ckpt

					sbatch -p rtx8000-long --gres gpu:2 --mem 128GB --job-name ce_nsw_7_5_precomp \
					bin/run.sh \
					python models/train.py \
					--config config/el_zeshel_cross_enc.json \
					--exp_id 6_ReprCrossEnc \
					--pooling_type cls_w_lin \
					--num_negs 64 \
					--neg_strategy precomp \
					--reload_dataloaders_every_n_epochs 1 \
					--train_batch_size 4 \
					--ent_w_score_file_template ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_nsw_search_s=1234_64_negs_5_bs_10_max_nbrs_500_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/eval_w_exp_id_7_5/{}/m=-1_k=100_nsw_bienc_10_5_500_/crossenc_topk_preds_w_nsw.txt \
					--neg_mine_bienc_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
					--eval_batch_size 30 \
					--ckpt_metric mrr \
					--warmup_proportion 0.01 \
					--strategy ddp \
					--num_gpus 2 \
					--eval_interval 0.2 \
					--misc 64_negs_from_nsw_w_7_5_bienc_from_scratch_w_reload


					sbatch -p rtx8000-long --gres gpu:2 --mem 128GB --job-name ce_ckpt_nsw_7_5_precomp \
					bin/run.sh \
					python models/train.py \
					--config config/el_zeshel_cross_enc.json \
					--exp_id 6_ReprCrossEnc \
					--pooling_type cls_w_lin \
					--num_negs 64 \
					--neg_strategy precomp \
					--reload_dataloaders_every_n_epochs 1 \
					--train_batch_size 4 \
					--ent_w_score_file_template ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_nsw_search_s=1234_64_negs_5_bs_10_max_nbrs_500_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/eval_w_exp_id_7_5/{}/m=-1_k=100_nsw_bienc_10_5_500_/crossenc_topk_preds_w_nsw.txt \
					--ckpt_path ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_nsw_search_s=1234_64_negs_5_bs_10_max_nbrs_500_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/model/0-last.ckpt \
					--eval_batch_size 30 \
					--ckpt_metric mrr \
					--warmup_proportion 0.01 \
					--strategy ddp \
					--num_gpus 2 \
					--eval_interval 0.2 \
					--misc 64_negs_from_nsw_w_7_5_bienc_from_ckpt_w_reload

				-Built on biencoder from 6-20 (standard biencoder used for training crossencoder)

					sbatch -p rtx8000-long --gres gpu:2 --mem 128GB --job-name ce_nsw_6_20_precomp \
					bin/run.sh \
					python models/train.py \
					--config config/el_zeshel_cross_enc.json \
					--exp_id 6_ReprCrossEnc \
					--pooling_type cls_w_lin \
					--num_negs 64 \
					--neg_strategy precomp \
					--reload_dataloaders_every_n_epochs 1 \
					--train_batch_size 4 \
					--ent_w_score_file_template ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_nsw_search_s=1234_64_negs_5_bs_10_max_nbrs_500_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/eval/{}/m=-1_k=100_nsw_bienc_10_5_500_/crossenc_topk_preds_w_nsw.txt \
					--neg_mine_bienc_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
					--eval_batch_size 30 \
					--ckpt_metric mrr \
					--warmup_proportion 0.01 \
					--strategy ddp \
					--num_gpus 2 \
					--eval_interval 0.2 \
					--misc 64_negs_from_nsw_w_6_20_bienc_from_scratch_w_reload

					sbatch -p rtx8000-long --gres gpu:2 --mem 128GB --job-name ce_ckpt_nsw_7_5_precomp \
					bin/run.sh \
					python models/train.py \
					--config config/el_zeshel_cross_enc.json \
					--exp_id 6_ReprCrossEnc \
					--pooling_type cls_w_lin \
					--num_negs 64 \
					--neg_strategy precomp \
					--reload_dataloaders_every_n_epochs 1 \
					--train_batch_size 4 \
					--ent_w_score_file_template ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_nsw_search_s=1234_64_negs_5_bs_10_max_nbrs_500_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/eval/{}/m=-1_k=100_nsw_bienc_10_5_500_/crossenc_topk_preds_w_nsw.txt \
					--ckpt_path ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_nsw_search_s=1234_64_negs_5_bs_10_max_nbrs_500_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/model/0-last.ckpt \
					--eval_batch_size 30 \
					--ckpt_metric mrr \
					--warmup_proportion 0.01 \
					--strategy ddp \
					--num_gpus 2 \
					--eval_interval 0.2 \
					--misc 64_negs_from_nsw_w_6_20_bienc_from_ckpt_w_reload

				Debug
					python models/train.py \
					--config config/el_zeshel_cross_enc_debug.json \
					--exp_id _0_Debug \
					--pooling_type cls_w_lin \
					--num_negs 4 \
					--neg_strategy precomp \
					--reload_dataloaders_every_n_epochs 0 \
					--train_batch_size 1 \
					--grad_acc_steps 1 \
					--ent_w_score_file_template ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_nsw_search_s=1234_64_negs_5_bs_10_max_nbrs_500_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/eval/{}/m=-1_k=100_nsw_bienc_10_5_500_/crossenc_topk_preds_w_nsw.txt \
					--neg_mine_bienc_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
					--ckpt_path ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_nsw_search_s=1234_64_negs_5_bs_10_max_nbrs_500_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/model/0-last.ckpt \
					--eval_batch_size 10 \
					--ckpt_metric mrr \
					--warmup_proportion 0.01 \
					--eval_interval 0.2 \
					--misc 64_negs_from_nsw_w_6_20_bienc_from_scratch_from_ckpt


		Running matrix computation for full matrix for lego and pro_wrestling domains w/ 0-last ckpt for 63/500 NSW search w/ bs 5 and max_nbrs 10

			sbatch -p m40-long --gres gpu:1 --mem 64GB --job-name mat0-lego bin/run.sh  \
			python eval/run_cross_encoder_for_ment_ent_matrix_zeshel.py \
			--n_ment 1199 \
			--n_ent -1 \
			--batch_size 1000 \
			--data_name lego \
			--layers final \
			--cross_model_ckpt ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_nsw_search_s=1234_64_negs_5_bs_10_max_nbrs_500_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/model/0-last.ckpt \
			--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_nsw_search_s=1234_64_negs_5_bs_10_max_nbrs_500_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/score_mats_0-last.ckpt


			sbatch -p m40-long --gres gpu:1 --mem 64GB --job-name mat0-pro bin/run.sh  \
			python eval/run_cross_encoder_for_ment_ent_matrix_zeshel.py \
			--n_ment 1392 \
			--n_ent -1 \
			--batch_size 1000 \
			--data_name pro_wrestling \
			--layers final \
			--cross_model_ckpt ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_nsw_search_s=1234_64_negs_5_bs_10_max_nbrs_500_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/model/0-last.ckpt \
			--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_nsw_search_s=1234_64_negs_5_bs_10_max_nbrs_500_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/score_mats_0-last.ckpt


	31 March

		Training on 500 BiEncoder negs reranked using crossencoder
			sbatch -p rtx8000-long --gres gpu:2 --mem 128GB --job-name rerank \
			bin/run.sh python models/train.py \
			--config config/el_zeshel_cross_enc.json \
			--exp_id 6_ReprCrossEnc \
			--pooling_type cls_w_lin \
			--init_num_negs 500 \
			--num_negs 63 \
			--neg_strategy bienc_hard_negs_w_rerank \
			--reload_dataloaders_every_n_epochs 1 \
			--train_batch_size 4 \
			--neg_mine_bienc_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
			--eval_batch_size 30 \
			--ckpt_metric mrr \
			--warmup_proportion 0.01 \
			--strategy ddp \
			--num_gpus 2 \
			--eval_interval 0.2 \
			--dump_data 1 \
			--misc 63_from_500_negs_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx


		Training with neg strategy = bienc_hard_negs_w_knn_rank w/ margin based loss this time

		 --dist_to_prob_method negate_linear

			sbatch -p rtx8000-long --gres gpu:2 --mem 128GB --job-name neg_w_rank_margin \
			bin/run.sh python models/train.py \
			--config config/el_zeshel_cross_enc.json \
			--exp_id 6_ReprCrossEnc \
			--pooling_type cls_w_lin \
			--loss rank_margin \
			--neg_strategy bienc_hard_negs_w_knn_rank \
			--num_negs 63 \
			--dist_to_prob_method negate_linear \
			--train_batch_size 4 \
			--neg_mine_bienc_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
			--reload_dataloaders_every_n_epochs 0 \
			--eval_batch_size 32 \
			--ckpt_metric loss \
			--warmup_proportion 0.01 \
			--strategy ddp \
			--num_gpus 2 \
			--eval_interval 0.2 \
			--misc 63_negs


		Running matrix computation for full matrix for lego and pro_wrestling domains

			sbatch -p m40-long --gres gpu:1 --mem 64GB --job-name mat1-lego bin/run.sh  \
			python eval/run_cross_encoder_for_ment_ent_matrix_zeshel.py \
			--n_ment 1199 \
			--n_ent -1 \
			--batch_size 1100 \
			--data_name lego \
			--layers final \
			--cross_model_ckpt ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=rank_ce_neg=bienc_hard_negs_w_knn_rank_s=1234_63_negs_w_ddp_w_cls_w_lin_d2p_neg_lin/model/model-1-12279.0-3.21.ckpt \
			--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=rank_ce_neg=bienc_hard_negs_w_knn_rank_s=1234_63_negs_w_ddp_w_cls_w_lin_d2p_neg_lin/score_mats_model-1-12279.0-3.21.ckpt


			sbatch -p m40-long --gres gpu:1 --mem 64GB --job-name mat1-pro bin/run.sh  \
			python eval/run_cross_encoder_for_ment_ent_matrix_zeshel.py \
			--n_ment 1392 \
			--n_ent -1 \
			--batch_size 1100 \
			--data_name pro_wrestling \
			--layers final \
			--cross_model_ckpt ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=rank_ce_neg=bienc_hard_negs_w_knn_rank_s=1234_63_negs_w_ddp_w_cls_w_lin_d2p_neg_lin/model/model-1-12279.0-3.21.ckpt \
			--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=rank_ce_neg=bienc_hard_negs_w_knn_rank_s=1234_63_negs_w_ddp_w_cls_w_lin_d2p_neg_lin/score_mats_model-1-12279.0-3.21.ckpt


April 2022

	1 April

		Training on negatives mined using MUVER model with view merge 

			sbatch -p rtx8000-long --gres gpu:2 --mem 128GB --job-name muver_all \
			bin/run.sh \
			python models/train.py \
			--config config/el_zeshel_cross_enc.json \
			--exp_id 6_ReprCrossEnc \
			--num_negs 63 \
			--neg_strategy precomp \
			--reload_dataloaders_every_n_epochs 0 \
			--train_batch_size 4 \
			--ent_w_score_file_template ../../data/muver/topk_ents_w_view_merge/{}_topk_ents.json \
			--eval_batch_size 30 \
			--ckpt_metric mrr \
			--warmup_proportion 0.01 \
			--strategy ddp \
			--num_gpus 2 \
			--eval_interval 0.2 \
			--misc 63_negs_from_muver_w_view_merge

			sbatch -p rtx8000-long --gres gpu:2 --mem 128GB --job-name muver_small \
			bin/run.sh \
			python models/train.py \
			--config config/el_zeshel_cross_enc_small_train.json \
			--exp_id 6_ReprCrossEnc \
			--num_negs 63 \
			--neg_strategy precomp \
			--reload_dataloaders_every_n_epochs 0 \
			--train_batch_size 4 \
			--ent_w_score_file_template ../../data/muver/topk_ents_w_view_merge/{}_topk_ents.json \
			--eval_batch_size 30 \
			--ckpt_metric mrr \
			--warmup_proportion 0.01 \
			--strategy ddp \
			--num_gpus 2 \
			--eval_interval 0.2 \
			--misc 63_negs_from_muver_w_view_merge


		Training entity model only w/ score distribution from rank_ce loss w/ knn ranking model

			sbatch -p m40-long --gres gpu:4 --mem 128GB --job-name ent_distill \
			bin/run.sh python models/train.py \
			--config config/el_zeshel_bi_enc_small_train.json \
			--exp_id 7_EntModel \
			--neg_strategy ent_distill  \
			--ent_distill_pair_method consec \
			--distill_n_labels 32 \
			--train_batch_size 8 \
			--path_to_model ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
			--ent_w_score_file_template ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=rank_ce_neg=bienc_hard_negs_w_knn_rank_s=1234_63_negs_w_ddp_w_cls_w_lin_d2p_neg_lin/eval/{}/m=-1_k=100_nsw_bienc_10_5_500_/crossenc_topk_preds_w_nsw.txt \
			--reload_dataloaders_every_n_epochs 0 \
			--eval_batch_size 32 \
			--warmup_proportion 0.01 \
			--strategy ddp \
			--num_gpus 2 \
			--eval_interval 0.2 \
			--misc ent_distill_w_32_negs_wrt_cross_id_6_93_0


		Computing entity-entity scores using mention-entity crossencoder (6-82)

			sbatch -p gpu --gres gpu:1 --mem 64GB --exclude gpu-0-0 --job-name ent_ent_6_82_pro \
			bin/run.sh \
			python eval/run_cross_encoder_for_ent_ent_matrix.py \
			--data_name pro_wrestling \
			--n_ent_x -1 \
			--n_ent_y -1 \
			--topk 100 \
			--embed_type anchor \
			--batch_size 1200 \
			--res_dir          ../../results/6_ReprCrossEnc/d\=ent_link/m\=cross_enc_l\=ce_neg\=bienc_nsw_search_s\=1234_64_negs_5_bs_10_max_nbrs_500_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/score_mats_0-last.ckpt \
			--cross_model_file ../../results/6_ReprCrossEnc/d\=ent_link/m\=cross_enc_l\=ce_neg\=bienc_nsw_search_s\=1234_64_negs_5_bs_10_max_nbrs_500_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/model/0-last.ckpt \
			--bi_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt 

	2 April

		Training entity model only w/ score distribution from rank_ce loss w/ knn ranking model

			sbatch -p m40-long --gres gpu:4 --mem 128GB --job-name distill \
			bin/run.sh python models/train.py \
			--config config/el_zeshel_bi_enc_small_train.json \
			--exp_id 7_EntModel \
			--neg_strategy distill  \
			--ent_distill_pair_method consec \
			--distill_n_labels 32 \
			--train_batch_size 8 \
			--path_to_model ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
			--ent_w_score_file_template ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=rank_ce_neg=bienc_hard_negs_w_knn_rank_s=1234_63_negs_w_ddp_w_cls_w_lin_d2p_neg_lin/eval/{}/m=-1_k=100_nsw_bienc_10_5_500_/crossenc_topk_preds_w_nsw.txt \
			--reload_dataloaders_every_n_epochs 0 \
			--eval_batch_size 32 \
			--warmup_proportion 0.01 \
			--strategy ddp \
			--num_gpus 2 \
			--eval_interval 0.2 \
			--misc distill_w_32_negs_wrt_cross_id_6_93_0



		Running crossencoder robustness evaluation

			python eval/run_crossenc_robustness_eval.py \
			--data_name lego \
			--n_ment 5 \
			--top_k 64 \
			--batch_size 1 \
			--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
			--res_dir          ../../results/6_ReprCrossEnc/_robust_eval_w_ment_bndry/eval \
			--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_nsw_search_s=1234_64_negs_5_bs_10_max_nbrs_500_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/model/model-1-10959.0--78.85.ckpt \
			--misc 6_82_model-1-10959.0--78.85.ckpt

			python eval/run_crossenc_robustness_eval.py \
			--data_name lego \
			--n_ment 5 \
			--top_k 64 \
			--batch_size 1 \
			--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
			--res_dir          ../../results/6_ReprCrossEnc/_robust_eval_w_ment_bndry/eval \
			--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_nsw_search_s=1234_64_negs_5_bs_10_max_nbrs_500_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/model/model-1-10959.0--78.85.ckpt \
			--misc 6_82_model-1-10959.0--78.85.ckpt_cross_enc_only_perturb

	3 April
		
		Training entity model only w/ score distribution from rank_ce loss w/ knn ranking model - w/ self.config.ent_distill_pair_sim == "prod":

			sbatch -p m40-long --gres gpu:4 --mem 128GB --job-name ent_distill \
			bin/run.sh python models/train.py \
			--config config/el_zeshel_bi_enc_small_train.json \
			--exp_id 7_EntModel \
			--neg_strategy ent_distill  \
			--ent_distill_pair_method consec \
			--distill_n_labels 32 \
			--train_batch_size 8 \
			--path_to_model ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
			--ent_w_score_file_template ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=rank_ce_neg=bienc_hard_negs_w_knn_rank_s=1234_63_negs_w_ddp_w_cls_w_lin_d2p_neg_lin/eval/{}/m=-1_k=100_nsw_bienc_10_5_500_/crossenc_topk_preds_w_nsw.txt \
			--reload_dataloaders_every_n_epochs 0 \
			--eval_batch_size 32 \
			--warmup_proportion 0.01 \
			--strategy ddp \
			--num_gpus 2 \
			--eval_interval 0.2 \
			--ent_distill_pair_sim prod \
			--misc ent_distill_w_32_negs_wrt_cross_id_6_93_0_prod_pair_sim


		Training w/ distilled biencoder reranked negatives - 500 negs reranked

			sbatch -p rtx8000-long --gres gpu:2 --mem 128GB --job-name rerank \
			bin/run.sh python models/train.py \
			--config config/el_zeshel_cross_enc.json \
			--exp_id 6_ReprCrossEnc \
			--pooling_type cls_w_lin \
			--init_num_negs 500 \
			--num_negs 63 \
			--neg_strategy bienc_hard_negs_w_rerank \
			--reload_dataloaders_every_n_epochs 1 \
			--train_batch_size 4 \
			--neg_mine_bienc_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
			--eval_batch_size 30 \
			--ckpt_metric mrr \
			--warmup_proportion 0.01 \
			--strategy ddp \
			--num_gpus 2 \
			--eval_interval 0.2 \
			--dump_data 1 \
			--misc 63_from_500_negs_w_distilled_bienc_7_5_ep_1

			sbatch -p rtx8000-long --gres gpu:2 --mem 128GB --job-name rerank \
			bin/run.sh python models/train.py \
			--config config/el_zeshel_cross_enc.json \
			--exp_id 6_ReprCrossEnc \
			--pooling_type cls_w_lin \
			--init_num_negs 500 \
			--num_negs 63 \
			--neg_strategy bienc_hard_negs_w_rerank \
			--reload_dataloaders_every_n_epochs 1 \
			--train_batch_size 4 \
			--neg_mine_bienc_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
			--ckpt_path ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_w_rerank_s=1234_63_from_500_negs_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/model/0-last.ckpt \
			--eval_batch_size 30 \
			--ckpt_metric mrr \
			--warmup_proportion 0.01 \
			--strategy ddp \
			--num_gpus 2 \
			--eval_interval 0.2 \
			--dump_data 1 \
			--misc 63_from_500_negs_w_distilled_bienc_7_5_ep_1_from_6_115_0_last

	5 April

		Computing entity2entity scores using mention-entity encoder - but with first entity in the pair tokenized as a mention with ent_start and ent_end tokens 
			for best model wrt 6_82 5-10-500 NSW bienc search
			sbatch -p m40-long --gres gpu:1 --mem 64GB --job-name e2e_lego_bienc \
			bin/run.sh \
			python eval/run_cross_encoder_for_ent_ent_matrix.py \
			--data_name lego \
			--n_ent_x -1 \
			--n_ent_y -1 \
			--topk 100 \
			--embed_type bienc \
			--token_opt m2e \
			--batch_size 600 \
			--res_dir ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_nsw_search_s=1234_64_negs_5_bs_10_max_nbrs_500_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/score_mats_0-last.ckpt \
			--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_nsw_search_s=1234_64_negs_5_bs_10_max_nbrs_500_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/model/0-last.ckpt \
			--bi_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt


			sbatch -p m40-long --gres gpu:1 --mem 64GB --job-name e2e_lego_anchor \
			bin/run.sh \
			python eval/run_cross_encoder_for_ent_ent_matrix.py \
			--data_name lego \
			--n_ent_x -1 \
			--n_ent_y -1 \
			--topk 100 \
			--embed_type anchor \
			--token_opt m2e \
			--batch_size 600 \
			--res_dir ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_nsw_search_s=1234_64_negs_5_bs_10_max_nbrs_500_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/score_mats_0-last.ckpt \
			--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_nsw_search_s=1234_64_negs_5_bs_10_max_nbrs_500_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/model/0-last.ckpt \
			--bi_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt


			sbatch -p m40-long --gres gpu:1 --mem 64GB --job-name e2e_pro_bienc \
			bin/run.sh \
			python eval/run_cross_encoder_for_ent_ent_matrix.py \
			--data_name pro_wrestling \
			--n_ent_x -1 \
			--n_ent_y -1 \
			--topk 100 \
			--embed_type bienc \
			--token_opt m2e \
			--batch_size 600 \
			--res_dir ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_nsw_search_s=1234_64_negs_5_bs_10_max_nbrs_500_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/score_mats_0-last.ckpt \
			--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_nsw_search_s=1234_64_negs_5_bs_10_max_nbrs_500_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/model/0-last.ckpt \
			--bi_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt


			sbatch -p m40-long --gres gpu:1 --mem 64GB --job-name e2e_pro_anchor \
			bin/run.sh \
			python eval/run_cross_encoder_for_ent_ent_matrix.py \
			--data_name pro_wrestling \
			--n_ent_x -1 \
			--n_ent_y -1 \
			--topk 100 \
			--embed_type anchor \
			--token_opt m2e \
			--batch_size 600 \
			--res_dir ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_nsw_search_s=1234_64_negs_5_bs_10_max_nbrs_500_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/score_mats_0-last.ckpt \
			--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_nsw_search_s=1234_64_negs_5_bs_10_max_nbrs_500_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/model/0-last.ckpt \
			--bi_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt

		Training an entity-entity model with 6-117 rank_margin w/ KNN rank model
			sbatch -p m40-long --gres gpu:4 --mem 128GB --job-name ent_distill \
			bin/run.sh python models/train.py \
			--config config/el_zeshel_bi_enc_small_train.json \
			--exp_id 7_EntModel \
			--neg_strategy ent_distill  \
			--ent_distill_pair_method consec \
			--distill_n_labels 32 \
			--train_batch_size 8 \
			--path_to_model ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
			--ent_w_score_file_template ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=rank_ce_neg=bienc_hard_negs_w_knn_rank_s=1234_63_negs_w_ddp_w_cls_w_lin_d2p_neg_lin/eval/{}/m=-1_k=100_nsw_bienc_10_5_500_/crossenc_topk_preds_w_nsw.txt \
			--reload_dataloaders_every_n_epochs 0 \
			--eval_batch_size 32 \
			--warmup_proportion 0.01 \
			--strategy ddp \
			--num_gpus 4 \
			--eval_interval 0.2 \
			--misc w_32_negs_wrt_cross_id_6_117


			- First fix the issue that crossenc gives lower score to better entities 
			
			[To-Run] sbatch -p m40-long --gres gpu:4 --mem 128GB --job-name ent_distill \
			bin/run.sh python models/train.py \
			--config config/el_zeshel_bi_enc_small_train.json \
			--exp_id 7_EntModel \
			--neg_strategy distill  \
			--ent_distill_pair_method consec \
			--distill_n_labels 32 \
			--train_batch_size 8 \
			--path_to_model ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
			--ent_w_score_file_template ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=rank_ce_neg=bienc_hard_negs_w_knn_rank_s=1234_63_negs_w_ddp_w_cls_w_lin_d2p_neg_lin/eval/{}/m=-1_k=100_nsw_bienc_10_5_500_/crossenc_topk_preds_w_nsw.txt \
			--reload_dataloaders_every_n_epochs 0 \
			--eval_batch_size 32 \
			--warmup_proportion 0.01 \
			--strategy ddp \
			--num_gpus 4 \
			--eval_interval 0.2 \
			--misc w_32_negs_wrt_cross_id_6_117


			sbatch -p m40-long --gres gpu:2 --mem 128GB --job-name ent_mse_distill \
			bin/run.sh \
			python models/train.py \
			--config config/el_zeshel_bi_enc_small_train.json \
			--exp_id 7_EntModel \
			--loss_type mse \
			--neg_strategy ent_distill  \
			--ent_distill_pair_method consec \
			--distill_n_labels 32 \
			--train_batch_size 8 \
			--path_to_model ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
			--ent_w_score_file_template ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=rank_ce_neg=bienc_hard_negs_w_knn_rank_s=1234_63_negs_w_ddp_w_cls_w_lin_d2p_neg_lin/eval/{}/m=-1_k=100_nsw_bienc_10_5_500_/crossenc_topk_preds_w_nsw.txt \
			--reload_dataloaders_every_n_epochs 0 \
			--eval_batch_size 32 \
			--warmup_proportion 0.01 \
			--strategy ddp \
			--num_gpus 2 \
			--eval_interval 0.2 \
			--misc w_32_negs_wrt_cross_id_6_117



			python models/train.py \
			--config config/el_zeshel_bi_enc_debug.json \
			--exp_id _0_Debug \
			--loss_type mse \
			--neg_strategy ent_distill  \
			--ent_distill_pair_method consec \
			--distill_n_labels 4 \
			--train_batch_size 4 \
			--path_to_model ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
			--ent_w_score_file_template ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=rank_ce_neg=bienc_hard_negs_w_knn_rank_s=1234_63_negs_w_ddp_w_cls_w_lin_d2p_neg_lin/eval/{}/m=-1_k=100_nsw_bienc_10_5_500_/crossenc_topk_preds_w_nsw.txt \
			--reload_dataloaders_every_n_epochs 0 \
			--eval_batch_size 32 \
			--warmup_proportion 0.01 \
			--num_gpus 1 \
			--eval_interval 0.2 \
			--misc debug_w_32_negs_wrt_cross_id_6_117

	7 April 


		Training NSW search and bienc reranking exp on smaller subset of training data

			sbatch -p rtx8000-long --gres gpu:2 --mem 128GB --job-name small_binsw \
			bin/run.sh \
			python models/train.py \
			--config config/el_zeshel_cross_enc_small_train.json \
			--exp_id 6_ReprCrossEnc \
			--pooling_type cls_w_lin \
			--num_negs 63 \
			--neg_strategy bienc_nsw_search \
			--nsw_comp_budget 500 \
			--nsw_beamsize 5 \
			--nsw_max_nbrs 10 \
			--reload_dataloaders_every_n_epochs 1 \
			--train_batch_size 4 \
			--neg_mine_bienc_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
			--eval_batch_size 32 \
			--ckpt_metric mrr \
			--warmup_proportion 0.01 \
			--strategy ddp \
			--num_gpus 2 \
			--eval_interval 0.2 \
			--misc 63_negs_5_bs_10_max_nbrs_500_budget_small_train

			sbatch -p rtx8000-long --gres gpu:2 --mem 128GB --job-name small_birerank \
			bin/run.sh \
			python models/train.py \
			--config config/el_zeshel_cross_enc_small_train.json \
			--exp_id 6_ReprCrossEnc \
			--pooling_type cls_w_lin \
			--init_num_negs 500 \
			--num_negs 63 \
			--neg_strategy bienc_hard_negs_w_rerank \
			--reload_dataloaders_every_n_epochs 1 \
			--train_batch_size 4 \
			--neg_mine_bienc_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
			--eval_batch_size 30 \
			--ckpt_metric mrr \
			--warmup_proportion 0.01 \
			--strategy ddp \
			--num_gpus 2 \
			--eval_interval 0.2 \
			--dump_data 1 \
			--misc 63_from_500_negs_small_train


		Training a crossencoder by distilling information from a biencoder

			Debug
				python models/train.py \
				--config config/el_zeshel_cross_enc_one_domain.json \
				--exp_id _0_Debug \
				--loss_type mse \
				--neg_strategy bienc_distill \
				--distill_n_labels 8 \
				--reload_dataloaders_every_n_epochs 0 \
				--train_batch_size 1 \
				--grad_acc_steps 1 \
				--neg_mine_bienc_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
				--eval_batch_size 32 \
				--warmup_proportion 0.01 \
				--eval_interval 0.9 \
				--misc debug_bi_to_cross_distill


			Running on pro_wrestling only
				sbatch -p m40-long --gres gpu:2 --mem 128GB --job-name b2c_pro \
				bin/run.sh \
				python models/train.py \
				--config config/el_zeshel_cross_enc_one_domain.json \
				--exp_id 6_ReprCrossEnc \
				--neg_strategy bienc_distill \
				--distill_n_labels 64 \
				--num_neg_splits 4 \
				--train_batch_size 4 \
				--neg_mine_bienc_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
				--eval_batch_size 32 \
				--warmup_proportion 0.01 \
				--eval_interval 0.9 \
				--strategy ddp \
				--num_gpus 2 \
				--num_epochs 50 \
				--misc trn_pro_only

				sbatch -p m40-long --gres gpu:2 --mem 128GB --job-name b2c_pro2 \
				bin/run.sh \
				python models/train.py \
				--config config/el_zeshel_cross_enc_one_domain.json \
				--exp_id 6_ReprCrossEnc \
				--neg_strategy bienc_distill \
				--distill_n_labels 64 \
				--num_neg_splits 4 \
				--train_batch_size 4 \
				--neg_mine_bienc_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
				--path_to_model ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_w_bienc_w_ddp_w_best_wrt_dev_mrr_cls_w_lin/model/model-1-11359.0--80.19.ckpt \
				--eval_batch_size 32 \
				--warmup_proportion 0.01 \
				--eval_interval 0.9 \
				--strategy ddp \
				--num_gpus 2 \
				--num_epochs 50 \
				--misc trn_pro_only_w_6_49_init

			Running on small_subset of trainign data
				sbatch -p m40-long --gres gpu:4 --mem 128GB --job-name bi_to_cross_small \
				bin/run.sh \
				python models/train.py \
				--config config/el_zeshel_cross_enc_small_train.json \
				--exp_id 6_ReprCrossEnc \
				--neg_strategy bienc_distill \
				--distill_n_labels 64 \
				--num_neg_splits 4 \
				--train_batch_size 4 \
				--neg_mine_bienc_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
				--eval_batch_size 32 \
				--warmup_proportion 0.01 \
				--eval_interval 0.2 \
				--strategy ddp \
				--num_gpus 4 \
				--misc small_train

	8 April

		Running on pro_wrestling only w/ MSE loss
			sbatch -p m40-long --gres gpu:2 --mem 128GB --job-name b2c_mse_pro \
			bin/run.sh \
			python models/train.py \
			--config config/el_zeshel_cross_enc_one_domain.json \
			--exp_id 6_ReprCrossEnc \
			--loss_type mse \
			--neg_strategy bienc_distill \
			--distill_n_labels 64 \
			--num_neg_splits 4 \
			--train_batch_size 4 \
			--neg_mine_bienc_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
			--eval_batch_size 32 \
			--warmup_proportion 0.01 \
			--eval_interval 0.9 \
			--strategy ddp \
			--num_gpus 2 \
			--num_epochs 50 \
			--misc trn_pro_only

			sbatch -p m40-long --gres gpu:2 --mem 128GB --job-name b2c_mse_pro2 \
			bin/run.sh \
			python models/train.py \
			--config config/el_zeshel_cross_enc_one_domain.json \
			--exp_id 6_ReprCrossEnc \
			--loss_type mse \
			--neg_strategy bienc_distill \
			--distill_n_labels 64 \
			--num_neg_splits 4 \
			--train_batch_size 4 \
			--neg_mine_bienc_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
			--path_to_model ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_w_bienc_w_ddp_w_best_wrt_dev_mrr_cls_w_lin/model/model-1-11359.0--80.19.ckpt \
			--eval_batch_size 32 \
			--warmup_proportion 0.01 \
			--eval_interval 0.9 \
			--strategy ddp \
			--num_gpus 2 \
			--num_epochs 50 \
			--misc trn_pro_only_w_6_49_init

	9 April
			
			Analyzing NSW graph for search
				python eval/analyze_nsw_graph.py \
				--res_dir ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_nsw_search_s=1234_64_negs_5_bs_10_max_nbrs_500_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/score_mats_model-1-10959.0--78.85.ckpt \
				--data_name lego \
				--embed_type bienc
			
			Debugging graph search over KNN graph

				sbatch -p gpu --gres gpu:1 --mem 32GB --job-name b2c_mse_pro2 --exclude gpu-0-0 bin/run.sh \
				python eval/nsw_eval_zeshel.py \
				--data_name lego \
				--embed_type anchor \
				--nsw_metric inner_prod \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--res_dir          ../../results/6_ReprCrossEnc/_knn_debug \
				--graph_type knn 

	10 April

			Debugging graph search w/ graph built using entity-entity scores from crossencoder

				KNN graph using e2e scores
					sbatch -p gpu --gres gpu:1 --mem 32GB --job-name b2c_mse_pro2 --exclude gpu-0-0 bin/run.sh \
					python eval/nsw_eval_zeshel.py \
					--data_name lego \
					--embed_type tfidf \
					--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
					--res_dir          ../../results/6_ReprCrossEnc/_knn_debug \
					--graph_type knn_e2e  \
					--misc try_2


					sbatch -p gpu --gres gpu:1 --mem 32GB --job-name b2c_mse_pro2 --exclude gpu-0-0 bin/run.sh \
					python eval/nsw_eval_zeshel.py \
					--data_name lego \
					--embed_type bienc \
					--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
					--res_dir          ../../results/6_ReprCrossEnc/_knn_debug \
					--graph_type knn_e2e \
					--misc try_2

				NSW
					inner_prod
						sbatch -p gpu --gres gpu:1 --mem 32GB --job-name nsw_tfidf --exclude gpu-0-0 bin/run.sh \
						python eval/nsw_eval_zeshel.py \
						--data_name lego \
						--embed_type tfidf \
						--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
						--res_dir          ../../results/6_ReprCrossEnc/_knn_debug \
						--graph_type nsw \
						--nsw_metric inner_prod \
						--misc inner_prod


						sbatch -p gpu --gres gpu:1 --mem 32GB --job-name nsw_bienc --exclude gpu-0-0 bin/run.sh \
						python eval/nsw_eval_zeshel.py \
						--data_name lego \
						--embed_type bienc \
						--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
						--res_dir          ../../results/6_ReprCrossEnc/_knn_debug \
						--graph_type nsw \
						--nsw_metric inner_prod \
						--misc inner_prod

						sbatch -p gpu --gres gpu:1 --mem 32GB --job-name nsw_anchor --exclude gpu-0-0 bin/run.sh \
						python eval/nsw_eval_zeshel.py \
						--data_name lego \
						--embed_type anchor \
						--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
						--res_dir          ../../results/6_ReprCrossEnc/_knn_debug \
						--graph_type nsw  \
						--nsw_metric inner_prod \
						--misc inner_prod

					l2
						sbatch -p gpu --gres gpu:1 --mem 32GB --job-name nsw_tfidf --exclude gpu-0-0 bin/run.sh \
						python eval/nsw_eval_zeshel.py \
						--data_name lego \
						--embed_type tfidf \
						--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
						--res_dir          ../../results/6_ReprCrossEnc/_knn_debug \
						--graph_type nsw \
						--nsw_metric l2 \
						--misc l2


						sbatch -p gpu --gres gpu:1 --mem 32GB --job-name nsw_bienc --exclude gpu-0-0 bin/run.sh \
						python eval/nsw_eval_zeshel.py \
						--data_name lego \
						--embed_type bienc \
						--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
						--res_dir          ../../results/6_ReprCrossEnc/_knn_debug \
						--graph_type nsw \
						--nsw_metric l2 \
						--misc l2

						sbatch -p gpu --gres gpu:1 --mem 32GB --job-name nsw_anchor --exclude gpu-0-0 bin/run.sh \
						python eval/nsw_eval_zeshel.py \
						--data_name lego \
						--embed_type anchor \
						--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
						--res_dir          ../../results/6_ReprCrossEnc/_knn_debug \
						--graph_type nsw  \
						--nsw_metric l2 \
						--misc l2

				KNN
					sbatch -p gpu --gres gpu:1 --mem 32GB --job-name knn_tfidf_inner_prod --exclude gpu-0-0 bin/run.sh \
					python eval/nsw_eval_zeshel.py \
					--data_name lego \
					--embed_type tfidf \
					--nsw_metric inner_prod \
					--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
					--res_dir          ../../results/6_ReprCrossEnc/_knn_debug \
					--graph_type knn \
					--misc inner_prod

					sbatch -p gpu --gres gpu:1 --mem 32GB --job-name knn_bienc_inner_prod --exclude gpu-0-0 bin/run.sh \
					python eval/nsw_eval_zeshel.py \
					--data_name lego \
					--embed_type bienc \
					--nsw_metric inner_prod \
					--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
					--res_dir          ../../results/6_ReprCrossEnc/_knn_debug \
					--graph_type knn \
					--misc inner_prod

					sbatch -p gpu --gres gpu:1 --mem 32GB --job-name knn_anchor_inner_prod --exclude gpu-0-0 bin/run.sh \
					python eval/nsw_eval_zeshel.py \
					--data_name lego \
					--embed_type anchor \
					--nsw_metric inner_prod \
					--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
					--res_dir          ../../results/6_ReprCrossEnc/_knn_debug \
					--graph_type knn  \
					--misc inner_prod




			Training on small data with distilled BiEncoder

				sbatch -p rtx8000-long --gres gpu:2 --mem 128GB --job-name small_binsw \
				bin/run.sh \
				python models/train.py \
				--config config/el_zeshel_cross_enc_small_train.json \
				--exp_id 6_ReprCrossEnc \
				--pooling_type cls_w_lin \
				--num_negs 63 \
				--neg_strategy bienc_nsw_search \
				--nsw_comp_budget 500 \
				--nsw_beamsize 5 \
				--nsw_max_nbrs 10 \
				--reload_dataloaders_every_n_epochs 1 \
				--train_batch_size 4 \
				--neg_mine_bienc_model_file ../../results/7_EntModel/d=ent_link/m=bi_enc_l=ce_neg=distill_s=1234_distill_w_64_negs_wrt_cross_id_6_82_0_all_data/model/model-3-12318.0-1.92.ckpt \
				--eval_batch_size 32 \
				--ckpt_metric mrr \
				--warmup_proportion 0.01 \
				--strategy ddp \
				--num_gpus 2 \
				--eval_interval 0.2 \
				--misc 63_negs_5_bs_10_max_nbrs_500_budget_w_distilled_bienc_7_5_small_train

				sbatch -p rtx8000-long --gres gpu:2 --mem 128GB --job-name small_birerank \
				bin/run.sh \
				python models/train.py \
				--config config/el_zeshel_cross_enc_small_train.json \
				--exp_id 6_ReprCrossEnc \
				--pooling_type cls_w_lin \
				--init_num_negs 500 \
				--num_negs 63 \
				--neg_strategy bienc_hard_negs_w_rerank \
				--reload_dataloaders_every_n_epochs 1 \
				--train_batch_size 4 \
				--neg_mine_bienc_model_file ../../results/7_EntModel/d=ent_link/m=bi_enc_l=ce_neg=distill_s=1234_distill_w_64_negs_wrt_cross_id_6_82_0_all_data/model/model-3-12318.0-1.92.ckpt \
				--eval_batch_size 30 \
				--ckpt_metric mrr \
				--warmup_proportion 0.01 \
				--strategy ddp \
				--num_gpus 2 \
				--eval_interval 0.2 \
				--dump_data 1 \
				--misc 63_from_500_negs_w_distilled_bienc_7_5_small_train

	11 April

		Running entity-entity score computation using cross-encoder
			sbatch -p m40-long --gres gpu:1 --mem 64GB --job-name e2e_af_anchor \
			bin/run.sh \
			python eval/run_cross_encoder_for_ent_ent_matrix.py \
			--data_name american_football \
			--n_ent_x -1 \
			--n_ent_y -1 \
			--topk 100 \
			--embed_type anchor \
			--token_opt m2e \
			--batch_size 600 \
			--res_dir ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_nsw_search_s=1234_64_negs_5_bs_10_max_nbrs_500_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/score_mats_0-last.ckpt \
			--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_nsw_search_s=1234_64_negs_5_bs_10_max_nbrs_500_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/model/0-last.ckpt \
			--bi_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt

			sbatch -p m40-long --gres gpu:1 --mem 64GB --job-name e2e_af_bienc \
			bin/run.sh \
			python eval/run_cross_encoder_for_ent_ent_matrix.py \
			--data_name american_football \
			--n_ent_x -1 \
			--n_ent_y -1 \
			--topk 100 \
			--embed_type bienc \
			--token_opt m2e \
			--batch_size 600 \
			--res_dir ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_nsw_search_s=1234_64_negs_5_bs_10_max_nbrs_500_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/score_mats_0-last.ckpt \
			--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_nsw_search_s=1234_64_negs_5_bs_10_max_nbrs_500_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/model/0-last.ckpt \
			--bi_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt


			sbatch -p m40-long --gres gpu:1 --mem 64GB --job-name e2e_dw_anchor \
			bin/run.sh \
			python eval/run_cross_encoder_for_ent_ent_matrix.py \
			--data_name doctor_who \
			--n_ent_x -1 \
			--n_ent_y -1 \
			--topk 100 \
			--embed_type anchor \
			--token_opt m2e \
			--batch_size 600 \
			--res_dir ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_nsw_search_s=1234_64_negs_5_bs_10_max_nbrs_500_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/score_mats_0-last.ckpt \
			--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_nsw_search_s=1234_64_negs_5_bs_10_max_nbrs_500_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/model/0-last.ckpt \
			--bi_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt

			sbatch -p m40-long --gres gpu:1 --mem 64GB --job-name e2e_dw_bienc \
			bin/run.sh \
			python eval/run_cross_encoder_for_ent_ent_matrix.py \
			--data_name doctor_who \
			--n_ent_x -1 \
			--n_ent_y -1 \
			--topk 100 \
			--embed_type bienc \
			--token_opt m2e \
			--batch_size 600 \
			--res_dir ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_nsw_search_s=1234_64_negs_5_bs_10_max_nbrs_500_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/score_mats_0-last.ckpt \
			--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_nsw_search_s=1234_64_negs_5_bs_10_max_nbrs_500_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/model/0-last.ckpt \
			--bi_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt

	13 April

		[Did not run] Training w/ distilled bienc hard negs w rerank - initializing model using 0-last.ckpt from 6-82 
			sbatch -p rtx8000-long --gres gpu:2 --mem 128GB --job-name rerank_from_ckpt \
			bin/run.sh python models/train.py \
			--config config/el_zeshel_cross_enc.json \
			--exp_id 6_ReprCrossEnc \
			--pooling_type cls_w_lin \
			--init_num_negs 500 \
			--num_negs 63 \
			--neg_strategy bienc_hard_negs_w_rerank \
			--reload_dataloaders_every_n_epochs 1 \
			--train_batch_size 4 \
			--neg_mine_bienc_model_file ../../results/7_EntModel/d=ent_link/m=bi_enc_l=ce_neg=distill_s=1234_distill_w_64_negs_wrt_cross_id_6_82_0_all_data/model/model-3-12318.0-1.92.ckpt \
			--path_to_model ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_nsw_search_s=1234_64_negs_5_bs_10_max_nbrs_500_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/model/0-last.ckpt \
			--eval_batch_size 30 \
			--ckpt_metric mrr \
			--warmup_proportion 0.01 \
			--strategy ddp \
			--num_gpus 2 \
			--eval_interval 0.2 \
			--dump_data 1 \
			--misc 63_from_500_negs_w_distilled_bienc_7_5_ep_1_from_6_82_0_last

		Training w/ bienc hard negs w rerank - initializing model using best baseline crossencoder model
			sbatch -p rtx8000-long --gres gpu:2 --mem 128GB --job-name rerank_from_best_baseline \
			bin/run.sh python models/train.py \
			--config config/el_zeshel_cross_enc.json \
			--exp_id 6_ReprCrossEnc \
			--pooling_type cls_w_lin \
			--init_num_negs 500 \
			--num_negs 63 \
			--neg_strategy bienc_hard_negs_w_rerank \
			--reload_dataloaders_every_n_epochs 1 \
			--train_batch_size 4 \
			--neg_mine_bienc_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
			--path_to_model ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_w_bienc_w_ddp_w_best_wrt_dev_mrr_cls_w_lin/model/model-1-11359.0--80.19.ckpt \
			--eval_batch_size 30 \
			--ckpt_metric mrr \
			--warmup_proportion 0.01 \
			--strategy ddp \
			--num_gpus 2 \
			--eval_interval 0.2 \
			--dump_data 1 \
			--misc 63_from_500_negs_from_6_49_best


			sbatch -p rtx8000-long --gres gpu:2 --mem 128GB --job-name small_birerank \
			bin/run.sh \
			python models/train.py \
			--config config/el_zeshel_cross_enc_small_train.json \
			--exp_id 6_ReprCrossEnc \
			--pooling_type cls_w_lin \
			--init_num_negs 500 \
			--num_negs 63 \
			--neg_strategy bienc_hard_negs_w_rerank \
			--reload_dataloaders_every_n_epochs 1 \
			--train_batch_size 4 \
			--neg_mine_bienc_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
			--eval_batch_size 30 \
			--ckpt_metric mrr \
			--warmup_proportion 0.01 \
			--strategy ddp \
			--num_gpus 2 \
			--eval_interval 0.2 \
			--dump_data 1 \
			--misc 63_from_500_negs_small_train

	14 April
		
		Re-running distillation again as something seems off with previously trained models 
			sbatch -p rtx8000-long --gres gpu:2 --mem 128GB --job-name distill_try_2 \
			bin/run.sh \
			python models/train.py \
			--config config/el_zeshel_bi_enc.json \
			--exp_id 7_EntModel \
			--neg_strategy distill \
			--distill_n_labels 64 \
			--train_batch_size 8 \
			--path_to_model ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
			--ent_w_score_file_template ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_nsw_search_s=1234_64_negs_5_bs_10_max_nbrs_500_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/eval/{}/m=-1_k=100_nsw_bienc_10_5_500_/crossenc_topk_preds_w_nsw.txt \
			--reload_dataloaders_every_n_epochs 0 \
			--eval_batch_size 32 \
			--warmup_proportion 0.01 \
			--strategy ddp \
			--num_gpus 2 \
			--eval_interval 0.2 \
			--misc distill_w_64_negs_wrt_cross_id_6_82_0_all_data_try2


		Re-running NSW exps systematically

			python eval/nsw_eval_zeshel.py \
			--data_name lego \
			--embed_type bienc \
			--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
			--res_dir          ../../results/6_ReprCrossEnc/_del \
			--graph_type nsw  \
			--entry_method bienc \
			--debug_mode 1 \
			--misc try_2

			python eval/nsw_eval_zeshel.py \
			--data_name lego \
			--embed_type tfidf \
			--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
			--res_dir          ../../results/6_ReprCrossEnc/_del \
			--graph_type nsw  \
			--entry_method tfidf \
			--debug_mode 1 \
			--misc try_2

	15 April
		
		Running ent2ent computation for rank_ce, rank_margin and ce trained best crossencoder ckpts
			pro_wrestling
				sbatch -p m40-long --gres gpu:1 --mem 64GB --job-name e2e1_pro \
				bin/run.sh \
				python eval/run_cross_encoder_for_ent_ent_matrix.py \
				--data_name pro_wrestling \
				--n_ent_x -1 \
				--n_ent_y -1 \
				--topk 100 \
				--embed_type bienc \
				--token_opt m2e \
				--batch_size 600 \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=rank_margin_neg=bienc_hard_negs_w_knn_rank_s=1234_63_negs/score_mats_model-2-17239.0-1.22.ckpt \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=rank_margin_neg=bienc_hard_negs_w_knn_rank_s=1234_63_negs/model/model-2-17239.0-1.22.ckpt \
				--bi_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt


				sbatch -p m40-long --gres gpu:1 --mem 64GB --job-name e2e2_pro \
				bin/run.sh \
				python eval/run_cross_encoder_for_ent_ent_matrix.py \
				--data_name pro_wrestling \
				--n_ent_x -1 \
				--n_ent_y -1 \
				--topk 100 \
				--embed_type bienc \
				--token_opt m2e \
				--batch_size 600 \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=rank_ce_neg=bienc_hard_negs_w_knn_rank_s=1234_63_negs_w_ddp_w_cls_w_lin_d2p_neg_lin/score_mats_model-1-12279.0-3.21.ckpt \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=rank_ce_neg=bienc_hard_negs_w_knn_rank_s=1234_63_negs_w_ddp_w_cls_w_lin_d2p_neg_lin/model/model-1-12279.0-3.21.ckpt \
				--bi_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt

				sbatch -p 2080ti-long --gres gpu:1 --mem 64GB --job-name e2e3_pro \
				bin/run.sh \
				python eval/run_cross_encoder_for_ent_ent_matrix.py \
				--data_name pro_wrestling \
				--n_ent_x -1 \
				--n_ent_y -1 \
				--topk 100 \
				--embed_type bienc \
				--token_opt m2e \
				--batch_size 150 \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_w_bienc_w_ddp_w_best_wrt_dev_mrr_cls_w_lin/score_mats_model-1-11359.0--80.19.ckpt \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_w_bienc_w_ddp_w_best_wrt_dev_mrr_cls_w_lin/model/model-1-11359.0--80.19.ckpt \
				--bi_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt

			lego
				sbatch -p 2080ti-long --gres gpu:1 --mem 64GB --job-name e2e1_lego \
				bin/run.sh \
				python eval/run_cross_encoder_for_ent_ent_matrix.py \
				--data_name lego \
				--n_ent_x -1 \
				--n_ent_y -1 \
				--topk 100 \
				--embed_type bienc \
				--token_opt m2e \
				--batch_size 150 \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=rank_margin_neg=bienc_hard_negs_w_knn_rank_s=1234_63_negs/score_mats_model-2-17239.0-1.22.ckpt \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=rank_margin_neg=bienc_hard_negs_w_knn_rank_s=1234_63_negs/model/model-2-17239.0-1.22.ckpt \
				--bi_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt


				sbatch -p 2080ti-long --gres gpu:1 --mem 64GB --job-name e2e2_lego \
				bin/run.sh \
				python eval/run_cross_encoder_for_ent_ent_matrix.py \
				--data_name lego \
				--n_ent_x -1 \
				--n_ent_y -1 \
				--topk 100 \
				--embed_type bienc \
				--token_opt m2e \
				--batch_size 150 \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=rank_ce_neg=bienc_hard_negs_w_knn_rank_s=1234_63_negs_w_ddp_w_cls_w_lin_d2p_neg_lin/score_mats_model-1-12279.0-3.21.ckpt \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=rank_ce_neg=bienc_hard_negs_w_knn_rank_s=1234_63_negs_w_ddp_w_cls_w_lin_d2p_neg_lin/model/model-1-12279.0-3.21.ckpt \
				--bi_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt

				sbatch -p 2080ti-long --gres gpu:1 --mem 64GB --job-name e2e3_lego \
				bin/run.sh \
				python eval/run_cross_encoder_for_ent_ent_matrix.py \
				--data_name lego \
				--n_ent_x -1 \
				--n_ent_y -1 \
				--topk 100 \
				--embed_type bienc \
				--token_opt m2e \
				--batch_size 150 \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_w_bienc_w_ddp_w_best_wrt_dev_mrr_cls_w_lin/score_mats_model-1-11359.0--80.19.ckpt \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_w_bienc_w_ddp_w_best_wrt_dev_mrr_cls_w_lin/model/model-1-11359.0--80.19.ckpt \
				--bi_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt

	16 April
		Training w/ distilled bienc hard negs w rerank - initializing model using 0-last.ckpt from 6-82 
			sbatch -p rtx8000-long --gres gpu:2 --mem 128GB --job-name rerank_from_ckpt \
			bin/run.sh python models/train.py \
			--config config/el_zeshel_cross_enc.json \
			--exp_id 6_ReprCrossEnc \
			--pooling_type cls_w_lin \
			--init_num_negs 500 \
			--num_negs 63 \
			--neg_strategy bienc_hard_negs_w_rerank \
			--reload_dataloaders_every_n_epochs 1 \
			--train_batch_size 4 \
			--neg_mine_bienc_model_file ../../results/7_EntModel/d=ent_link/m=bi_enc_l=ce_neg=distill_s=1234_distill_w_64_negs_wrt_cross_id_6_82_0_all_data/model/model-3-12318.0-1.92.ckpt \
			--path_to_model ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_nsw_search_s=1234_64_negs_5_bs_10_max_nbrs_500_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/model/0-last.ckpt \
			--eval_batch_size 30 \
			--ckpt_metric mrr \
			--warmup_proportion 0.01 \
			--strategy ddp \
			--num_gpus 2 \
			--eval_interval 0.2 \
			--dump_data 1 \
			--misc 63_from_500_negs_w_distilled_bienc_7_5_ep_1_from_6_82_0_last


			sbatch -p rtx8000-long --gres gpu:2 --mem 128GB --job-name rerank_from_ckpt \
			bin/run.sh python models/train.py \
			--config config/el_zeshel_cross_enc_one_domain.json \
			--exp_id _0_Debug \
			--pooling_type cls_w_lin \
			--init_num_negs 500 \
			--num_negs 63 \
			--neg_strategy bienc_hard_negs_w_rerank \
			--reload_dataloaders_every_n_epochs 1 \
			--train_batch_size 4 \
			--neg_mine_bienc_model_file ../../results/7_EntModel/d=ent_link/m=bi_enc_l=ce_neg=distill_s=1234_distill_w_64_negs_wrt_cross_id_6_82_0_all_data/model/model-3-12318.0-1.92.ckpt \
			--path_to_model ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_nsw_search_s=1234_64_negs_5_bs_10_max_nbrs_500_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/model/0-last.ckpt \
			--eval_batch_size 30 \
			--ckpt_metric mrr \
			--warmup_proportion 0.01 \
			--strategy ddp \
			--num_gpus 2 \
			--eval_interval 0.2 \
			--dump_data 1 \
			--misc 63_from_500_negs_w_distilled_bienc_7_5_ep_1_from_6_82_0_last \
			--debug_w_small_data 1 

	19 April

			sbatch -p rtx8000-long --gres gpu:2 --mem 128GB --job-name rerank_from_ckpt \
			bin/run.sh python models/train.py \
			--config config/el_zeshel_cross_enc_small_train.json \
			--exp_id 6_ReprCrossEnc \
			--pooling_type cls_w_lin \
			--num_negs 63 \
			--neg_strategy bienc_nsw_search \
			--nsw_comp_budget 500 \
			--nsw_beamsize 5 \
			--nsw_max_nbrs 10 \
			--reload_dataloaders_every_n_epochs 1 \
			--train_batch_size 4 \
			--neg_mine_bienc_model_file ../../results/7_EntModel/d=ent_link/m=bi_enc_l=ce_neg=distill_s=1234_distill_w_64_negs_wrt_cross_id_6_82_0_all_data/model/model-3-12318.0-1.92.ckpt \
			--eval_batch_size 32 \
			--ckpt_metric mrr \
			--warmup_proportion 0.01 \
			--strategy ddp \
			--num_gpus 2 \
			--eval_interval 0.2 \
			--misc 63_negs_5_bs_10_max_nbrs_500_budget_w_distilled_bienc_7_5_small_train


			COmputing e2e scores for doctor_who
				sbatch -p 2080ti-long --gres gpu:1 --mem 64GB --job-name e2e1_pro \
				bin/run.sh \
				python eval/run_cross_encoder_for_ent_ent_matrix.py \
				--data_name doctor_who \
				--n_ent_x -1 \
				--n_ent_y -1 \
				--topk 100 \
				--embed_type bienc \
				--token_opt m2e \
				--batch_size 150 \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=rank_ce_neg=bienc_hard_negs_w_knn_rank_s=1234_63_negs_w_ddp_w_cls_w_lin_d2p_neg_lin/score_mats_model-1-12279.0-3.21.ckpt \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=rank_ce_neg=bienc_hard_negs_w_knn_rank_s=1234_63_negs_w_ddp_w_cls_w_lin_d2p_neg_lin/model/model-1-12279.0-3.21.ckpt \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt

				sbatch -p 1080ti-long --gres gpu:1 --mem 64GB --job-name e2e2_pro \
				bin/run.sh \
				python eval/run_cross_encoder_for_ent_ent_matrix.py \
				--data_name doctor_who \
				--n_ent_x -1 \
				--n_ent_y -1 \
				--topk 100 \
				--embed_type bienc \
				--token_opt m2e \
				--batch_size 150 \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_w_bienc_w_ddp_w_best_wrt_dev_mrr_cls_w_lin/score_mats_model-1-11359.0--80.19.ckpt \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_w_bienc_w_ddp_w_best_wrt_dev_mrr_cls_w_lin/model/model-1-11359.0--80.19.ckpt \
				--bi_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt

				sbatch -p titanx-long --gres gpu:1 --mem 64GB --job-name e2e3_pro \
				bin/run.sh \
				python eval/run_cross_encoder_for_ent_ent_matrix.py \
				--data_name doctor_who \
				--n_ent_x -1 \
				--n_ent_y -1 \
				--topk 100 \
				--embed_type bienc \
				--token_opt m2e \
				--batch_size 150 \
				--res_dir ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_nsw_search_s=1234_64_negs_5_bs_10_max_nbrs_500_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/score_mats_0-last.ckpt \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_nsw_search_s=1234_64_negs_5_bs_10_max_nbrs_500_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/model/0-last.ckpt \
				--bi_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt

	21 April


			Training with rank_ce objective on pro_wrestling domain only  
				sbatch -p rtx8000-long --gres gpu:2 --mem 128GB --job-name neg_w_rank_lin \
				bin/run.sh python models/train.py \
				--config config/el_zeshel_cross_enc_one_domain.json \
				--exp_id 6_ReprCrossEnc \
				--pooling_type cls_w_lin \
				--loss rank_ce \
				--neg_strategy bienc_hard_negs_w_knn_rank \
				--num_negs 63 \
				--dist_to_prob_method negate_linear \
				--train_batch_size 4 \
				--neg_mine_bienc_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
				--reload_dataloaders_every_n_epochs 0 \
				--eval_batch_size 32 \
				--ckpt_metric loss \
				--warmup_proportion 0.01 \
				--strategy ddp \
				--num_gpus 2 \
				--eval_interval 0.2 \
				--misc 63_negs_d2p_neg_lin_trn_pro_only \
				--num_epochs 50 

	22 April

			Debugging  CrossEncoder model w/ embeddings idea:
				python models/train.py \
				--config config/el_zeshel_crossl_cross_enc_one_domain.json \
				--exp_id _0_Debug \
				--num_negs 4 \
				--neg_strategy random \
				--train_batch_size 4 \
				--neg_mine_bienc_model_file ../../results/7_EntModel/d=ent_link/m=bi_enc_l=ce_neg=distill_s=1234_distill_w_64_negs_wrt_cross_id_6_82_0_all_data/model/model-3-12318.0-1.92.ckpt \
				--eval_batch_size 30 \
				--warmup_proportion 0.01 \
				--eval_interval 0.9 \
				--misc debug_arch \
				--debug_w_small_data 1 


			Training CrossEncoder model w/ embedding idea

				sbatch -p rtx8000-long --gres gpu:2 --mem 128GB --job-name new_ce_pro \
				bin/run.sh python models/train.py \
				--config config/el_zeshel_cross_enc_one_domain.json \
				--exp_id 6_ReprCrossEnc \
				--cross_enc_type w_embeds \
				--loss ce \
				--neg_strategy bienc_hard_negs \
				--num_negs 63 \
				--train_batch_size 4 \
				--neg_mine_bienc_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
				--reload_dataloaders_every_n_epochs 0 \
				--eval_batch_size 32 \
				--ckpt_metric mrr \
				--warmup_proportion 0.01 \
				--strategy ddp \
				--num_gpus 2 \
				--eval_interval 0.2 \
				--misc 63_negs_w_crossenc_w_embeds_trn_pro_only \
				--num_epochs 50 

				sbatch -p rtx8000-long --gres gpu:2 --mem 128GB --job-name neg_w_rank_lin \
				bin/run.sh python models/train.py \
				--config config/el_zeshel_cross_enc_small_train.json \
				--exp_id 6_ReprCrossEnc \
				--cross_enc_type w_embeds \
				--loss ce \
				--neg_strategy bienc_hard_negs \
				--num_negs 63 \
				--train_batch_size 4 \
				--neg_mine_bienc_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
				--reload_dataloaders_every_n_epochs 0 \
				--eval_batch_size 32 \
				--ckpt_metric mrr \
				--warmup_proportion 0.01 \
				--strategy ddp \
				--num_gpus 2 \
				--eval_interval 0.2 \
				--misc 63_negs_w_crossenc_w_embeds_small_train


				sbatch -p rtx8000-long --gres gpu:2 --mem 128GB --job-name new_ce_distill \
				bin/run.sh python models/train.py \
				--config config/el_zeshel_cross_enc_one_domain.json \
				--exp_id 6_ReprCrossEnc \
				--cross_enc_type w_embeds \
				--neg_strategy bienc_distill \
				--distill_n_labels 64 \
				--train_batch_size 4 \
				--neg_mine_bienc_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
				--eval_batch_size 32 \
				--warmup_proportion 0.01 \
				--eval_interval 0.9 \
				--strategy ddp \
				--num_gpus 2 \
				--num_epochs 25 \
				--misc w_crossenc_w_embeds_trn_pro_only

	23 April

			Training CrossEncoder model w/ embedding idea on all data	
				sbatch -p rtx8000-long --gres gpu:2 --mem 128GB --job-name all_new_ce \
				bin/run.sh python models/train.py \
				--config config/el_zeshel_cross_enc.json \
				--exp_id 6_ReprCrossEnc \
				--cross_enc_type w_embeds \
				--loss ce \
				--neg_strategy bienc_hard_negs \
				--num_negs 63 \
				--train_batch_size 4 \
				--neg_mine_bienc_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
				--reload_dataloaders_every_n_epochs 0 \
				--eval_batch_size 32 \
				--ckpt_metric mrr \
				--warmup_proportion 0.01 \
				--strategy ddp \
				--num_gpus 2 \
				--eval_interval 0.2 \
				--misc 63_negs_w_crossenc_w_embeds


				sbatch -p rtx8000-long --gres gpu:2 --mem 128GB --job-name all_new_rank_ce \
				bin/run.sh python models/train.py \
				--config config/el_zeshel_cross_enc.json \
				--exp_id 6_ReprCrossEnc \
				--cross_enc_type w_embeds \
				--loss rank_ce \
				--neg_strategy bienc_hard_negs_w_knn_rank \
				--num_negs 63 \
				--dist_to_prob_method negate_linear \
				--train_batch_size 4 \
				--neg_mine_bienc_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
				--reload_dataloaders_every_n_epochs 0 \
				--eval_batch_size 32 \
				--ckpt_metric loss \
				--warmup_proportion 0.01 \
				--strategy ddp \
				--num_gpus 2 \
				--eval_interval 0.2 \
				--misc 63_negs_w_crossenc_w_embeds

				
			Running analysis for e-crossenc

				python eval/analyze_e_crossenc_model.py \
				--data_name pro_wrestling \
				--top_k 64 \
				--res_dir ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_small_train/score_mats_model-1-3999.0--77.67.ckpt

			Running eval with e-crossenc being used as a BiEncoder

				sbatch -p gpu --gres gpu:1 --mem 64GB --job-name eval \
				bin/run.sh \
				python eval/run_e-crossencoder_eval.py \
				--data_name pro_wrestling \
				--n_ment -1 \
				--top_k 64 \
				--batch_size 1 \
				--model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_small_train/model/model-1-3999.0--77.67.ckpt \
				--res_dir    ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_small_train/eval \
				--use_dummy_ment 0 \
				--misc w_self_retr_wo_dummy_ment

				sbatch -p gpu --gres gpu:1 --mem 64GB --job-name eval \
				bin/run.sh \
				python eval/run_e-crossencoder_eval.py \
				--data_name pro_wrestling \
				--n_ment -1 \
				--top_k 64 \
				--batch_size 1 \
				--model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_small_train/model/model-1-3999.0--77.67.ckpt \
				--res_dir    ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_small_train/eval \
				--use_dummy_ment 1 \
				--misc w_self_retr_w_dummy_ment


			Computing ment2ent score matrix for CUR and other eval that can use all CrossEncoder scores
				sbatch -p 2080ti-long --gres gpu:1 --mem 64GB --job-name mat2-ce-pro \
				bin/run.sh  \
				python eval/run_cross_encoder_for_ment_ent_matrix_zeshel.py \
				--n_ment 1392 \
				--n_ent -1 \
				--batch_size 300 \
				--data_name pro_wrestling \
				--layers final \
				--cross_model_ckpt ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_small_train/model/model-1-3999.0--77.67.ckpt \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_small_train/score_mats_model-1-3999.0--77.67.ckpt


				sbatch -p 2080ti-long --gres gpu:1 --mem 64GB --job-name mat2-ce-lego \
				bin/run.sh  \
				python eval/run_cross_encoder_for_ment_ent_matrix_zeshel.py \
				--n_ment 1199 \
				--n_ent -1 \
				--batch_size 300 \
				--data_name lego \
				--layers final \
				--cross_model_ckpt ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_small_train/model/model-1-3999.0--77.67.ckpt \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_small_train/score_mats_model-1-3999.0--77.67.ckpt

	24 April
		

			Running CUR approx eval 

			# Run on e-crossenc model

				# all data
					sbatch -p gpu --gres gpu:1 --mem 32GB --exclude gpu-0-0 --job-name cur_lego_all \
					bin/run.sh  \
					python eval/matrix_approx_zeshel.py \
					--data_name lego \
					--n_seeds 10 \
					--n_ment 1199 \
					--res_dir ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_small_train/score_mats_model-1-3999.0--77.67.ckpt \
					--bi_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt


					sbatch -p gpu --gres gpu:1 --mem 32GB --exclude gpu-0-0 --job-name cur_pro_alll \
					bin/run.sh  \
					python eval/matrix_approx_zeshel.py \
					--data_name lego \
					--n_seeds 10 \
					--n_ment 1392 \
					--res_dir ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_small_train/score_mats_model-1-3999.0--77.67.ckpt \
					--bi_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt

				# 500 mentions
					sbatch -p gpu --gres gpu:1 --mem 32GB --exclude gpu-0-0 --job-name cur_lego_500 \
					bin/run.sh  \
					python eval/matrix_approx_zeshel.py \
					--data_name lego \
					--n_seeds 10 \
					--n_ment 500 \
					--res_dir ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_small_train/score_mats_model-1-3999.0--77.67.ckpt 


					sbatch -p gpu --gres gpu:1 --mem 32GB --exclude gpu-0-0 --job-name cur_pro_500 \
					bin/run.sh  \
					python eval/matrix_approx_zeshel.py \
					--data_name pro_wrestling \
					--n_seeds 10 \
					--n_ment 500 \
					--res_dir ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_small_train/score_mats_model-1-3999.0--77.67.ckpt 

				# 100 mentions
					sbatch -p gpu --gres gpu:1 --mem 32GB --exclude gpu-0-0 --job-name cur_lego_100 \
					bin/run.sh  \
					python eval/matrix_approx_zeshel.py \
					--data_name lego \
					--n_seeds 10 \
					--n_ment 100 \
					--res_dir ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_small_train/score_mats_model-1-3999.0--77.67.ckpt 


					sbatch -p gpu --gres gpu:1 --mem 32GB --exclude gpu-0-0 --job-name cur_pro_100 \
					bin/run.sh  \
					python eval/matrix_approx_zeshel.py \
					--data_name pro_wrestling \
					--n_seeds 10 \
					--n_ment 100 \
					--res_dir ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_small_train/score_mats_model-1-3999.0--77.67.ckpt 

			# Run on [CLS]-crossenc model 

				sbatch -p gpu --gres gpu:1 --mem 32GB --exclude gpu-0-0 --job-name cur_ce_lego_100 \
				bin/run.sh  \
				python eval/matrix_approx_zeshel.py \
				--data_name lego \
				--n_seeds 10 \
				--n_ment 100 \
				--res_dir ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_w_bienc_w_ddp_w_best_wrt_dev_mrr_cls_w_lin/score_mats_model-1-11359.0--80.19.ckpt


				sbatch -p gpu --gres gpu:1 --mem 32GB --exclude gpu-0-0 --job-name cur_ce_pro_100 \
				bin/run.sh  \
				python eval/matrix_approx_zeshel.py \
				--data_name pro_wrestling \
				--n_seeds 10 \
				--n_ment 100 \
				--res_dir ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_w_bienc_w_ddp_w_best_wrt_dev_mrr_cls_w_lin/score_mats_model-1-11359.0--80.19.ckpt

	25 April

		Training w/ distilled bienc hard negs w rerank - 
		initializing model using 0-last.ckpt from 6-82 - using ckpt arg instead of path_to_model arg
			sbatch -p rtx8000-long --gres gpu:2 --mem 128GB --job-name rerank_2from_ckpt \
			bin/run.sh python models/train.py \
			--config config/el_zeshel_cross_enc.json \
			--exp_id 6_ReprCrossEnc \
			--pooling_type cls_w_lin \
			--init_num_negs 500 \
			--num_negs 63 \
			--neg_strategy bienc_hard_negs_w_rerank \
			--reload_dataloaders_every_n_epochs 1 \
			--train_batch_size 4 \
			--neg_mine_bienc_model_file ../../results/7_EntModel/d=ent_link/m=bi_enc_l=ce_neg=distill_s=1234_distill_w_64_negs_wrt_cross_id_6_82_0_all_data/model/model-3-12318.0-1.92.ckpt \
			--ckpt_path ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_nsw_search_s=1234_64_negs_5_bs_10_max_nbrs_500_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/model/0-last.ckpt \
			--eval_batch_size 30 \
			--ckpt_metric mrr \
			--warmup_proportion 0.01 \
			--strategy ddp \
			--num_gpus 2 \
			--eval_interval 0.2 \
			--dump_data 1 \
			--misc 63_from_500_negs_w_distilled_bienc_7_5_ep_1_from_6_82_0_last_as_ckpt



		Launching some m2e and e2e score computation w/ e-crossenc w/ ce and e-crossenc w/ rank-ce loss on all data

				for data in lego pro_wrestling doctor_who
				do

				    # CE E-CrossEnc
				    sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name m2e-ce-$data bin/run.sh  \
				    python eval/run_cross_encoder_for_ment_ent_matrix_zeshel.py \
				    --n_ment 100 \
				    --n_ent -1 \
				    --batch_size 300 \
				    --data_name $data \
				    --layers final \
				    --cross_model_ckpt ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/model/model-2-15999.0--79.46.ckpt \
				    --res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt

				    sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name m2e-ce-${data}_all bin/run.sh  \
				    python eval/run_cross_encoder_for_ment_ent_matrix_zeshel.py \
				    --n_ment -1 \
				    --n_ent -1 \
				    --batch_size 300 \
				    --data_name $data \
				    --layers final \
				    --cross_model_ckpt ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/model/model-2-15999.0--79.46.ckpt \
				    --res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt

				    sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name e2e-ce-$data bin/run.sh  \
				    bin/run.sh \
				    python eval/run_cross_encoder_for_ent_ent_matrix.py \
				    --data_name $data \
				    --n_ent_x -1 \
				    --n_ent_y -1 \
				    --topk 100 \
				    --embed_type bienc \
				    --token_opt m2e \
				    --batch_size 150 \
				    --res_dir ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt \
				    --cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/model/model-2-15999.0--79.46.ckpt \
				    --bi_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt

				#    # Rank-CE E-CrossEnc
				#    sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name m2e-rce-$data bin/run.sh  \
				#    python eval/run_cross_encoder_for_ment_ent_matrix_zeshel.py \
				#    --n_ment 100 \
				#    --n_ent -1 \
				#    --batch_size 300 \
				#    --data_name $data \
				#    --layers final \
				#    --cross_model_ckpt ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=rank_ce_neg=bienc_hard_negs_w_knn_rank_s=1234_63_negs_w_crossenc_w_embeds/model/model-1-12279.0-3.19.ckpt \
				#    --res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=rank_ce_neg=bienc_hard_negs_w_knn_rank_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-1-12279.0-3.19.ckpt
				#
				#    sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name m2e-rce-${data}_all bin/run.sh  \
				#    python eval/run_cross_encoder_for_ment_ent_matrix_zeshel.py \
				#    --n_ment -1 \
				#    --n_ent -1 \
				#    --batch_size 300 \
				#    --data_name $data \
				#    --layers final \
				#    --cross_model_ckpt ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=rank_ce_neg=bienc_hard_negs_w_knn_rank_s=1234_63_negs_w_crossenc_w_embeds/model/model-1-12279.0-3.19.ckpt \
				#    --res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=rank_ce_neg=bienc_hard_negs_w_knn_rank_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-1-12279.0-3.19.ckpt
				#
				#
				#    sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name e2e-rce-$data bin/run.sh  \
				#    bin/run.sh \
				#    python eval/run_cross_encoder_for_ent_ent_matrix.py \
				#    --data_name $data \
				#    --n_ent_x -1 \
				#    --n_ent_y -1 \
				#    --topk 100 \
				#    --embed_type bienc \
				#    --token_opt m2e \
				#    --batch_size 150 \
				#    --res_dir ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=rank_ce_neg=bienc_hard_negs_w_knn_rank_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-1-12279.0-3.19.ckpt \
				#    --cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=rank_ce_neg=bienc_hard_negs_w_knn_rank_s=1234_63_negs_w_crossenc_w_embeds/model/model-1-12279.0-3.19.ckpt \
				#    --bi_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt

				done

	26 April

		Training w/ Rank-CE loss w/ bienc hard negs w rerank hard negs - 
		It is easier to search for Cross-Encoder hard negs for rank-ce trained cross-encoder model

			sbatch -p rtx8000-long --gres gpu:1 --mem 32GB --job-name e2e-rce-doctor_who bin/run.sh  \
			python models/train.py \
			--config config/el_zeshel_cross_enc_small_train.json \
			--exp_id 6_ReprCrossEnc \
			--pooling_type cls_w_lin \
			--loss rank_ce \
			--dist_to_prob_method negate_linear \
			--init_num_negs 500 \
			--num_negs 63 \
			--neg_strategy bienc_hard_negs_w_rerank \
			--reload_dataloaders_every_n_epochs 1 \
			--train_batch_size 4 \
			--neg_mine_bienc_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
			--eval_batch_size 30 \
			--ckpt_metric mrr \
			--warmup_proportion 0.01 \
			--strategy ddp \
			--num_gpus 2 \
			--eval_interval 0.2 \
			--dump_data 1 \
			--misc 63_from_500_negs_d2p_neg_lin_small_train

			Debug

				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name e2e-rce-doctor_who bin/run.sh  \
				python models/train.py \
				--config config/el_zeshel_cross_enc_one_domain.json \
				--exp_id _0_Debug \
				--pooling_type cls_w_lin \
				--loss rank_ce \
				--dist_to_prob_method negate_linear \
				--init_num_negs 10 \
				--num_negs 4 \
				--neg_strategy bienc_hard_negs_w_rerank \
				--reload_dataloaders_every_n_epochs 1 \
				--train_batch_size 1 \
				--grad_acc_steps 1 \
				--neg_mine_bienc_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
				--eval_batch_size 30 \
				--ckpt_metric mrr \
				--warmup_proportion 0.01 \
				--strategy ddp \
				--num_gpus 2 \
				--eval_interval 0.9 \
				--dump_data 1 \
				--misc 63_from_500_negs_d2p_neg_lin


		Training w/ NSW re-ranking negs w/ E-CrossEnc model -
		it looks like it is easier to search or mine hard negs 

			sbatch -p rtx8000-long --gres gpu:2 --mem 32GB --job-name ece_w_nsw bin/run.sh  \
			python models/train.py \
			--config config/el_zeshel_cross_enc_small_train.json \
			--exp_id 6_ReprCrossEnc \
			--pooling_type cls_w_lin \
			--num_negs 63 \
			--cross_enc_type w_embeds \
			--neg_strategy bienc_nsw_search \
			--nsw_comp_budget 500 \
			--nsw_beamsize 5 \
			--nsw_max_nbrs 10 \
			--reload_dataloaders_every_n_epochs 1 \
			--train_batch_size 4 \
			--neg_mine_bienc_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
			--eval_batch_size 32 \
			--ckpt_metric mrr \
			--warmup_proportion 0.01 \
			--strategy ddp \
			--num_gpus 2 \
			--eval_interval 0.2 \
			--misc 63_negs_5_bs_10_max_nbrs_500_budget_w_crossenc_w_embeds_small_train


			sbatch -p rtx8000-long --gres gpu:2 --mem 32GB --job-name ece_w_rerank bin/run.sh  \
			python models/train.py \
			--config config/el_zeshel_cross_enc_small_train.json \
			--exp_id 6_ReprCrossEnc \
			--pooling_type cls_w_lin \
			--cross_enc_type w_embeds \
			--init_num_negs 500 \
			--num_negs 63 \
			--neg_strategy bienc_hard_negs_w_rerank \
			--reload_dataloaders_every_n_epochs 1 \
			--train_batch_size 4 \
			--neg_mine_bienc_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
			--eval_batch_size 32 \
			--ckpt_metric mrr \
			--warmup_proportion 0.01 \
			--strategy ddp \
			--num_gpus 2 \
			--eval_interval 0.2 \
			--misc 63_from_500_negs_w_crossenc_w_embeds_small_train

	28 April

		Analyzing NSW graph

			python eval/analyze_nsw_graph.py \
			--bi_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
			--data_name lego \
			--embed_type anchor \
			--res_dir ../../results/6_ReprCrossEnc/_analyze_graph_debug/score_mats
	
	29 April


		Launch jobs for training w/ all layers - maybe not needed right now?


		Analyzing graph jobs for anchor and bienc embeddings

			# for data in lego pro_wrestling
			for data in lego
			do
			for embed in anchor bienc 
			do
			for gtype in knn nsw
			do
			sbatch -p gpu --gres gpu:1 --mem 32GB --exclude gpu-0-0 --job-name ap-$data-$gtype-$embed \
			bin/run.sh  \
			python eval/analyze_nsw_graph.py \
			--bi_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
			--data_name $data \
			--embed_type $embed \
			--res_dir ../../results/6_ReprCrossEnc/_analyze_graph_debug \
			--entry_method bienc \
			--graph_type $gtype \
			--n_ment -1 \
			--misc all_ments
			done
			done
			done

	30 April


		Training w/ NSW re-ranking negs w/ E-CrossEnc model -
		it looks like it is easier to search or mine hard negs - Now training with even larger budget

			sbatch -p rtx8000-long --gres gpu:2 --mem 32GB --job-name ece_w_nsw bin/run.sh  \
			python models/train.py \
			--config config/el_zeshel_cross_enc_small_train.json \
			--exp_id 6_ReprCrossEnc \
			--pooling_type cls_w_lin \
			--num_negs 63 \
			--cross_enc_type w_embeds \
			--neg_strategy bienc_nsw_search \
			--nsw_comp_budget 1000 \
			--nsw_beamsize 5 \
			--nsw_max_nbrs 10 \
			--reload_dataloaders_every_n_epochs 1 \
			--train_batch_size 4 \
			--neg_mine_bienc_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
			--eval_batch_size 32 \
			--ckpt_metric mrr \
			--warmup_proportion 0.01 \
			--strategy ddp \
			--num_gpus 2 \
			--eval_interval 0.2 \
			--misc 63_negs_5_bs_10_max_nbrs_1000_budget_w_crossenc_w_embeds_small_train


			sbatch -p rtx8000-long --gres gpu:2 --mem 32GB --job-name ece_w_rerank bin/run.sh  \
			python models/train.py \
			--config config/el_zeshel_cross_enc_small_train.json \
			--exp_id 6_ReprCrossEnc \
			--pooling_type cls_w_lin \
			--cross_enc_type w_embeds \
			--init_num_negs 1000 \
			--num_negs 63 \
			--neg_strategy bienc_hard_negs_w_rerank \
			--reload_dataloaders_every_n_epochs 1 \
			--train_batch_size 4 \
			--neg_mine_bienc_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
			--eval_batch_size 32 \
			--ckpt_metric mrr \
			--warmup_proportion 0.01 \
			--strategy ddp \
			--num_gpus 2 \
			--eval_interval 0.2 \
			--misc 63_from_1000_negs_w_crossenc_w_embeds_small_train


		Computing anchor entities for scoring all other entities
			for data in pro_wrestling lego
			do
			# sbatch -p gpu --gres gpu:1 --mem 32GB --exclude gpu-0-0 --job-name anc1-$data-all_anchor \
			sbatch -p cpu --mem 32GB --job-name easy_pam_anc1-$data-all_anchor \
			bin/run.sh  \
			python eval/find_anchor_entities.py \
			--data_name $data \
			--embed_type anchor \
			--res_dir ../../results/6_ReprCrossEnc/_analyze_graph_debug \
			--n_ment -1 \
			--misc all_ments

			# sbatch -p gpu --gres gpu:1 --mem 32GB --exclude gpu-0-0 --job-name anc2-$data-100_anchor \
			sbatch -p cpu --mem 32GB --job-name easy_pam_anc2-$data-100_anchor \
			bin/run.sh  \
			python eval/find_anchor_entities.py \
			--data_name $data \
			--embed_type anchor \
			--res_dir ../../results/6_ReprCrossEnc/_analyze_graph_debug \
			--n_ment 100 \
			--misc 100_ments

			# sbatch -p gpu --gres gpu:1 --mem 32GB --exclude gpu-0-0 --job-name anc3-$data-bienc \
			sbatch -p cpu --mem 32GB --job-name easy_pam_anc3-$data-bienc \
			bin/run.sh  \
			python eval/find_anchor_entities.py \
			--bi_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
			--data_name $data \
			--embed_type bienc \
			--res_dir ../../results/6_ReprCrossEnc/_analyze_graph_debug
			done


		Computing e2e scores with some precomputed entity anchors

			for data in pro_wrestling lego 
			do
			for topk in 1000
			do
			sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name e2e-ece1-$data-$topk bin/run.sh  \
			bin/run.sh \
			python eval/run_cross_encoder_for_ent_ent_matrix.py \
			--data_name $data \
			--n_ent_x -1 \
			--n_ent_y -1 \
			--topk $topk \
			--embed_type bienc \
			--token_opt m2e \
			--batch_size 150 \
			--topk_ents_file ../../results/6_ReprCrossEnc/_analyze_graph_debug/$data/easy_10_iter_cpu_anchor_ents_emb=anchor_all_ments.json \
			--res_dir ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt \
			--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/model/model-2-15999.0--79.46.ckpt \
			--bi_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
			--misc all_m2e_anchor_cluster

			sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name e2e-ece2-$data-$topk bin/run.sh  \
			bin/run.sh \
			python eval/run_cross_encoder_for_ent_ent_matrix.py \
			--data_name $data \
			--n_ent_x -1 \
			--n_ent_y -1 \
			--topk $topk \
			--embed_type bienc \
			--token_opt m2e \
			--batch_size 150 \
			--topk_ents_file ../../results/6_ReprCrossEnc/_analyze_graph_debug/$data/easy_10_iter_cpu_anchor_ents_emb=anchor_100_ments.json \
			--res_dir ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt \
			--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/model/model-2-15999.0--79.46.ckpt \
			--bi_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
			--misc 100_m2e_anchor_cluster

			sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name e2e-ece3-$data-$topk bin/run.sh  \
			bin/run.sh \
			python eval/run_cross_encoder_for_ent_ent_matrix.py \
			--data_name $data \
			--n_ent_x -1 \
			--n_ent_y -1 \
			--topk $topk \
			--embed_type bienc \
			--token_opt m2e \
			--batch_size 150 \
			--topk_ents_file ../../results/6_ReprCrossEnc/_analyze_graph_debug/$data/easy_10_iter_cpu_anchor_ents_emb=bienc.json \
			--res_dir ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt \
			--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/model/model-2-15999.0--79.46.ckpt \
			--bi_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
			--misc bienc_cluster

			done
			done

May 

	2-4 May 

		[Re-run w/ mode frequent eval?]
		Training biencoder w/ shared params with special token based pooling
			sbatch -p rtx8000-long --gres gpu:2 --mem 32GB --job-name bi_w_shared bin/run.sh  \
			python models/train.py \
			--config config/el_zeshel_bi_enc.json \
			--exp_id 6_ReprCrossEnc \
			--bi_enc_type shared \
			--pooling_type spl_tkns \
			--neg_strategy bienc_hard_negs \
			--num_negs 63 \
			--train_batch_size 8 \
			--eval_batch_size 64 \
			--reload_dataloaders_every_n_epochs 1 \
			--num_epochs 4 \
			--warmup_proportion 0.01 \
			--strategy ddp \
			--num_gpus 2 \
			--eval_interval 0.2 \
			--misc 63_hard_negs_w_shared_params_n_spl_tkn_pool

		Training biencoder w/ shared params but using cls token
			sbatch -p rtx8000-long --gres gpu:2 --mem 32GB --job-name cls_bi_w_shared bin/run.sh  \
			python models/train.py \
			--config config/el_zeshel_bi_enc.json \
			--exp_id 6_ReprCrossEnc \
			--bi_enc_type shared \
			--pooling_type cls \
			--neg_strategy bienc_hard_negs \
			--num_negs 63 \
			--train_batch_size 8 \
			--eval_batch_size 32 \
			--reload_dataloaders_every_n_epochs 1 \
			--num_epochs 4 \
			--warmup_proportion 0.01 \
			--strategy ddp \
			--num_gpus 2 \
			--eval_interval 0.2 \
			--misc 63_hard_negs_w_shared_params_n_cls_pool

			Some debugging commands

				sbatch -p rtx8000-long --gres gpu:2 --mem 32GB --job-name ece_w_nsw bin/run.sh  \
				python models/train.py \
				--config config/el_zeshel_bi_enc_debug.json \
				--exp_id _0_Debug \
				--pooling_type spl_tkns \
				--bi_enc_type shared \
				--num_negs 4 \
				--neg_strategy bienc_hard_negs \
				--reload_dataloaders_every_n_epochs 1 \
				--train_batch_size 4 \
				--eval_batch_size 32 \
				--warmup_proportion 0.01 \
				--eval_interval 0.9 \
				--misc debug_shared

				python models/train.py \
				--config config/el_zeshel_bi_enc_debug.json \
				--exp_id _0_Debug \
				--pooling_type cls_w_lin \
				--bi_enc_type shared \
				--num_negs 4 \
				--neg_strategy bienc_hard_negs \
				--reload_dataloaders_every_n_epochs 1 \
				--train_batch_size 4 \
				--eval_batch_size 32 \
				--warmup_proportion 0.01 \
				--eval_interval 0.9 \
				--misc debug_shared_2


				python models/train.py \
				--config config/el_zeshel_bi_enc_debug.json \
				--exp_id _0_Debug \
				--num_negs 4 \
				--neg_strategy random \
				--reload_dataloaders_every_n_epochs 0 \
				--train_batch_size 4 \
				--eval_batch_size 32 \
				--warmup_proportion 0.01 \
				--eval_interval 0.9 \
				--misc debug_separate

		Training e-crossenc model w/ both cross-encoder as well as biencoder loss

			sbatch -p rtx8000-long --gres gpu:2 --mem 32GB --job-name e_joint_train bin/run.sh  \
			python models/train.py \
			--config config/el_zeshel_cross_enc.json \
			--exp_id 6_ReprCrossEnc \
			--cross_enc_type w_embeds \
			--loss ce \
			--neg_strategy bienc_hard_negs \
			--joint_train_alpha 0.5 \
			--num_negs 63 \
			--train_batch_size 4 \
			--neg_mine_bienc_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
			--reload_dataloaders_every_n_epochs 0 \
			--eval_batch_size 32 \
			--ckpt_metric mrr \
			--warmup_proportion 0.01 \
			--strategy ddp \
			--num_gpus 2 \
			--eval_interval 0.2 \
			--misc 63_negs_w_6_20_bienc_w_crossenc_w_embeds_w_0.5_bi_cross_loss

		Training cls-crossenc model w/ both cross-encoder as well as biencoder loss

			sbatch -p rtx8000-long --gres gpu:2 --mem 32GB --job-name cls_joint_train bin/run.sh  \
			python models/train.py \
			--config config/el_zeshel_cross_enc.json \
			--exp_id 6_ReprCrossEnc \
			--cross_enc_type default \
			--loss ce \
			--neg_strategy bienc_hard_negs \
			--joint_train_alpha 0.5 \
			--num_negs 63 \
			--train_batch_size 4 \
			--neg_mine_bienc_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
			--reload_dataloaders_every_n_epochs 0 \
			--eval_batch_size 32 \
			--ckpt_metric mrr \
			--warmup_proportion 0.01 \
			--strategy ddp \
			--num_gpus 2 \
			--eval_interval 0.2 \
			--misc 63_negs_w_6_20_bienc_w_cls_crossenc_w_0.5_bi_cross_loss



					Debug - 

						standard cross-encoder only training 
						python models/train.py \
						--config config/el_zeshel_cross_enc_debug.json \
						--exp_id _0_Debug \
						--cross_enc_type w_embeds \
						--num_negs 4 \
						--joint_train_alpha 1.0 \
						--neg_strategy bienc_hard_negs \
						--reload_dataloaders_every_n_epochs 0 \
						--train_batch_size 4 \
						--grad_acc_steps 2  \
						--neg_mine_bienc_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
						--eval_batch_size 32 \
						--ckpt_metric mrr \
						--warmup_proportion 0.01 \
						--eval_interval 0.9 \
						--misc debug_ce_only

						w/ crossenc w/ embeds		
						python models/train.py \
						--config config/el_zeshel_cross_enc_debug.json \
						--exp_id _0_Debug \
						--cross_enc_type w_embeds \
						--loss ce \
						--neg_strategy random \
						--joint_train_alpha 0.5 \
						--num_negs 4 \
						--train_batch_size 4 \
						--grad_acc_steps 2 \
						--neg_mine_bienc_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
						--reload_dataloaders_every_n_epochs 0 \
						--eval_batch_size 32 \
						--ckpt_metric mrr \
						--warmup_proportion 0.01 \
						--eval_interval 0.9 \
						--misc debug_ce_w_bi_joint_0.5

						w/ cls crossenc 
						python models/train.py \
						--config config/el_zeshel_cross_enc_debug.json \
						--exp_id _0_Debug \
						--cross_enc_type default \
						--loss ce \
						--neg_strategy random \
						--joint_train_alpha 0.5 \
						--num_negs 4 \
						--train_batch_size 4 \
						--grad_acc_steps 2 \
						--neg_mine_bienc_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
						--reload_dataloaders_every_n_epochs 0 \
						--eval_batch_size 32 \
						--ckpt_metric mrr \
						--warmup_proportion 0.01 \
						--eval_interval 0.9 \
						--misc debug_ce_w_bi_joint_0.5
									


						python models/train.py \
						--config config/el_zeshel_bi_enc_debug.json \
						--exp_id _0_Debug \
						--pooling_type cls \
						--bi_enc_type shared \
						--num_negs 4 \
						--neg_strategy random \
						--reload_dataloaders_every_n_epochs 0 \
						--train_batch_size 4 \
						--eval_batch_size 32 \
						--warmup_proportion 0.01 \
						--eval_interval 0.9 \
						--misc debug_shared_cls


	5 May 

		Training e-cross-encoder with shared parameter w/ biencoder from scratch


			sbatch -p rtx8000-long --gres gpu:2 --mem 32GB --job-name e_joint_scratch bin/run.sh  \
			python models/train.py \
			--config config/el_zeshel_cross_enc.json \
			--exp_id 6_ReprCrossEnc \
			--cross_enc_type w_embeds \
			--loss ce \
			--neg_strategy bienc_hard_negs \
			--joint_train_alpha 0.5 \
			--num_negs 63 \
			--train_batch_size 4 \
			--reload_dataloaders_every_n_epochs 1 \
			--eval_batch_size 32 \
			--ckpt_metric mrr \
			--warmup_proportion 0.01 \
			--strategy ddp \
			--num_gpus 2 \
			--eval_interval 0.2 \
			--misc 63_negs_w_crossenc_w_embeds_w_0.5_bi_cross_loss_from_scratch

			Training cls-cross-encoder with shared parameter w/ biencoder from scratch
				sbatch -p rtx8000-long --gres gpu:2 --mem 32GB --job-name cls_joint_scratch bin/run.sh  \
				python models/train.py \
				--config config/el_zeshel_cross_enc.json \
				--exp_id 6_ReprCrossEnc \
				--cross_enc_type default \
				--loss ce \
				--neg_strategy bienc_hard_negs \
				--joint_train_alpha 0.5 \
				--num_negs 63 \
				--train_batch_size 4 \
				--reload_dataloaders_every_n_epochs 1 \
				--eval_batch_size 32 \
				--ckpt_metric mrr \
				--warmup_proportion 0.01 \
				--strategy ddp \
				--num_gpus 2 \
				--eval_interval 0.2 \
				--misc 63_negs_w_cls_crossenc_w_0.5_bi_cross_loss_from_scratch


			Debug 

				python models/train.py \
				--config config/el_zeshel_cross_enc_debug.json \
				--exp_id _0_Debug \
				--cross_enc_type w_embeds \
				--loss ce \
				--neg_strategy bienc_hard_negs \
				--joint_train_alpha 0.5 \
				--num_negs 4 \
				--train_batch_size 4 \
				--grad_acc_steps 2 \
				--neg_mine_bienc_model_file "" \
				--reload_dataloaders_every_n_epochs 1 \
				--eval_batch_size 32 \
				--ckpt_metric mrr \
				--warmup_proportion 0.01 \
				--eval_interval 0.9 \
				--misc debug_param_share


	6 May

		Distilling from e-crossenc model (with separate label and input encoder)

		(with separate label and input encoder)
			sbatch -p rtx8000-long --gres gpu:2 --mem 32GB --job-name e_cross_distill bin/run.sh  \
			python models/train.py \
			--config config/el_zeshel_bi_enc.json \
			--exp_id 7_EntModel \
			--neg_strategy distill \
			--distill_n_labels 64 \
			--train_batch_size 8 \
			--path_to_model ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
			--ent_w_score_file_template ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/eval/{}/m=-1_k=64_1_model-2-15999.0--79.46.ckpt/crossenc_topk_preds_w_bienc_retrvr.txt \
			--reload_dataloaders_every_n_epochs 0 \
			--eval_batch_size 32 \
			--warmup_proportion 0.01 \
			--strategy ddp \
			--num_gpus 2 \
			--eval_interval 0.2 \
			--misc distill_w_64_negs_wrt_cross_id_6_256_0_all_data

		(with shared param b/w label and input encoder)
			sbatch -p rtx8000-long --gres gpu:2 --mem 32GB --job-name e_cross_distill bin/run.sh  \
			python models/train.py \
			--config config/el_zeshel_bi_enc.json \
			--exp_id 7_EntModel \
			--bi_enc_type shared \
			--pooling_type spl_tkns \
			--neg_strategy distill \
			--distill_n_labels 64 \
			--train_batch_size 8 \
			--path_to_model ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_w_shared_params_n_spl_tkn_pool/model/model-3-12239.0-2.15.ckpt \
			--ent_w_score_file_template ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/eval/{}/m=-1_k=64_1_model-2-15999.0--79.46.ckpt/crossenc_topk_preds_w_bienc_retrvr.txt \
			--reload_dataloaders_every_n_epochs 0 \
			--eval_batch_size 32 \
			--warmup_proportion 0.01 \
			--strategy ddp \
			--num_gpus 2 \
			--eval_interval 0.2 \
			--misc distill_w_64_negs_wrt_cross_id_6_256_0_all_data_w_shared_param


		Distilling from cls-crossenc model

		(with separate label and input encoder)
			sbatch -p rtx8000-long --gres gpu:2 --mem 32GB --job-name cls_cross_distill bin/run.sh  \
			python models/train.py \
			--config config/el_zeshel_bi_enc.json \
			--exp_id 7_EntModel \
			--neg_strategy distill \
			--distill_n_labels 64 \
			--train_batch_size 8 \
			--path_to_model ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
			--ent_w_score_file_template ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_w_bienc_w_ddp_w_best_wrt_dev_mrr_cls_w_lin/eval/{}/m=-1_k=64_1_model-1-11359.0--80.19.ckpt/crossenc_topk_preds_w_bienc_retrvr.txt \
			--reload_dataloaders_every_n_epochs 0 \
			--eval_batch_size 32 \
			--warmup_proportion 0.01 \
			--strategy ddp \
			--num_gpus 2 \
			--eval_interval 0.2 \
			--misc distill_w_64_negs_wrt_cross_id_6_49_0_all_data

		(with shared param b/w label and input encoder)
			sbatch -p rtx8000-long --gres gpu:2 --mem 32GB --job-name cls_cross_distill bin/run.sh  \
			python models/train.py \
			--config config/el_zeshel_bi_enc.json \
			--exp_id 7_EntModel \
			--bi_enc_type shared \
			--pooling_type cls \
			--neg_strategy distill \
			--distill_n_labels 64 \
			--train_batch_size 8 \
			--path_to_model ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_w_shared_params_n_cls_pool/model/model-3-12318.0-2.04.ckpt \
			--ent_w_score_file_template ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_w_bienc_w_ddp_w_best_wrt_dev_mrr_cls_w_lin/eval/{}/m=-1_k=64_1_model-1-11359.0--80.19.ckpt/crossenc_topk_preds_w_bienc_retrvr.txt \
			--reload_dataloaders_every_n_epochs 0 \
			--eval_batch_size 32 \
			--warmup_proportion 0.01 \
			--strategy ddp \
			--num_gpus 2 \
			--eval_interval 0.2 \
			--misc distill_w_64_negs_wrt_cross_id_6_49_0_all_data_w_shared_params


		Debug
			python models/train.py \
			--config config/el_zeshel_bi_enc_debug.json \
			--exp_id _0_Debug \
			--neg_strategy distill \
			--distill_n_labels 4 \
			--train_batch_size 4 \
			--path_to_model ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
			--ent_w_score_file_template ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/eval/{}/m=-1_k=64_1_model-2-15999.0--79.46.ckpt/crossenc_topk_preds_w_bienc_retrvr.txt \
			--reload_dataloaders_every_n_epochs 0 \
			--eval_batch_size 32 \
			--warmup_proportion 0.01 \
			--eval_interval 0.9 \
			--misc debug_distill


	8 May
		
		Running E2E on jointly trained CE models

			for data in pro_wrestling lego doctor_who
			do
			sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name e2e-joint-$data bin/run.sh  \
			bin/run.sh \
			python eval/run_cross_encoder_for_ent_ent_matrix.py \
			--data_name $data \
			--n_ent_x -1 \
			--n_ent_y -1 \
			--topk 100 \
			--embed_type bienc \
			--token_opt m2e \
			--batch_size 150 \
			--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_6_20_bienc_w_crossenc_w_embeds_w_0.5_bi_cross_loss/score_mats_model-1-12279.0--78.92.ckpt \
			--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_6_20_bienc_w_crossenc_w_embeds_w_0.5_bi_cross_loss/model/model-1-12279.0--78.92.ckpt \
			--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt 
			done



	9 May
		
		Training cross-enc w/ by distilling information from a biencoder

			sbatch -p rtx8000-long --gres gpu:2 --mem 64GB --job-name bi2cross bin/run.sh  \
			python models/train.py \
			--config config/el_zeshel_cross_enc_one_domain.json \
			--exp_id 6_ReprCrossEnc \
			--cross_enc_type w_embeds \
			--neg_strategy bienc_distill \
			--distill_n_labels 64 \
			--train_batch_size 4 \
			--neg_mine_bienc_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
			--eval_batch_size 32 \
			--warmup_proportion 0.01 \
			--eval_interval 0.9 \
			--strategy ddp \
			--num_gpus 2 \
			--num_epochs 25 \
			--misc trn_pro_only_w_crossenc_w_embeds


			sbatch -p rtx8000-long --gres gpu:2 --mem 64GB --job-name bi2cross_all bin/run.sh  \
			python models/train.py \
			--config config/el_zeshel_cross_enc.json \
			--exp_id 6_ReprCrossEnc \
			--cross_enc_type w_embeds \
			--neg_strategy bienc_distill \
			--distill_n_labels 64 \
			--train_batch_size 4 \
			--neg_mine_bienc_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
			--eval_batch_size 32 \
			--warmup_proportion 0.01 \
			--eval_interval 0.2 \
			--strategy ddp \
			--num_gpus 2 \
			--num_epochs 4 \
			--misc crossenc_w_embeds


	10-11 May

		Running E-CrossEnc training with kNN e2e eval 

			Training for 1 epoch 
				sbatch -p rtx8000-long --gres gpu:2 --mem 64GB --job-name e-crossenc bin/run.sh  \
				python models/train.py \
				--config config/el_zeshel_cross_enc.json \
				--exp_id 6_ReprCrossEnc \
				--cross_enc_type w_embeds \
				--loss ce \
				--neg_strategy bienc_hard_negs \
				--num_negs 63 \
				--train_batch_size 4 \
				--neg_mine_bienc_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
				--reload_dataloaders_every_n_epochs 0 \
				--eval_batch_size 32 \
				--ckpt_metric mrr \
				--warmup_proportion 0.01 \
				--strategy ddp \
				--num_gpus 2 \
				--eval_interval 0.9 \
				--num_epochs 1 \
				--misc 63_negs_w_crossenc_w_embeds_e2e_graph	

		 	Launching e2e score computation - fix model name, also launch top-1000 bienc inference, and then laucnh graph search w/ e2e graph. Then train for another 1 epoch and repeat this process.

				for data in american_football doctor_who fallout final_fantasy military pro_wrestling starwars world_of_warcraft 
				for data in coronation_street elder_scrolls ice_hockey muppets

				for data in pro_wrestling world_of_warcraft coronation_street elder_scrolls ice_hockey muppets
				do
				sbatch -p m40-long --gres gpu:1 --mem 32GB --job-name e2e-ece0-$data bin/run.sh  \
				bin/run.sh \
				python eval/run_cross_encoder_for_ent_ent_matrix.py \
				--data_name $data \
				--n_ent_x -1 \
				--n_ent_y -1 \
				--topk 100 \
				--embed_type bienc \
				--token_opt m2e \
				--batch_size 150 \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/score_mats_eoe-0-last.ckpt \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/model/eoe-0-last.ckpt \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt 
				done

				Chunking computation for domain=military
					for xstart in 0 30000 60000 
					for xstart in 60000 
					do
					sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name e2e-ece0-miliary-$xstart bin/run.sh  \
					bin/run.sh \
					python eval/run_cross_encoder_for_ent_ent_matrix.py \
					--data_name military \
					--n_ent_x_start $xstart \
					--n_ent_x  30000 \
					--n_ent_y -1 \
					--topk 100 \
					--embed_type bienc \
					--token_opt m2e \
					--batch_size 150 \
					--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/score_mats_eoe-0-last.ckpt \
					--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/model/eoe-0-last.ckpt \
					--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
					--misc x_start_$xstart
					done

					sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name e2e-ece0-miliary-100K bin/run.sh  \
					bin/run.sh \
					python eval/run_cross_encoder_for_ent_ent_matrix.py \
					--data_name military \
					--n_ent_x_start 90000 \
					--n_ent_x  14520 \
					--n_ent_y -1 \
					--topk 100 \
					--embed_type bienc \
					--token_opt m2e \
					--batch_size 150 \
					--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/score_mats_eoe-0-last.ckpt \
					--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/model/eoe-0-last.ckpt \
					--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
					--misc x_start_90000

				Chunking computation for domain=starwars
					for xstart in 0 30000 
					do
					sbatch -p m40-long --gres gpu:1 --mem 32GB --job-name e2e-ece0-startwars-$xstart bin/run.sh  \
					bin/run.sh \
					python eval/run_cross_encoder_for_ent_ent_matrix.py \
					--data_name starwars \
					--n_ent_x_start $xstart \
					--n_ent_x  30000 \
					--n_ent_y -1 \
					--topk 100 \
					--embed_type bienc \
					--token_opt m2e \
					--batch_size 150 \
					--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/score_mats_eoe-0-last.ckpt \
					--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/model/eoe-0-last.ckpt \
					--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
					--misc x_start_$xstart
					done

					sbatch -p m40-long --gres gpu:1 --mem 32GB --job-name e2e-ece0-startwars-100K bin/run.sh  \
					bin/run.sh \
					python eval/run_cross_encoder_for_ent_ent_matrix.py \
					--data_name starwars \
					--n_ent_x_start 60000 \
					--n_ent_x  27056 \
					--n_ent_y -1 \
					--topk 100 \
					--embed_type bienc \
					--token_opt m2e \
					--batch_size 150 \
					--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/score_mats_eoe-0-last.ckpt \
					--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/model/eoe-0-last.ckpt \
					--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
					--misc x_start_60000


			Launching bi+cross eval w/ top-500 bienc entities
			
				for data in american_football doctor_who fallout final_fantasy
				do
				sbatch -p m40-long --gres gpu:1 --mem 32GB --job-name m1cross_w_bi_$data bin/run.sh \
				python eval/run_cross_encoder_w_binenc_retriever_zeshel.py \
				--data_name $data \
				--n_ment -1 \
				--top_k 500 \
				--batch_size 1 \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/model/eoe-0-last.ckpt \
				--misc eoe-0-last.ckpt
				done


				# For military

					for mstart in 0 2000 4000 6000 8000 10000  
					for mstart in 10000  
					do
					sbatch -p rtx8000-long --gres gpu:1 --mem 32GB --job-name m1cross_w_bi_military-$mstart bin/run.sh \
					python eval/run_cross_encoder_w_binenc_retriever_zeshel.py \
					--data_name military \
					--n_ment_start $mstart \
					--n_ment 2000 \
					--top_k 500 \
					--batch_size 1 \
					--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
					--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/eval \
					--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/model/eoe-0-last.ckpt \
					--misc eoe-0-last.ckpt_mstart_$mstart
					done

					sbatch -p rtx8000-long --gres gpu:1 --mem 32GB --job-name m1cross_w_bi_military-90K bin/run.sh \
					python eval/run_cross_encoder_w_binenc_retriever_zeshel.py \
					--data_name military \
					--n_ment_start 12000 \
					--n_ment 1063 \
					--top_k 500 \
					--batch_size 1 \
					--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
					--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/eval \
					--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/model/eoe-0-last.ckpt \
					--misc eoe-0-last.ckpt_mstart_$mstart


				# For starwars

					# for mstart in 0 30000 
					for mstart in 0 2000 4000 6000 8000  
					do
					sbatch -p m40-long --gres gpu:1 --mem 32GB --job-name m1cross_w_bi_starwars-$mstart bin/run.sh \
					python eval/run_cross_encoder_w_binenc_retriever_zeshel.py \
					--data_name starwars \
					--n_ment_start $mstart \
					--n_ment 2000 \
					--top_k 500 \
					--batch_size 1 \
					--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
					--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/eval \
					--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/model/eoe-0-last.ckpt \
					--misc eoe-0-last.ckpt_mstart_$mstart
					done

					sbatch -p m40-long --gres gpu:1 --mem 32GB --job-name m1cross_w_bi_starwars-10K bin/run.sh \
					python eval/run_cross_encoder_w_binenc_retriever_zeshel.py \
					--data_name starwars \
					--n_ment_start 10000 \
					--n_ment 1824 \
					--top_k 500 \
					--batch_size 1 \
					--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
					--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/eval \
					--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/model/eoe-0-last.ckpt \
					--misc eoe-0-last.ckpt_mstart_$mstart


			Launching graph eval for crossencoder w/ E2E graph


				sbatch -p rtx8000-long --gres gpu:1 --mem 32GB --job-name nsw_american_football bin/run.sh \
				python eval/run_cross_encoder_w_nsw_eval.py \
				--data_name american_football \
				--n_ment -1 \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/model/eoe-0-last.ckpt \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/eval \
				--batch_size 150 \
				--embed_type bienc \
				--max_nbrs 10 \
				--beamsize 5 \
				--top_k 100 \
				--comp_budget 500 \
				--graph_metric l2 \
				--graph_type knn_e2e \
				--e2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/score_mats_eoe-0-last.ckpt/american_football/ent_to_ent_scores_n_e_31929x31929_topk_100_embed_bienc_m2e_.pkl


				sbatch -p rtx8000-long --gres gpu:1 --mem 32GB --job-name nsw_doctor_who bin/run.sh \
				python eval/run_cross_encoder_w_nsw_eval.py \
				--data_name doctor_who \
				--n_ment -1 \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/model/eoe-0-last.ckpt \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/eval \
				--batch_size 150 \
				--embed_type bienc \
				--max_nbrs 10 \
				--beamsize 5 \
				--top_k 100 \
				--comp_budget 500 \
				--graph_metric l2 \
				--graph_type knn_e2e \
				--e2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/score_mats_eoe-0-last.ckpt/doctor_who/ent_to_ent_scores_n_e_40281x40281_topk_100_embed_bienc_m2e_.pkl

				Chunking final_fantasy
				sbatch -p rtx8000-long --gres gpu:1 --mem 32GB --job-name nsw_final_fantasy bin/run.sh \
				python eval/run_cross_encoder_w_nsw_eval.py \
				--data_name final_fantasy \
				--n_ment -1 \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/model/eoe-0-last.ckpt \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/eval \
				--batch_size 150 \
				--embed_type bienc \
				--max_nbrs 10 \
				--beamsize 5 \
				--top_k 100 \
				--comp_budget 500 \
				--graph_metric l2 \
				--graph_type knn_e2e \
				--e2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/score_mats_eoe-0-last.ckpt/final_fantasy/ent_to_ent_scores_n_e_14044x14044_topk_100_embed_bienc_m2e_.pkl


				for mstart in 500 1000 1500 2000 2500 3000 3500 4000 4500 5000 
				for mstart in 0
				do
				sbatch -p rtx8000-long --gres gpu:1 --mem 32GB --job-name nsw_final_fantasy-$mstart bin/run.sh \
				python eval/run_cross_encoder_w_nsw_eval.py \
				--data_name final_fantasy \
				--n_ment_start $mstart \
				--n_ment 500 \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/model/eoe-0-last.ckpt \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/eval \
				--batch_size 150 \
				--embed_type bienc \
				--max_nbrs 10 \
				--beamsize 5 \
				--top_k 100 \
				--comp_budget 500 \
				--graph_metric l2 \
				--graph_type knn_e2e \
				--e2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/score_mats_eoe-0-last.ckpt/final_fantasy/ent_to_ent_scores_n_e_14044x14044_topk_100_embed_bienc_m2e_.pkl \
				--misc mstart_$mstart
				done

				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name nsw_final_fantasy-last bin/run.sh \
				python eval/run_cross_encoder_w_nsw_eval.py \
				--data_name final_fantasy \
				--n_ment_start 5500 \
				--n_ment 541 \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/model/eoe-0-last.ckpt \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/eval \
				--batch_size 150 \
				--embed_type bienc \
				--max_nbrs 10 \
				--beamsize 5 \
				--top_k 100 \
				--comp_budget 500 \
				--graph_metric l2 \
				--graph_type knn_e2e \
				--e2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/score_mats_eoe-0-last.ckpt/final_fantasy/ent_to_ent_scores_n_e_14044x14044_topk_100_embed_bienc_m2e_.pkl \
				--misc mstart_5500


				sbatch -p rtx8000-long --gres gpu:1 --mem 32GB --job-name nsw_fallout bin/run.sh \
				python eval/run_cross_encoder_w_nsw_eval.py \
				--data_name fallout \
				--n_ment -1 \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/model/eoe-0-last.ckpt \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/eval \
				--batch_size 150 \
				--embed_type bienc \
				--max_nbrs 10 \
				--beamsize 5 \
				--top_k 100 \
				--comp_budget 500 \
				--graph_metric l2 \
				--graph_type knn_e2e \
				--e2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/score_mats_eoe-0-last.ckpt/fallout/ent_to_ent_scores_n_e_16992x16992_topk_100_embed_bienc_m2e_.pkl



				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name nsw_coronation_street bin/run.sh \
				python eval/run_cross_encoder_w_nsw_eval.py \
				--data_name coronation_street \
				--n_ment -1 \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/model/eoe-0-last.ckpt \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/eval \
				--batch_size 150 \
				--embed_type bienc \
				--max_nbrs 10 \
				--beamsize 5 \
				--top_k 100 \
				--comp_budget 500 \
				--graph_metric l2 \
				--graph_type knn_e2e \
				--e2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/score_mats_eoe-0-last.ckpt/coronation_street/ent_to_ent_scores_n_e_17809x17809_topk_100_embed_bienc_m2e_.pkl

				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name nsw_elder_scrolls bin/run.sh \
				python eval/run_cross_encoder_w_nsw_eval.py \
				--data_name elder_scrolls \
				--n_ment -1 \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/model/eoe-0-last.ckpt \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/eval \
				--batch_size 150 \
				--embed_type bienc \
				--max_nbrs 10 \
				--beamsize 5 \
				--top_k 100 \
				--comp_budget 500 \
				--graph_metric l2 \
				--graph_type knn_e2e \
				--e2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/score_mats_eoe-0-last.ckpt/elder_scrolls/ent_to_ent_scores_n_e_21712x21712_topk_100_embed_bienc_m2e_.pkl

				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name nsw_ice_hockey bin/run.sh \
				python eval/run_cross_encoder_w_nsw_eval.py \
				--data_name ice_hockey \
				--n_ment -1 \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/model/eoe-0-last.ckpt \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/eval \
				--batch_size 150 \
				--embed_type bienc \
				--max_nbrs 10 \
				--beamsize 5 \
				--top_k 100 \
				--comp_budget 500 \
				--graph_metric l2 \
				--graph_type knn_e2e \
				--e2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/score_mats_eoe-0-last.ckpt/ice_hockey/ent_to_ent_scores_n_e_28684x28684_topk_100_embed_bienc_m2e_.pkl

				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name nsw_muppets bin/run.sh \
				python eval/run_cross_encoder_w_nsw_eval.py \
				--data_name muppets \
				--n_ment -1 \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/model/eoe-0-last.ckpt \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/eval \
				--batch_size 150 \
				--embed_type bienc \
				--max_nbrs 10 \
				--beamsize 5 \
				--top_k 100 \
				--comp_budget 500 \
				--graph_metric l2 \
				--graph_type knn_e2e \
				--e2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/score_mats_eoe-0-last.ckpt/muppets/ent_to_ent_scores_n_e_21344x21344_topk_100_embed_bienc_m2e_.pkl

				sbatch -p rtx8000-long --gres gpu:1 --mem 32GB --job-name nsw_pro_wrestling bin/run.sh \
				python eval/run_cross_encoder_w_nsw_eval.py \
				--data_name pro_wrestling \
				--n_ment -1 \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/model/eoe-0-last.ckpt \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/eval \
				--batch_size 150 \
				--embed_type bienc \
				--max_nbrs 10 \
				--beamsize 5 \
				--top_k 100 \
				--comp_budget 500 \
				--graph_metric l2 \
				--graph_type knn_e2e \
				--e2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/score_mats_eoe-0-last.ckpt/pro_wrestling/ent_to_ent_scores_n_e_10133x10133_topk_100_embed_bienc_m2e_.pkl


				sbatch -p rtx8000-long --gres gpu:1 --mem 32GB --job-name nsw_world_of_warcraft bin/run.sh \
				python eval/run_cross_encoder_w_nsw_eval.py \
				--data_name world_of_warcraft \
				--n_ment -1 \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/model/eoe-0-last.ckpt \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/eval \
				--batch_size 150 \
				--embed_type bienc \
				--max_nbrs 10 \
				--beamsize 5 \
				--top_k 100 \
				--comp_budget 500 \
				--graph_metric l2 \
				--graph_type knn_e2e \
				--e2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/score_mats_eoe-0-last.ckpt/world_of_warcraft/ent_to_ent_scores_n_e_27677x27677_topk_100_embed_bienc_m2e_.pkl



				Chunking jobs for starwars

					for mstart in 0 3000 6000 
					do
					sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name nsw_sw_$mstart bin/run.sh \
					python eval/run_cross_encoder_w_nsw_eval.py \
					--data_name starwars \
					--n_ment_start $mstart \
					--n_ment 3000 \
					--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/model/eoe-0-last.ckpt \
					--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
					--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/eval \
					--batch_size 150 \
					--embed_type bienc \
					--max_nbrs 10 \
					--beamsize 5 \
					--top_k 100 \
					--comp_budget 500 \
					--graph_metric l2 \
					--graph_type knn_e2e \
					--e2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/score_mats_eoe-0-last.ckpt/starwars/ent_to_ent_scores_n_e_87056x87056_topk_100_embed_bienc_m2e_.pkl \
					--misc mstart_$mstart
					done

					sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name nsw_sw_9K bin/run.sh \
					python eval/run_cross_encoder_w_nsw_eval.py \
					--data_name starwars \
					--n_ment_start 9000 \
					--n_ment 2824 \
					--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/model/eoe-0-last.ckpt \
					--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
					--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/eval \
					--batch_size 150 \
					--embed_type bienc \
					--max_nbrs 10 \
					--beamsize 5 \
					--top_k 100 \
					--comp_budget 500 \
					--graph_metric l2 \
					--graph_type knn_e2e \
					--e2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/score_mats_eoe-0-last.ckpt/starwars/ent_to_ent_scores_n_e_87056x87056_topk_100_embed_bienc_m2e_.pkl \
					--misc mstart_9K


				Chunking jobs for military
					for mstart in 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 11000 
					for mstart in 0
					do
					sbatch -p rtx8000-long --gres gpu:1 --mem 32GB --job-name nsw_mil_$mstart bin/run.sh \
					python eval/run_cross_encoder_w_nsw_eval.py \
					--data_name military \
					--n_ment_start $mstart \
					--n_ment 1000 \
					--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/model/eoe-0-last.ckpt \
					--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
					--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/eval \
					--batch_size 150 \
					--embed_type bienc \
					--max_nbrs 10 \
					--beamsize 5 \
					--top_k 100 \
					--comp_budget 500 \
					--graph_metric l2 \
					--graph_type knn_e2e \
					--e2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/score_mats_eoe-0-last.ckpt/military/ent_to_ent_scores_n_e_104520x104520_topk_100_embed_bienc_m2e_.pkl \
					--misc mstart_$mstart
					done

					sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name nsw_mil_12K bin/run.sh \
					python eval/run_cross_encoder_w_nsw_eval.py \
					--data_name military \
					--n_ment_start 12000 \
					--n_ment 1063 \
					--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/model/eoe-0-last.ckpt \
					--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
					--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/eval \
					--batch_size 150 \
					--embed_type bienc \
					--max_nbrs 10 \
					--beamsize 5 \
					--top_k 100 \
					--comp_budget 500 \
					--graph_metric l2 \
					--graph_type knn_e2e \
					--e2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/score_mats_eoe-0-last.ckpt/military/ent_to_ent_scores_n_e_104520x104520_topk_100_embed_bienc_m2e_.pkl \
					--misc mstart_12K


			Distilling information into a biencoder model (with cls pool and separate params - just like base biencoder model)

				sbatch -p rtx8000-long --gres gpu:2 --mem 32GB --job-name e_cross_distill_bienc bin/run.sh  \
				python models/train.py \
				--config config/el_zeshel_bi_enc.json \
				--exp_id 7_EntModel \
				--neg_strategy distill \
				--distill_n_labels 64 \
				--train_batch_size 8 \
				--path_to_model             ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
				--ent_w_score_file_template ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/eval/{}/m=-1_k=500_1_eoe-0-last.ckpt/crossenc_topk_preds_w_bienc_retrvr.txt \
				--reload_dataloaders_every_n_epochs 0 \
				--eval_batch_size 32 \
				--warmup_proportion 0.01 \
				--strategy ddp \
				--num_gpus 2 \
				--eval_interval 0.2 \
				--misc distill_w_64_of_500_bienc_negs_wrt_6_306_all_data

				sbatch -p rtx8000-long --gres gpu:2 --mem 32GB --job-name e_cross_distill_e2e bin/run.sh  \
				python models/train.py \
				--config config/el_zeshel_bi_enc.json \
				--exp_id 7_EntModel \
				--neg_strategy distill \
				--distill_n_labels 64 \
				--train_batch_size 8 \
				--path_to_model             ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
				--ent_w_score_file_template ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/eval/{}/m=-1_k=100_g=knn_e2e_e=bienc_10_5_500_/crossenc_topk_preds_w_graph.txt \
				--reload_dataloaders_every_n_epochs 0 \
				--eval_batch_size 32 \
				--warmup_proportion 0.01 \
				--strategy ddp \
				--num_gpus 2 \
				--eval_interval 0.2 \
				--misc distill_w_64_of_100_e2e_graph_negs_wrt_6_306_all_data

				sbatch -p rtx8000-long --gres gpu:2 --mem 32GB --job-name e_cross_distill_64 bin/run.sh  \
				python models/train.py \
				--config config/el_zeshel_bi_enc.json \
				--exp_id 7_EntModel \
				--neg_strategy distill \
				--distill_n_labels 64 \
				--train_batch_size 8 \
				--path_to_model             ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
				--ent_w_score_file_template ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/eval/{}/m=-1_k=64_1_eoe-0-last.ckpt/crossenc_topk_preds_w_bienc_retrvr.txt \
				--reload_dataloaders_every_n_epochs 0 \
				--eval_batch_size 32 \
				--warmup_proportion 0.01 \
				--strategy ddp \
				--num_gpus 2 \
				--eval_interval 0.2 \
				--misc distill_w_64_bienc_negs_wrt_6_306_all_data


			Add -> git commit info f5be9c66922536efae50d836773b42b978e55b82
			Training w/ reranked biencoder hard negs (for epoch 1, using params only) 

				sbatch -p rtx8000-long --gres gpu:2 --mem 64GB --job-name ece_bienc_rerank bin/run.sh  \
				python models/train.py \
				--config config/el_zeshel_cross_enc.json \
				--exp_id 6_ReprCrossEnc \
				--cross_enc_type w_embeds \
				--loss ce \
				--neg_strategy precomp \
				--num_negs 64 \
				--train_batch_size 4 \
				--ent_w_score_file_template ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/eval/{}/m=-1_k=500_1_eoe-0-last.ckpt/crossenc_topk_preds_w_bienc_retrvr.txt \
				--path_to_model             ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/model/eoe-0-last.ckpt \
				--reload_dataloaders_every_n_epochs 0 \
				--eval_batch_size 32 \
				--ckpt_metric mrr \
				--warmup_proportion 0.01 \
				--strategy ddp \
				--num_gpus 2 \
				--eval_interval 0.2 \
				--num_epochs 1 \
				--misc w_crossenc_w_embeds_6_306_w_64_reranked_of_500_bienc_hard_negs	

				Launching bi+cross eval w/ top-500 bienc entities

					for data in american_football doctor_who fallout final_fantasy world_of_warcraft
					for data in pro_wrestling
					for data in coronation_street elder_scrolls ice_hockey muppets
					do
					sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --exclude node176 --job-name m1cross_w_bi_$data bin/run.sh \
					python eval/run_cross_encoder_w_binenc_retriever_zeshel.py \
					--data_name $data \
					--n_ment -1 \
					--top_k 500 \
					--batch_size 1 \
					--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
					--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=precomp_s=1234_w_crossenc_w_embeds_6_306_w_64_reranked_of_500_bienc_hard_negs/eval \
					--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=precomp_s=1234_w_crossenc_w_embeds_6_306_w_64_reranked_of_500_bienc_hard_negs/model/eoe-0-last.ckpt \
					--misc eoe-0-last.ckpt
					done


					# For military

						for mstart in 0 2000 4000 6000 8000 10000  
						do
						sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --exclude node176 --job-name m1cross_w_bi_military-$mstart bin/run.sh \
						python eval/run_cross_encoder_w_binenc_retriever_zeshel.py \
						--data_name military \
						--n_ment_start $mstart \
						--n_ment 2000 \
						--top_k 500 \
						--batch_size 1 \
						--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
						--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=precomp_s=1234_w_crossenc_w_embeds_6_306_w_64_reranked_of_500_bienc_hard_negs/eval \
						--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=precomp_s=1234_w_crossenc_w_embeds_6_306_w_64_reranked_of_500_bienc_hard_negs/model/eoe-0-last.ckpt \
						--misc eoe-0-last.ckpt_mstart_$mstart
						done

						sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --exclude node176 --job-name m1cross_w_bi_military-90K bin/run.sh \
						python eval/run_cross_encoder_w_binenc_retriever_zeshel.py \
						--data_name military \
						--n_ment_start 12000 \
						--n_ment 1063 \
						--top_k 500 \
						--batch_size 1 \
						--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
						--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=precomp_s=1234_w_crossenc_w_embeds_6_306_w_64_reranked_of_500_bienc_hard_negs/eval \
						--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=precomp_s=1234_w_crossenc_w_embeds_6_306_w_64_reranked_of_500_bienc_hard_negs/model/eoe-0-last.ckpt \
						--misc eoe-0-last.ckpt_mstart_$mstart


					# For starwars

						# for mstart in 0 30000 
						for mstart in 0 2000 4000 6000 8000  
						do
						sbatch -p m40-long --gres gpu:1 --mem 32GB --exclude node176 --job-name m1cross_w_bi_starwars-$mstart bin/run.sh \
						python eval/run_cross_encoder_w_binenc_retriever_zeshel.py \
						--data_name starwars \
						--n_ment_start $mstart \
						--n_ment 2000 \
						--top_k 500 \
						--batch_size 1 \
						--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
						--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=precomp_s=1234_w_crossenc_w_embeds_6_306_w_64_reranked_of_500_bienc_hard_negs/eval \
						--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=precomp_s=1234_w_crossenc_w_embeds_6_306_w_64_reranked_of_500_bienc_hard_negs/model/eoe-0-last.ckpt \
						--misc eoe-0-last.ckpt_mstart_$mstart
						done

						sbatch -p m40-long --gres gpu:1 --mem 32GB --exclude node176 --job-name m1cross_w_bi_starwars-10K bin/run.sh \
						python eval/run_cross_encoder_w_binenc_retriever_zeshel.py \
						--data_name starwars \
						--n_ment_start 10000 \
						--n_ment 1824 \
						--top_k 500 \
						--batch_size 1 \
						--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
						--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=precomp_s=1234_w_crossenc_w_embeds_6_306_w_64_reranked_of_500_bienc_hard_negs/eval \
						--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=precomp_s=1234_w_crossenc_w_embeds_6_306_w_64_reranked_of_500_bienc_hard_negs/model/eoe-0-last.ckpt \
						--misc eoe-0-last.ckpt_mstart_$mstart


				Training w/ reranked biencoder hard negs for epoch = 2

					sbatch -p rtx8000-long --gres gpu:2 --mem 64GB --job-name ece_bienc_rerank2 bin/run.sh  \
					python models/train.py \
					--config config/el_zeshel_cross_enc.json \
					--exp_id 6_ReprCrossEnc \
					--cross_enc_type w_embeds \
					--loss ce \
					--neg_strategy precomp \
					--num_negs 64 \
					--train_batch_size 4 \
					--ent_w_score_file_template ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=precomp_s=1234_w_crossenc_w_embeds_6_306_w_64_reranked_of_500_bienc_hard_negs/eval/{}/m=-1_k=500_1_eoe-0-last.ckpt/crossenc_topk_preds_w_bienc_retrvr.txt \
					--path_to_model             ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=precomp_s=1234_w_crossenc_w_embeds_6_306_w_64_reranked_of_500_bienc_hard_negs/model/eoe-0-last.ckpt \
					--reload_dataloaders_every_n_epochs 0 \
					--eval_batch_size 32 \
					--ckpt_metric mrr \
					--warmup_proportion 0.01 \
					--strategy ddp \
					--num_gpus 2 \
					--eval_interval 0.2 \
					--num_epochs 1 \
					--misc w_crossenc_w_embeds_6_306_w_64_reranked_of_500_bienc_hard_negs/epoch_2	


			Training w/ reranked biencoder hard negs (for epoch 1, using ckpt)

				sbatch -p rtx8000-long --gres gpu:2 --mem 64GB --job-name ece_bienc_rerank_3 bin/run.sh  \
				python models/train.py \
				--config config/el_zeshel_cross_enc.json \
				--exp_id 6_ReprCrossEnc \
				--cross_enc_type w_embeds \
				--loss ce \
				--neg_strategy precomp \
				--num_negs 64 \
				--train_batch_size 4 \
				--ent_w_score_file_template ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/eval/{}/m=-1_k=500_1_eoe-0-last.ckpt/crossenc_topk_preds_w_bienc_retrvr.txt \
				--ckpt_path             	../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/model/eoe-0-last.ckpt \
				--reload_dataloaders_every_n_epochs 0 \
				--eval_batch_size 32 \
				--ckpt_metric mrr \
				--warmup_proportion 0.01 \
				--strategy ddp \
				--num_gpus 2 \
				--eval_interval 0.2 \
				--num_epochs 3 \
				--misc w_crossenc_w_embeds_6_306_w_64_reranked_of_500_bienc_hard_negs/epoch_1_3_w_ckpt	

			
			Training w/ hard negs mined using e2e graph

				sbatch -p rtx8000-long --gres gpu:2 --mem 64GB --job-name ece_graph_rerank bin/run.sh  \
				python models/train.py \
				--config config/el_zeshel_cross_enc.json \
				--exp_id 6_ReprCrossEnc \
				--cross_enc_type w_embeds \
				--loss ce \
				--neg_strategy precomp \
				--num_negs 64 \
				--train_batch_size 4 \
				--ent_w_score_file_template ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/eval/{}/m=-1_k=100_g=knn_e2e_e=bienc_10_5_500_/crossenc_topk_preds_w_graph.txt \
				--path_to_model             ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/model/eoe-0-last.ckpt \
				--reload_dataloaders_every_n_epochs 0 \
				--eval_batch_size 32 \
				--ckpt_metric mrr \
				--warmup_proportion 0.01 \
				--strategy ddp \
				--num_gpus 2 \
				--eval_interval 0.2 \
				--num_epochs 1 \
				--misc w_crossenc_w_embeds_6_306_w_64_reranked_of_100_e2e_graph_hard_negs


				Launching e2e score computation - fix model name, also launch top-1000 bienc inference, and then laucnh graph search w/ e2e graph. Then train for another 1 epoch and repeat this process.

					for data in american_football doctor_who fallout final_fantasy military pro_wrestling starwars world_of_warcraft 
					for data in american_football doctor_who fallout final_fantasy pro_wrestling world_of_warcraft 
					for data in coronation_street elder_scrolls ice_hockey muppets
					do
					sbatch -p 2080ti-long --gres gpu:1 --mem 32GB  --exclude node176 --job-name e2e-ece0-$data bin/run.sh  \
					bin/run.sh \
					python eval/run_cross_encoder_for_ent_ent_matrix.py \
					--data_name $data \
					--n_ent_x -1 \
					--n_ent_y -1 \
					--topk 100 \
					--embed_type bienc \
					--token_opt m2e \
					--batch_size 150 \
					--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=precomp_s=1234_w_crossenc_w_embeds_6_306_w_64_reranked_of_100_e2e_graph_hard_negs/score_mats_eoe-0-last.ckpt \
					--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=precomp_s=1234_w_crossenc_w_embeds_6_306_w_64_reranked_of_100_e2e_graph_hard_negs/model/eoe-0-last.ckpt \
					--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt 
					done

					# Chunking computation for domain=military
					for xstart in 0 30000 60000 
					do
					sbatch -p 2080ti-long --gres gpu:1 --mem 32GB  --job-name e2e-ece0-miliary-$xstart bin/run.sh  \
					bin/run.sh \
					python eval/run_cross_encoder_for_ent_ent_matrix.py \
					--data_name military \
					--n_ent_x_start $xstart \
					--n_ent_x  30000 \
					--n_ent_y -1 \
					--topk 100 \
					--embed_type bienc \
					--token_opt m2e \
					--batch_size 150 \
					--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=precomp_s=1234_w_crossenc_w_embeds_6_306_w_64_reranked_of_100_e2e_graph_hard_negs/score_mats_eoe-0-last.ckpt \
					--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=precomp_s=1234_w_crossenc_w_embeds_6_306_w_64_reranked_of_100_e2e_graph_hard_negs/model/eoe-0-last.ckpt \
					--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
					--misc x_start_$xstart
					done

					sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name e2e-ece0-miliary-90K bin/run.sh  \
					bin/run.sh \
					python eval/run_cross_encoder_for_ent_ent_matrix.py \
					--data_name military \
					--n_ent_x_start 90000 \
					--n_ent_x  14520 \
					--n_ent_y -1 \
					--topk 100 \
					--embed_type bienc \
					--token_opt m2e \
					--batch_size 150 \
					--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=precomp_s=1234_w_crossenc_w_embeds_6_306_w_64_reranked_of_100_e2e_graph_hard_negs/score_mats_eoe-0-last.ckpt \
					--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=precomp_s=1234_w_crossenc_w_embeds_6_306_w_64_reranked_of_100_e2e_graph_hard_negs/model/eoe-0-last.ckpt \
					--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
					--misc x_start_90000

					# Chunking computation for domain=starwars
					for xstart in 0 30000 
					do
					sbatch -p 2080ti-long --gres gpu:1 --mem 32GB  --job-name e2e-ece0-starwars-$xstart bin/run.sh  \
					bin/run.sh \
					python eval/run_cross_encoder_for_ent_ent_matrix.py \
					--data_name starwars \
					--n_ent_x_start $xstart \
					--n_ent_x  30000 \
					--n_ent_y -1 \
					--topk 100 \
					--embed_type bienc \
					--token_opt m2e \
					--batch_size 150 \
					--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=precomp_s=1234_w_crossenc_w_embeds_6_306_w_64_reranked_of_100_e2e_graph_hard_negs/score_mats_eoe-0-last.ckpt \
					--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=precomp_s=1234_w_crossenc_w_embeds_6_306_w_64_reranked_of_100_e2e_graph_hard_negs/model/eoe-0-last.ckpt \
					--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
					--misc x_start_$xstart
					done

					sbatch -p 2080ti-long --gres gpu:1 --mem 32GB  --job-name e2e-ece0-starwars-60K bin/run.sh  \
					bin/run.sh \
					python eval/run_cross_encoder_for_ent_ent_matrix.py \
					--data_name starwars \
					--n_ent_x_start 60000 \
					--n_ent_x  27056 \
					--n_ent_y -1 \
					--topk 100 \
					--embed_type bienc \
					--token_opt m2e \
					--batch_size 150 \
					--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=precomp_s=1234_w_crossenc_w_embeds_6_306_w_64_reranked_of_100_e2e_graph_hard_negs/score_mats_eoe-0-last.ckpt \
					--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=precomp_s=1234_w_crossenc_w_embeds_6_306_w_64_reranked_of_100_e2e_graph_hard_negs/model/eoe-0-last.ckpt \
					--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
					--misc x_start_60000


			Training w/ just simple biencoder hard negs from epoch 0 ckpt - using params only 

				sbatch -p rtx8000-long --gres gpu:2 --mem 64GB --job-name e-crossenc bin/run.sh  \
				python models/train.py \
				--config config/el_zeshel_cross_enc.json \
				--exp_id 6_ReprCrossEnc \
				--cross_enc_type w_embeds \
				--loss ce \
				--neg_strategy bienc_hard_negs \
				--num_negs 63 \
				--train_batch_size 4 \
				--neg_mine_bienc_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
				--path_to_model             ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/model/eoe-0-last.ckpt \
				--reload_dataloaders_every_n_epochs 0 \
				--eval_batch_size 32 \
				--ckpt_metric mrr \
				--warmup_proportion 0.01 \
				--strategy ddp \
				--num_gpus 2 \
				--eval_interval 0.2 \
				--num_epochs 1 \
				--misc w_crossenc_w_embeds_6_306_w_63_negs	


			Training w/ just simple biencoder hard negs from epoch 0 ckpt - using as ckpt 


				sbatch -p rtx8000-long --gres gpu:2 --mem 64GB --job-name e-crossenc_as_ckpt bin/run.sh  \
				python models/train.py \
				--config config/el_zeshel_cross_enc.json \
				--exp_id 6_ReprCrossEnc \
				--cross_enc_type w_embeds \
				--loss ce \
				--neg_strategy bienc_hard_negs \
				--num_negs 63 \
				--train_batch_size 4 \
				--neg_mine_bienc_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
				--ckpt_path             	../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/model/eoe-0-last.ckpt \
				--reload_dataloaders_every_n_epochs 0 \
				--eval_batch_size 32 \
				--ckpt_metric mrr \
				--warmup_proportion 0.01 \
				--strategy ddp \
				--num_gpus 2 \
				--eval_interval 0.2 \
				--num_epochs 2 \
				--misc w_crossenc_w_embeds_6_306_w_63_negs/as_ckpt	


			Training w/ reranked distilled biencoder (7-67) hard negs (for epoch 1, using ckpt)

				Launching bi+cross eval w/ top-500 bienc entities
			
					for data in american_football doctor_who fallout final_fantasy coronation_street elder_scrolls ice_hockey muppets
					for data in pro_wrestling world_of_warcraft 
					do
					sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name m1cross_w_bi_$data bin/run.sh \
					python eval/run_cross_encoder_w_binenc_retriever_zeshel.py \
					--data_name $data \
					--n_ment -1 \
					--top_k 500 \
					--batch_size 1 \
					--bi_model_file    ../../results/7_EntModel/d=ent_link/m=bi_enc_l=ce_neg=distill_s=1234_distill_w_64_bienc_negs_wrt_6_306_all_data/model/model-3-9855.0-1.54.ckpt  \
					--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/eval \
					--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/model/eoe-0-last.ckpt \
					--misc eoe-0-last.ckpt_7_67_bienc
					done

					# For military

					for mstart in 0 2000 4000 6000 8000 10000  
					do
					sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name m1cross_w_bi_military-$mstart bin/run.sh \
					python eval/run_cross_encoder_w_binenc_retriever_zeshel.py \
					--data_name military \
					--n_ment_start $mstart \
					--n_ment 2000 \
					--top_k 500 \
					--batch_size 1 \
					--bi_model_file    ../../results/7_EntModel/d=ent_link/m=bi_enc_l=ce_neg=distill_s=1234_distill_w_64_bienc_negs_wrt_6_306_all_data/model/model-3-9855.0-1.54.ckpt  \
					--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/eval \
					--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/model/eoe-0-last.ckpt \
					--misc eoe-0-last.ckpt_7_67_bienc_mstart_$mstart
					done

					sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name m1cross_w_bi_military-12K bin/run.sh \
					python eval/run_cross_encoder_w_binenc_retriever_zeshel.py \
					--data_name military \
					--n_ment_start 12000 \
					--n_ment 1063 \
					--top_k 500 \
					--batch_size 1 \
					--bi_model_file    ../../results/7_EntModel/d=ent_link/m=bi_enc_l=ce_neg=distill_s=1234_distill_w_64_bienc_negs_wrt_6_306_all_data/model/model-3-9855.0-1.54.ckpt  \
					--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/eval \
					--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/model/eoe-0-last.ckpt \
					--misc eoe-0-last.ckpt_7_67_bienc_mstart_12K


					# For starwars

					# for mstart in 0 30000 
					for mstart in 0 2000 4000 6000 8000  
					do
					sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name m1cross_w_bi_starwars-$mstart bin/run.sh \
					python eval/run_cross_encoder_w_binenc_retriever_zeshel.py \
					--data_name starwars \
					--n_ment_start $mstart \
					--n_ment 2000 \
					--top_k 500 \
					--batch_size 1 \
					--bi_model_file    ../../results/7_EntModel/d=ent_link/m=bi_enc_l=ce_neg=distill_s=1234_distill_w_64_bienc_negs_wrt_6_306_all_data/model/model-3-9855.0-1.54.ckpt  \
					--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/eval \
					--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/model/eoe-0-last.ckpt \
					--misc eoe-0-last.ckpt_7_67_bienc_mstart_$mstart
					done

					sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name m1cross_w_bi_starwars-10K bin/run.sh \
					python eval/run_cross_encoder_w_binenc_retriever_zeshel.py \
					--data_name starwars \
					--n_ment_start 10000 \
					--n_ment 1824 \
					--top_k 500 \
					--batch_size 1 \
					--bi_model_file    ../../results/7_EntModel/d=ent_link/m=bi_enc_l=ce_neg=distill_s=1234_distill_w_64_bienc_negs_wrt_6_306_all_data/model/model-3-9855.0-1.54.ckpt  \
					--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/eval \
					--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/model/eoe-0-last.ckpt \
					--misc eoe-0-last.ckpt_7_67_bienc_mstart_19K

				Actual training job launch
					sbatch -p rtx8000-long --gres gpu:2 --mem 64GB --job-name ece_bienc_rerank_dis_3 bin/run.sh  \
					python models/train.py \
					--config config/el_zeshel_cross_enc.json \
					--exp_id 6_ReprCrossEnc \
					--cross_enc_type w_embeds \
					--loss ce \
					--neg_strategy precomp \
					--num_negs 64 \
					--train_batch_size 4 \
					--ent_w_score_file_template ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/eval/{}/m=-1_k=500_1_eoe-0-last.ckpt_7_67_bienc/crossenc_topk_preds_w_bienc_retrvr.txt \
					--ckpt_path             	../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/model/eoe-0-last.ckpt \
					--reload_dataloaders_every_n_epochs 0 \
					--eval_batch_size 32 \
					--ckpt_metric mrr \
					--warmup_proportion 0.01 \
					--strategy ddp \
					--num_gpus 2 \
					--eval_interval 0.2 \
					--num_epochs 3 \
					--misc w_crossenc_w_embeds_6_306_w_64_reranked_of_500_bienc_hard_negs/epoch_1_3_w_ckpt_w_6_67_bienc	

		Debug 


			python eval/run_cross_encoder_w_nsw_eval.py \
			--data_name final_fantasy \
			--n_ment 10 \
			--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/model/eoe-0-last.ckpt \
			--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
			--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/eval \
			--batch_size 32 \
			--embed_type bienc \
			--max_nbrs 10 \
			--beamsize 5 \
			--top_k 100 \
			--comp_budget 500 \
			--graph_metric l2 \
			--graph_type knn_e2e \
			--e2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_e2e_graph/score_mats_eoe-0-last.ckpt/final_fantasy/ent_to_ent_scores_n_e_14044x14044_topk_100_embed_bienc_m2e_.pkl \
			--misc debug

				Debugging NSW w/ E2E graph

					python eval/nsw_eval_zeshel.py  \
					--data_name pro_wrestling  \
					--n_ment 100  \
					--embed_type none  \
					--entry_method random  \
					--graph_type nsw_e2e  \
					--graph_metric l2  \
					--misc c=05_E-CrossEnc_6_256_b=00_HardNegs_6_20_n_m=100  \
					--bi_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
					--e2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt/pro_wrestling/ent_to_ent_scores_n_e_10133x10133_topk_100_embed_bienc_m2e_.pkl  \
					--score_mat_dir ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt  \
					--res_dir ../../results/6_ReprCrossEnc/Graph_Search/pro_wrestling  \
					--force_exact_init_search 1  





	13 May
		
		Training e-cross-encoder w/ all layers
			sbatch -p rtx8000-long --gres gpu:2 --mem 64GB --job-name e-crossenc_all_layers bin/run.sh  \
			python models/train.py \
			--config config/el_zeshel_cross_enc.json \
			--exp_id 6_ReprCrossEnc \
			--cross_enc_type w_embeds \
			--loss ce \
			--neg_strategy bienc_hard_negs \
			--num_negs 63 \
			--use_all_layers 1 \
			--train_batch_size 4 \
			--neg_mine_bienc_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
			--reload_dataloaders_every_n_epochs 0 \
			--eval_batch_size 8 \
			--ckpt_metric loss \
			--warmup_proportion 0.01 \
			--strategy ddp \
			--num_gpus 2 \
			--eval_interval 0.2 \
			--num_epochs 4 \
			--misc 63_negs_w_crossenc_w_embeds_all_layers

		Training cls-cross-encoder w/ all layers

			sbatch -p rtx8000-long --gres gpu:2 --mem 64GB --job-name cls-crossenc_all_layers bin/run.sh  \
			python models/train.py \
			--config config/el_zeshel_cross_enc.json \
			--exp_id 6_ReprCrossEnc \
			--cross_enc_type default \
			--loss ce \
			--neg_strategy bienc_hard_negs \
			--num_negs 63 \
			--use_all_layers 1 \
			--train_batch_size 4 \
			--neg_mine_bienc_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
			--reload_dataloaders_every_n_epochs 0 \
			--eval_batch_size 8 \
			--ckpt_metric loss \
			--warmup_proportion 0.01 \
			--strategy ddp \
			--num_gpus 2 \
			--eval_interval 0.2 \
			--num_epochs 4 \
			--misc 63_negs_w_cls_crossenc_all_layers

		Training w/ cls cross-encoder w/ all layers and cls pooling

			sbatch -p rtx8000-long --gres gpu:2 --mem 64GB --job-name cls-crossenc_all_layers bin/run.sh  \
			python models/train.py \
			--config config/el_zeshel_cross_enc.json \
			--exp_id 6_ReprCrossEnc \
			--cross_enc_type default \
			--pooling_type cls \
			--loss ce \
			--neg_strategy bienc_hard_negs \
			--num_negs 63 \
			--use_all_layers 1 \
			--train_batch_size 4 \
			--neg_mine_bienc_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
			--reload_dataloaders_every_n_epochs 0 \
			--eval_batch_size 8 \
			--ckpt_metric loss \
			--warmup_proportion 0.01 \
			--strategy ddp \
			--num_gpus 2 \
			--eval_interval 0.2 \
			--num_epochs 4 \
			--misc 63_negs_w_cls_crossenc_all_layers

		Debug
			python models/train.py \
			--config config/el_zeshel_cross_enc_debug.json \
			--exp_id _0_Debug \
			--cross_enc_type w_embeds \
			--loss ce \
			--neg_strategy random \
			--num_negs 5 \
			--train_batch_size 4 \
			--grad_acc_steps 2 \
			--neg_mine_bienc_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
			--reload_dataloaders_every_n_epochs 0 \
			--eval_batch_size 32 \
			--ckpt_metric mrr \
			--warmup_proportion 0.01 \
			--eval_interval 0.9 \
			--num_epochs 1 \
			--use_all_layers 1 \
			--misc all_layers	




	15 May
		Debug Eval using all layers
			python eval/run_cross_encoder_w_binenc_retriever_zeshel.py \
			--data_name lego \
			--n_ment 10 \
			--top_k 64 \
			--batch_size 1 \
			--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
			--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_all_layers/eval \
			--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_all_layers/model/model-1-8599.0-1.70.ckpt \
			--use_all_layers 1 \
			--misc debug_all_layers

			python eval/run_cross_encoder_w_binenc_retriever_zeshel.py \
			--data_name lego \
			--n_ment 10 \
			--top_k 64 \
			--batch_size 1 \
			--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
			--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/eval \
			--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/model/model-2-15999.0--79.46.ckpt \
			--use_all_layers 1 \
			--misc debug_all_layers


			
	17 May
		
		Train biencoder with separate label encoder and input encoder

			sbatch -p rtx8000-long --gres gpu:2 --mem 64GB --job-name bienc_8_spl_tkn bin/run.sh  \
			python models/train.py \
			--config config/el_zeshel_bi_enc.json \
			--exp_id 6_ReprCrossEnc \
			--bi_enc_type separate \
			--pooling_type spl_tkns \
			--neg_strategy bienc_hard_negs \
			--num_negs 63 \
			--train_batch_size 8 \
			--eval_batch_size 64 \
			--reload_dataloaders_every_n_epochs 1 \
			--num_epochs 8 \
			--warmup_proportion 0.01 \
			--strategy ddp \
			--num_gpus 2 \
			--eval_interval 0.2 \
			--misc 63_hard_negs_w_separate_params_n_spl_tkn_pool_8_epochs


			sbatch -p rtx8000-long --gres gpu:2 --mem 64GB --job-name bienc_8_cls bin/run.sh  \
			python models/train.py \
			--config config/el_zeshel_bi_enc.json \
			--exp_id 6_ReprCrossEnc \
			--bi_enc_type separate \
			--pooling_type cls \
			--neg_strategy bienc_hard_negs \
			--num_negs 63 \
			--train_batch_size 8 \
			--eval_batch_size 64 \
			--reload_dataloaders_every_n_epochs 1 \
			--num_epochs 8 \
			--warmup_proportion 0.01 \
			--strategy ddp \
			--num_gpus 2 \
			--eval_interval 0.2 \
			--misc 63_hard_negs_w_separate_params_n_cls_pool_8_epochs

	19 May

		Train biencoder and cross-encoder w/ shared parameters and mutual distillation loss 
			For spl-tkn pooling
				sbatch -p rtx8000-long --gres gpu:2 --mem 64GB --job-name mutual_distill_ece bin/run.sh  \
				python models/train.py \
				--config config/el_zeshel_cross_enc.json \
				--exp_id 6_ReprCrossEnc \
				--cross_enc_type w_embeds \
				--loss ce \
				--neg_strategy bienc_hard_negs \
				--joint_train_alpha 0.5 \
				--mutual_distill_alpha 0.5 \
				--num_negs 63 \
				--train_batch_size 4 \
				--reload_dataloaders_every_n_epochs 1 \
				--eval_batch_size 32 \
				--ckpt_metric mrr \
				--warmup_proportion 0.01 \
				--strategy ddp \
				--num_gpus 2 \
				--eval_interval 0.2 \
				--misc 63_negs_w_crossenc_w_embeds_w_0.5_bi_cross_loss_0.5_mutual_distill_from_scratch


			For cls_w_lin pooling
				sbatch -p rtx8000-long --gres gpu:2 --mem 64GB --job-name mutual_distill_cls bin/run.sh  \
				python models/train.py \
				--config config/el_zeshel_cross_enc.json \
				--exp_id 6_ReprCrossEnc \
				--loss ce \
				--neg_strategy bienc_hard_negs \
				--joint_train_alpha 0.5 \
				--mutual_distill_alpha 0.5 \
				--num_negs 63 \
				--train_batch_size 4 \
				--reload_dataloaders_every_n_epochs 1 \
				--eval_batch_size 32 \
				--ckpt_metric mrr \
				--warmup_proportion 0.01 \
				--strategy ddp \
				--num_gpus 2 \
				--eval_interval 0.2 \
				--misc 63_negs_w_cls_w_lin_w_0.5_bi_cross_loss_0.5_mutual_distill_from_scratch

			For cls pooling
				sbatch -p rtx8000-long --gres gpu:2 --mem 64GB --job-name mutual_distill_cls bin/run.sh  \
				python models/train.py \
				--config config/el_zeshel_cross_enc.json \
				--exp_id 6_ReprCrossEnc \
				--loss ce \
				--pooling_type cls \
				--neg_strategy bienc_hard_negs \
				--joint_train_alpha 0.5 \
				--mutual_distill_alpha 0.5 \
				--num_negs 63 \
				--train_batch_size 4 \
				--reload_dataloaders_every_n_epochs 1 \
				--eval_batch_size 32 \
				--ckpt_metric mrr \
				--warmup_proportion 0.01 \
				--strategy ddp \
				--num_gpus 2 \
				--eval_interval 0.2 \
				--misc 63_negs_w_cls_wo_lin_w_0.5_bi_cross_loss_0.5_mutual_distill_from_scratch

		Train biencoder and cross-encoder w/ shared parameters and cls pool (not cls_w_lin pool)
			sbatch -p rtx8000-long --gres gpu:2 --mem 64GB --job-name joint_bi_cross_cls bin/run.sh  \
			python models/train.py \
			--config config/el_zeshel_cross_enc.json \
			--exp_id 6_ReprCrossEnc \
			--cross_enc_type default \
			--pooling_type cls \
			--loss ce \
			--neg_strategy bienc_hard_negs \
			--joint_train_alpha 0.5 \
			--num_negs 63 \
			--train_batch_size 4 \
			--reload_dataloaders_every_n_epochs 1 \
			--eval_batch_size 32 \
			--ckpt_metric mrr \
			--warmup_proportion 0.01 \
			--strategy ddp \
			--num_gpus 2 \
			--eval_interval 0.2 \
			--misc 63_negs_w_cls_wo_lin_crossenc_w_0.5_bi_cross_loss_from_scratch
		

		Launching ment2ent score for cross&bi models w/ shared params and trained from scratch


				for data in lego pro_wrestling
				do

				# sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name mat1-ce-$data  bin/run.sh  \
				# python eval/run_cross_encoder_for_ment_ent_matrix_zeshel.py \
				# --n_ment 100 \
				# --n_ent -1 \
				# --batch_size 300 \
				# --data_name $data \
				# --layers final \
				# --cross_model_ckpt ../../results/6_ReprCrossEnc/d=ent_link/joint_train/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_w_0.5_bi_cross_loss_from_scratch/model/model-3-19679.0--77.23.ckpt \
				# --res_dir          ../../results/6_ReprCrossEnc/d=ent_link/joint_train/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_w_0.5_bi_cross_loss_from_scratch/score_mats_model-3-19679.0--77.23.ckpt

				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name mat2-ce-$data  bin/run.sh  \
				python eval/run_cross_encoder_for_ment_ent_matrix_zeshel.py \
				--n_ment 100 \
				--n_ent -1 \
				--batch_size 300 \
				--data_name $data \
				--layers final \
				--cross_model_ckpt ../../results/6_ReprCrossEnc/d=ent_link/joint_train/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_crossenc_w_0.5_bi_cross_loss_from_scratch/model/model-3-20919.0--80.86.ckpt \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/joint_train/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_crossenc_w_0.5_bi_cross_loss_from_scratch/score_mats_model-3-20919.0--80.86.ckpt
				done



	20 May

	Cross-Enc Matrix Computation for yugioh dataset for multiple models
		for data in yugioh
		do
			sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name mat1-ce-$data  bin/run.sh  \
			python eval/run_cross_encoder_for_ment_ent_matrix_zeshel.py \
		   --n_ment 100 \
		   --n_ent -1 \
		   --batch_size 300 \
		   --data_name $data \
		   --layers final \
		   --cross_model_ckpt ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=rank_ce_neg=bienc_hard_negs_w_knn_rank_s=1234_63_negs_w_ddp_w_cls_w_lin_d2p_neg_lin/model/model-1-12279.0-3.21.ckpt \
		   --res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=rank_ce_neg=bienc_hard_negs_w_knn_rank_s=1234_63_negs_w_ddp_w_cls_w_lin_d2p_neg_lin/score_mats_model-1-12279.0-3.21.ckpt

		   sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name mat2-ce-$data  bin/run.sh  \
			python eval/run_cross_encoder_for_ment_ent_matrix_zeshel.py \
		   --n_ment 100 \
		   --n_ent -1 \
		   --batch_size 300 \
		   --data_name $data \
		   --layers final \
		   --cross_model_ckpt ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_w_bienc_w_ddp_w_best_wrt_dev_mrr_cls_w_lin/model/model-1-11359.0--80.19.ckpt \
		   --res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_w_bienc_w_ddp_w_best_wrt_dev_mrr_cls_w_lin/score_mats_model-1-11359.0--80.19.ckpt

		   sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name mat3-ce-$data  bin/run.sh  \
			python eval/run_cross_encoder_for_ment_ent_matrix_zeshel.py \
		   --n_ment 100 \
		   --n_ent -1 \
		   --batch_size 300 \
		   --data_name $data \
		   --layers final \
		   --cross_model_ckpt ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/model/model-2-15999.0--79.46.ckpt \
		   --res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt

		   sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name mat4-ce-$data  bin/run.sh  \
			python eval/run_cross_encoder_for_ment_ent_matrix_zeshel.py \
		   --n_ment 100 \
		   --n_ent -1 \
		   --batch_size 300 \
		   --data_name $data \
		   --layers final \
		   --cross_model_ckpt ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=rank_ce_neg=bienc_hard_negs_w_knn_rank_s=1234_63_negs_w_crossenc_w_embeds/model/model-1-12279.0-3.19.ckpt \
		   --res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=rank_ce_neg=bienc_hard_negs_w_knn_rank_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-1-12279.0-3.19.ckpt


		   sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name mat5-ce-$data  bin/run.sh  \
			python eval/run_cross_encoder_for_ment_ent_matrix_zeshel.py \
		   --n_ment 100 \
		   --n_ent -1 \
		   --batch_size 300 \
		   --data_name $data \
		   --layers final \
		   --cross_model_ckpt ../../results/6_ReprCrossEnc/d=ent_link/joint_train/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_6_20_bienc_w_crossenc_w_embeds_w_0.5_bi_cross_loss/model/model-1-12279.0--78.92.ckpt \
		   --res_dir          ../../results/6_ReprCrossEnc/d=ent_link/joint_train/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_6_20_bienc_w_crossenc_w_embeds_w_0.5_bi_cross_loss/score_mats_model-1-12279.0--78.92.ckpt

		   sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name mat6-ce-$data  bin/run.sh  \
			python eval/run_cross_encoder_for_ment_ent_matrix_zeshel.py \
		   --n_ment 100 \
		   --n_ent -1 \
		   --batch_size 300 \
		   --data_name $data \
		   --layers final \
		   --cross_model_ckpt ../../results/6_ReprCrossEnc/d=ent_link/joint_train/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_6_20_bienc_w_cls_crossenc_w_0.5_bi_cross_loss/model/model-2-18439.0--79.21.ckpt \
		   --res_dir          ../../results/6_ReprCrossEnc/d=ent_link/joint_train/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_6_20_bienc_w_cls_crossenc_w_0.5_bi_cross_loss/score_mats_model-2-18439.0--79.21.ckpt

		   sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name mat7-ce-$data  bin/run.sh  \
			python eval/run_cross_encoder_for_ment_ent_matrix_zeshel.py \
		   --n_ment 100 \
		   --n_ent -1 \
		   --batch_size 300 \
		   --data_name $data \
		   --layers final \
		   --cross_model_ckpt ../../results/6_ReprCrossEnc/d=ent_link/joint_train/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_w_0.5_bi_cross_loss_from_scratch/model/model-3-19679.0--77.23.ckpt \
		   --res_dir          ../../results/6_ReprCrossEnc/d=ent_link/joint_train/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_w_0.5_bi_cross_loss_from_scratch/score_mats_model-3-19679.0--77.23.ckpt

		   sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name mat8-ce-$data  bin/run.sh  \
			python eval/run_cross_encoder_for_ment_ent_matrix_zeshel.py \
		   --n_ment 100 \
		   --n_ent -1 \
		   --batch_size 300 \
		   --data_name $data \
		   --layers final \
		   --cross_model_ckpt ../../results/6_ReprCrossEnc/d=ent_link/joint_train/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_crossenc_w_0.5_bi_cross_loss_from_scratch/model/model-3-20919.0--80.86.ckpt \
		   --res_dir          ../../results/6_ReprCrossEnc/d=ent_link/joint_train/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_crossenc_w_0.5_bi_cross_loss_from_scratch/score_mats_model-3-20919.0--80.86.ckpt

		done


	21 May

		Debug - Running retrieve-and-rerank together with some exact inference

			python eval/run_cross_encoder_w_binenc_retriever_zeshel.py \
			--data_name lego \
			--n_ment 10 \
			--top_k 2 \
			--batch_size 1 \
			--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
			--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_distill_s=1234_trn_pro_only_w_crossenc_w_embeds/eval \
			--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_distill_s=1234_trn_pro_only_w_crossenc_w_embeds/model/model-23-4119.0-2.87.ckpt \
			--misc debug_model-23-4119.0-2.87.ckpt


		Running retrieve-and-rerank together with some exact inference
			# m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_w_bienc_w_ddp_w_best_wrt_dev_mrr_cls_w_lin/model/model-1-11359.0--80.19.ckpt
			# m=cross_enc_l=ce_neg=bienc_hard_negs_w_rerank_s=1234_63_from_500_negs_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/model/model-1-12279.0--79.47.ckpt
			# m=cross_enc_l=ce_neg=bienc_nsw_search_s=1234_64_negs_5_bs_10_max_nbrs_500_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/model/model-1-12159.0--79.02.ckpt
			# m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/model/model-2-15999.0--79.46.ckpt
			# m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_w_crossenc_w_embeds_6_306_w_63_negs/as_ckpt/model/model-1-12279.0--79.46.ckpt
			# m=cross_enc_l=ce_neg=precomp_s=1234_w_crossenc_w_embeds_6_306_w_64_reranked_of_500_bienc_hard_negs/epoch_1_3_w_ckpt/model/model-2-18439.0--77.09.ckpt
			# m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_w_0.5_bi_cross_loss_from_scratch/model/model-3-19679.0--77.23.ckpt
			# m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_crossenc_w_0.5_bi_cross_loss_from_scratch/model/model-3-20919.0--80.86.ckpt

			# (Also w/ self retrieval)

			# m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_w_0.5_bi_cross_loss_from_scratch/model/model-3-19679.0--77.23.ckpt
			# m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_crossenc_w_0.5_bi_cross_loss_from_scratch/model/model-3-20919.0--80.86.ckpt


				for data in star_trek elder_scrolls lego forgotten_realms yugioh coronation_street ice_hockey muppets
				do
				sbatch -p gpu --gres gpu:1 --mem 32GB --job-name m1cross_w_bi_$data --exclude gpu-0-0 bin/run.sh \
				python eval/run_cross_encoder_w_binenc_retriever_zeshel.py \
				--data_name $data \
				--n_ment -1 \
				--top_k 64 \
				--batch_size 1 \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_w_bienc_w_ddp_w_best_wrt_dev_mrr_cls_w_lin/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_w_bienc_w_ddp_w_best_wrt_dev_mrr_cls_w_lin/model/model-1-11359.0--80.19.ckpt \
				--misc model-1-11359.0--80.19.ckpt

				sbatch -p gpu --gres gpu:1 --mem 32GB --job-name m2cross_w_bi_$data --exclude gpu-0-0 bin/run.sh \
				python eval/run_cross_encoder_w_binenc_retriever_zeshel.py \
				--data_name $data \
				--n_ment -1 \
				--top_k 64 \
				--batch_size 1 \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_w_rerank_s=1234_63_from_500_negs_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_w_rerank_s=1234_63_from_500_negs_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/model/model-1-12279.0--79.47.ckpt \
				--misc model-1-12279.0--79.47.ckpt

				sbatch -p gpu --gres gpu:1 --mem 32GB --job-name m3cross_w_bi_$data --exclude gpu-0-0 bin/run.sh \
				python eval/run_cross_encoder_w_binenc_retriever_zeshel.py \
				--data_name $data \
				--n_ment -1 \
				--top_k 64 \
				--batch_size 1 \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_nsw_search_s=1234_64_negs_5_bs_10_max_nbrs_500_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_nsw_search_s=1234_64_negs_5_bs_10_max_nbrs_500_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/model/model-1-12159.0--79.02.ckpt \
				--misc model-1-12159.0--79.02.ckpt
				done

				for data in star_trek elder_scrolls lego forgotten_realms yugioh coronation_street ice_hockey muppets
				do
				sbatch -p gpu --gres gpu:1 --mem 32GB --job-name m4cross_w_bi_$data --exclude gpu-0-0 bin/run.sh \
				python eval/run_cross_encoder_w_binenc_retriever_zeshel.py \
				--data_name $data \
				--n_ment -1 \
				--top_k 64 \
				--batch_size 1 \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/model/model-2-15999.0--79.46.ckpt \
				--misc model-2-15999.0--79.46.ckpt

				sbatch -p gpu --gres gpu:1 --mem 32GB --job-name m5cross_w_bi_$data --exclude gpu-0-0 bin/run.sh \
				python eval/run_cross_encoder_w_binenc_retriever_zeshel.py \
				--data_name $data \
				--n_ment -1 \
				--top_k 64 \
				--batch_size 1 \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_w_crossenc_w_embeds_6_306_w_63_negs/as_ckpt/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_w_crossenc_w_embeds_6_306_w_63_negs/as_ckpt/model/model-1-12279.0--79.46.ckpt \
				--misc model-1-12279.0--79.46.ckpt

				sbatch -p gpu --gres gpu:1 --mem 32GB --job-name m6cross_w_bi_$data --exclude gpu-0-0 bin/run.sh \
				python eval/run_cross_encoder_w_binenc_retriever_zeshel.py \
				--data_name $data \
				--n_ment -1 \
				--top_k 64 \
				--batch_size 1 \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=precomp_s=1234_w_crossenc_w_embeds_6_306_w_64_reranked_of_500_bienc_hard_negs/epoch_1_3_w_ckpt/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=precomp_s=1234_w_crossenc_w_embeds_6_306_w_64_reranked_of_500_bienc_hard_negs/epoch_1_3_w_ckpt/model/model-2-18439.0--77.09.ckpt \
				--misc model-2-18439.0--77.09.ckpt
				done


				for data in star_trek elder_scrolls lego forgotten_realms yugioh coronation_street ice_hockey muppets
				do
				sbatch -p gpu --gres gpu:1 --mem 32GB --job-name m7cross_w_bi_$data --exclude gpu-0-0 bin/run.sh \
				python eval/run_cross_encoder_w_binenc_retriever_zeshel.py \
				--data_name $data \
				--n_ment -1 \
				--top_k 64 \
				--batch_size 1 \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/joint_train/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_w_0.5_bi_cross_loss_from_scratch/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/joint_train/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_w_0.5_bi_cross_loss_from_scratch/model/model-3-19679.0--77.23.ckpt \
				--misc model-3-19679.0--77.23.ckpt


				sbatch -p gpu --gres gpu:1 --mem 32GB --job-name m8cross_w_bi_$data --exclude gpu-0-0 bin/run.sh \
				python eval/run_cross_encoder_w_binenc_retriever_zeshel.py \
				--data_name $data \
				--n_ment -1 \
				--top_k 64 \
				--batch_size 1 \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/joint_train/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_crossenc_w_0.5_bi_cross_loss_from_scratch/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/joint_train/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_crossenc_w_0.5_bi_cross_loss_from_scratch/model/model-3-20919.0--80.86.ckpt \
				--misc model-3-20919.0--80.86.ckpt

				sbatch -p gpu --gres gpu:1 --mem 32GB --job-name m11cross_w_bi_$data --exclude gpu-0-0 bin/run.sh \
				python eval/run_e-crossencoder_eval.py \
				--data_name $data \
				--n_ment -1 \
				--top_k 64 \
				--batch_size 1 \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/joint_train/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_w_0.5_bi_cross_loss_from_scratch/eval \
				--model_file ../../results/6_ReprCrossEnc/d=ent_link/joint_train/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_w_0.5_bi_cross_loss_from_scratch/model/model-3-19679.0--77.23.ckpt \
				--use_dummy_ment 0 \
				--misc model-3-19679.0--77.23.ckpt_w_self_retr

				sbatch -p gpu --gres gpu:1 --mem 32GB --job-name m12cross_w_bi_$data --exclude gpu-0-0 bin/run.sh \
				python eval/run_e-crossencoder_eval.py \
				--data_name $data \
				--n_ment -1 \
				--top_k 64 \
				--batch_size 1 \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/joint_train/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_crossenc_w_0.5_bi_cross_loss_from_scratch/eval \
				--model_file ../../results/6_ReprCrossEnc/d=ent_link/joint_train/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_crossenc_w_0.5_bi_cross_loss_from_scratch/model/model-3-20919.0--80.86.ckpt \
				--use_dummy_ment 0 \
				--misc model-3-20919.0--80.86.ckpt_w_self_retr

				done



				Debug NSW eval on CPUs

				python eval/nsw_eval_zeshel.py \
				--project_name _0_Debug \
				--data_name lego \
				--embed_type bienc \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--res_dir          ../../results/6_ReprCrossEnc/_del \
				--graph_type knn_e2e  \
				--entry_method tfidf \
				--graph_metric ip \
				--debug_mode 1 \
				--misc try_2 \
				--e2e_score_filename ../../results/6_ReprCrossEnc/d\=ent_link/m\=cross_enc_l\=ce_neg\=bienc_nsw_search_s\=1234_64_negs_5_bs_10_max_nbrs_500_budget_w_ddp_w_best_wrt_dev_mrr_cls_w_lin_rtx/score_mats_0-last.ckpt/lego/ent_to_ent_scores_n_e_10076x10076_topk_100_embed_bienc_m2e_.pkl 




	22 May


		Training w/ cls_w_lin crossencoder for 1 Epoch and then training for subsequent epochs using hard negatives wrt cross-encoder


			sbatch -p rtx8000-long --gres gpu:2 --mem 64GB --job-name cls_train bin/run.sh  \
			python models/train.py \
			--config config/el_zeshel_cross_enc.json \
			--exp_id 6_ReprCrossEnc \
			--loss ce \
			--pooling_type cls_w_lin \
			--neg_strategy bienc_hard_negs \
			--neg_mine_bienc_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
			--num_negs 63 \
			--train_batch_size 4 \
			--reload_dataloaders_every_n_epochs 0 \
			--eval_batch_size 32 \
			--ckpt_metric mrr \
			--warmup_proportion 0.01 \
			--strategy ddp \
			--num_gpus 2 \
			--eval_interval 0.2 \
			--num_epochs 1 \
			--misc 63_negs_w_cls_w_lin_for_hard_neg_training


			sbatch -p rtx8000-long --gres gpu:2 --mem 64GB --job-name cls_train_rep bin/run.sh  \
			python models/train.py \
			--config config/el_zeshel_cross_enc.json \
			--exp_id 6_ReprCrossEnc \
			--loss ce \
			--pooling_type cls_w_lin \
			--neg_strategy bienc_hard_negs \
			--neg_mine_bienc_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
			--num_negs 63 \
			--train_batch_size 4 \
			--reload_dataloaders_every_n_epochs 0 \
			--eval_batch_size 32 \
			--ckpt_metric mrr \
			--warmup_proportion 0.01 \
			--strategy ddp \
			--num_gpus 2 \
			--eval_interval 0.2 \
			--num_epochs 4 \
			--misc 63_negs_w_cls_w_lin_4_epochs_reproduce_6_49

			Computing ment2ent scores for some embedding model training - world_of_warcraft and pro_wrestling

				for start in 0 300 600 900
				do
				# sbatch -p gpu --gres gpu:1 --mem 32GB --job-name mat1-ce-pro_wrestling-${start} --exclude gpu-0-0 bin/run.sh \
				sbatch -p 1080ti-long --gres gpu:1 --mem 32GB --job-name mat1-ce-pro_wrestling-${start} bin/run.sh \
				python eval/run_cross_encoder_for_ment_ent_matrix_zeshel.py \
				--n_ment_start $start \
				--n_ment 300 \
				--n_ent -1 \
				--batch_size 100 \
				--data_name pro_wrestling \
				--layers final \
				--cross_model_ckpt ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt \
				--misc n_m_start_${start}
				done

				# sbatch -p gpu --gres gpu:1 --mem 32GB --job-name mat1-ce-pro_wrestling-1200 --exclude gpu-0-0 bin/run.sh \
				sbatch -p 1080ti-long --gres gpu:1 --mem 32GB --job-name mat1-ce-pro_wrestling-1200 bin/run.sh \
				python eval/run_cross_encoder_for_ment_ent_matrix_zeshel.py \
				--n_ment_start 1200 \
				--n_ment 192 \
				--n_ent -1 \
				--batch_size 100 \
				--data_name pro_wrestling \
				--layers final \
				--cross_model_ckpt ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt \
				--misc n_m_start_1200


				for start in 0 300 600 900
				do
				sbatch -p 1080ti-long --gres gpu:1 --mem 32GB --job-name mat1-ce-world_of_warcraft-${start} bin/run.sh \
				python eval/run_cross_encoder_for_ment_ent_matrix_zeshel.py \
				--n_ment_start $start \
				--n_ment 300 \
				--n_ent -1 \
				--batch_size 100 \
				--data_name world_of_warcraft \
				--layers final \
				--cross_model_ckpt ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt \
				--misc n_m_start_${start}
				done

				sbatch -p 1080ti-long --gres gpu:1 --mem 32GB --job-name mat1-ce-world_of_warcraft-1200 bin/run.sh \
				python eval/run_cross_encoder_for_ment_ent_matrix_zeshel.py \
				--n_ment_start 1200 \
				--n_ment 237 \
				--n_ent -1 \
				--batch_size 100 \
				--data_name world_of_warcraft \
				--layers final \
				--cross_model_ckpt ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt \
				--misc n_m_start_1200


			Launching bi+cross eval w/ top-500 bienc entities
			
				for data in american_football fallout final_fantasy military pro_wrestling starwars world_of_warcraft  coronation_street elder_scrolls ice_hockey muppets
				do
				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name m1cross_w_bi_$data bin/run.sh \
				python eval/run_cross_encoder_w_binenc_retriever_zeshel.py \
				--data_name $data \
				--n_ment -1 \
				--top_k 500 \
				--batch_size 1 \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
				--misc eoe-0-last.ckpt
				done

				# For doctor_who
				for mstart in 0 3000 
				do
				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name m1cross_w_bi_doctor_who-$mstart bin/run.sh \
				python eval/run_cross_encoder_w_binenc_retriever_zeshel.py \
				--data_name doctor_who \
				--n_ment_start $mstart \
				--n_ment 3000 \
				--top_k 500 \
				--batch_size 1 \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
				--misc eoe-0-last.ckpt_mstart_${mstart}
				done

				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name m1cross_w_bi_doctor_who-6K bin/run.sh \
				python eval/run_cross_encoder_w_binenc_retriever_zeshel.py \
				--data_name doctor_who \
				--n_ment_start 6000 \
				--n_ment 2334 \
				--top_k 500 \
				--batch_size 1 \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
				--misc eoe-0-last.ckpt_mstart_6000

					# For military

				for mstart in 0 2000 4000 6000 8000 10000  
				do
				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name m1cross_w_bi_military-$mstart bin/run.sh \
				python eval/run_cross_encoder_w_binenc_retriever_zeshel.py \
				--data_name military \
				--n_ment_start $mstart \
				--n_ment 2000 \
				--top_k 500 \
				--batch_size 1 \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
				--misc eoe-0-last.ckpt_mstart_${mstart}
				done

				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name m1cross_w_bi_military-12K bin/run.sh \
				python eval/run_cross_encoder_w_binenc_retriever_zeshel.py \
				--data_name military \
				--n_ment_start 12000 \
				--n_ment 1063 \
				--top_k 500 \
				--batch_size 1 \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
				--misc eoe-0-last.ckpt_mstart_12000


					# For starwars

				for mstart in 0 2000 4000 6000 8000  
				do
				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name m1cross_w_bi_starwars-$mstart bin/run.sh \
				python eval/run_cross_encoder_w_binenc_retriever_zeshel.py \
				--data_name starwars \
				--n_ment_start $mstart \
				--n_ment 2000 \
				--top_k 500 \
				--batch_size 1 \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
				--misc eoe-0-last.ckpt_mstart_${mstart}
				done

				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name m1cross_w_bi_starwars-10K bin/run.sh \
				python eval/run_cross_encoder_w_binenc_retriever_zeshel.py \
				--data_name starwars \
				--n_ment_start 10000 \
				--n_ment 1824 \
				--top_k 500 \
				--batch_size 1 \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
				--misc eoe-0-last.ckpt_mstart_10000


			Launching bi+cross eval w/ top-1000 bienc entities
			
				for data in pro_wrestling world_of_warcraft coronation_street ice_hockey muppets
				do
				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name m1cross_w_bi_$data bin/run.sh \
				python eval/run_cross_encoder_w_binenc_retriever_zeshel.py \
				--data_name $data \
				--n_ment -1 \
				--top_k 1000 \
				--batch_size 1 \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
				--misc eoe-0-last.ckpt
				done

				# For fallout
				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name m1cross_w_bi_fallout-0 bin/run.sh \
				python eval/run_cross_encoder_w_binenc_retriever_zeshel.py \
				--data_name fallout \
				--n_ment_start 0 \
				--n_ment 2000 \
				--top_k 1000 \
				--batch_size 1 \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
				--misc eoe-0-last.ckpt_mstart_0
				done

				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name m1cross_w_bi_fallout-6K bin/run.sh \
				python eval/run_cross_encoder_w_binenc_retriever_zeshel.py \
				--data_name fallout \
				--n_ment_start 2000 \
				--n_ment 1286 \
				--top_k 1000 \
				--batch_size 1 \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
				--misc eoe-0-last.ckpt_mstart_2000


				# For elder_scrolls
				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name m1cross_w_bi_elder_scrolls-0 bin/run.sh \
				python eval/run_cross_encoder_w_binenc_retriever_zeshel.py \
				--data_name elder_scrolls \
				--n_ment_start 0 \
				--n_ment 2000 \
				--top_k 1000 \
				--batch_size 1 \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
				--misc eoe-0-last.ckpt_mstart_0
				done

				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name m1cross_w_bi_elder_scrolls-6K bin/run.sh \
				python eval/run_cross_encoder_w_binenc_retriever_zeshel.py \
				--data_name elder_scrolls \
				--n_ment_start 2000 \
				--n_ment 2275 \
				--top_k 1000 \
				--batch_size 1 \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
				--misc eoe-0-last.ckpt_mstart_2000

				# For american_football
				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name m1cross_w_bi_american_football-0 bin/run.sh \
				python eval/run_cross_encoder_w_binenc_retriever_zeshel.py \
				--data_name american_football \
				--n_ment_start 0 \
				--n_ment 2000 \
				--top_k 1000 \
				--batch_size 1 \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
				--misc eoe-0-last.ckpt_mstart_0
				done

				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name m1cross_w_bi_american_football-6K bin/run.sh \
				python eval/run_cross_encoder_w_binenc_retriever_zeshel.py \
				--data_name american_football \
				--n_ment_start 2000 \
				--n_ment 1898 \
				--top_k 1000 \
				--batch_size 1 \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
				--misc eoe-0-last.ckpt_mstart_2000


				# For final_fantasy
				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name m1cross_w_bi_final_fantasy-0 bin/run.sh \
				python eval/run_cross_encoder_w_binenc_retriever_zeshel.py \
				--data_name final_fantasy \
				--n_ment_start 0 \
				--n_ment 3000 \
				--top_k 1000 \
				--batch_size 1 \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
				--misc eoe-0-last.ckpt_mstart_0
				done

				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name m1cross_w_bi_final_fantasy-6K bin/run.sh \
				python eval/run_cross_encoder_w_binenc_retriever_zeshel.py \
				--data_name final_fantasy \
				--n_ment_start 3000 \
				--n_ment 3041 \
				--top_k 1000 \
				--batch_size 1 \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
				--misc eoe-0-last.ckpt_mstart_3000

				# For doctor_who
				for mstart in 0 3000 
				do
				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name m1cross_w_bi_doctor_who-$mstart bin/run.sh \
				python eval/run_cross_encoder_w_binenc_retriever_zeshel.py \
				--data_name doctor_who \
				--n_ment_start $mstart \
				--n_ment 3000 \
				--top_k 1000 \
				--batch_size 1 \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
				--misc eoe-0-last.ckpt_mstart_${mstart}
				done

				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name m1cross_w_bi_doctor_who-6K bin/run.sh \
				python eval/run_cross_encoder_w_binenc_retriever_zeshel.py \
				--data_name doctor_who \
				--n_ment_start 6000 \
				--n_ment 2334 \
				--top_k 1000 \
				--batch_size 1 \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
				--misc eoe-0-last.ckpt_mstart_6000

				# For military

				for mstart in 0 2000 4000 6000 8000 10000  
				do
				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name m1cross_w_bi_military-$mstart bin/run.sh \
				python eval/run_cross_encoder_w_binenc_retriever_zeshel.py \
				--data_name military \
				--n_ment_start $mstart \
				--n_ment 2000 \
				--top_k 1000 \
				--batch_size 1 \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
				--misc eoe-0-last.ckpt_mstart_${mstart}
				done

				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name m1cross_w_bi_military-12K bin/run.sh \
				python eval/run_cross_encoder_w_binenc_retriever_zeshel.py \
				--data_name military \
				--n_ment_start 12000 \
				--n_ment 1063 \
				--top_k 1000 \
				--batch_size 1 \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
				--misc eoe-0-last.ckpt_mstart_12000


				# For starwars

				for mstart in 0 2000 4000 6000 8000  
				do
				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name m1cross_w_bi_starwars-$mstart bin/run.sh \
				python eval/run_cross_encoder_w_binenc_retriever_zeshel.py \
				--data_name starwars \
				--n_ment_start $mstart \
				--n_ment 2000 \
				--top_k 1000 \
				--batch_size 1 \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
				--misc eoe-0-last.ckpt_mstart_${mstart}
				done

				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name m1cross_w_bi_starwars-10K bin/run.sh \
				python eval/run_cross_encoder_w_binenc_retriever_zeshel.py \
				--data_name starwars \
				--n_ment_start 10000 \
				--n_ment 1824 \
				--top_k 1000 \
				--batch_size 1 \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
				--misc eoe-0-last.ckpt_mstart_10000


			Launching e2e score computation.

				for data in american_football doctor_who fallout final_fantasy pro_wrestling world_of_warcraft  coronation_street elder_scrolls ice_hockey muppets
				do
				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name e2e-ece0-$data bin/run.sh  \
				python eval/run_cross_encoder_for_ent_ent_matrix.py \
				--data_name $data \
				--n_ent_x -1 \
				--n_ent_y -1 \
				--topk 100 \
				--embed_type bienc \
				--token_opt m2e \
				--batch_size 150 \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt 
				done

				# Chunking computation for domain=military
				for xstart in 0 20000 40000 60000 
				do
				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name e2e-ece0-miliary-$xstart bin/run.sh  \
				python eval/run_cross_encoder_for_ent_ent_matrix.py \
				--data_name military \
				--n_ent_x_start $xstart \
				--n_ent_x  20000 \
				--n_ent_y -1 \
				--topk 100 \
				--embed_type bienc \
				--token_opt m2e \
				--batch_size 150 \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
				--misc x_start_$xstart
				done

				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name e2e-ece0-miliary-90K bin/run.sh  \
				python eval/run_cross_encoder_for_ent_ent_matrix.py \
				--data_name military \
				--n_ent_x_start 80000 \
				--n_ent_x  24520 \
				--n_ent_y -1 \
				--topk 100 \
				--embed_type bienc \
				--token_opt m2e \
				--batch_size 150 \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
				--misc x_start_80000

				# Chunking computation for domain=starwars
				for xstart in 0 20000 40000
				do
				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name e2e-ece0-startwars-$xstart bin/run.sh  \
				python eval/run_cross_encoder_for_ent_ent_matrix.py \
				--data_name starwars \
				--n_ent_x_start $xstart \
				--n_ent_x  20000 \
				--n_ent_y -1 \
				--topk 100 \
				--embed_type bienc \
				--token_opt m2e \
				--batch_size 150 \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
				--misc x_start_$xstart
				done

				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name e2e-ece0-startwars-60K bin/run.sh  \
				python eval/run_cross_encoder_for_ent_ent_matrix.py \
				--data_name starwars \
				--n_ent_x_start 60000 \
				--n_ent_x  27056 \
				--n_ent_y -1 \
				--topk 100 \
				--embed_type bienc \
				--token_opt m2e \
				--batch_size 150 \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
				--misc x_start_60000


			Launching graph eval for crossencoder w/ E2E graph 5-20-500

				# Chunking for doctor_who
				# sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name nsw_doctor_who bin/run.sh \
				# python eval/run_cross_encoder_w_nsw_eval.py \
				# --data_name doctor_who \
				# --n_ment -1 \
				# --res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval \
				# --cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last_copy.ckpt \
				# --bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				# --batch_size 150 \
				# --embed_type bienc \
				# --max_nbrs 20 \
				# --beamsize 5 \
				# --top_k 100 \
				# --comp_budget 500 \
				# --graph_metric l2 \
				# --graph_type knn_e2e \
				# --e2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt/doctor_who/ent_to_ent_scores_n_e_40281x40281_topk_100_embed_bienc_m2e_.pkl

				for mstart in 0 2000 4000
				do
				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name nsw_doctor_who_{mstart} bin/run.sh \
				python eval/run_cross_encoder_w_nsw_eval.py \
				--data_name doctor_who \
				--n_ment_start $mstart \
				--n_ment 2000 \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last_copy.ckpt \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--batch_size 150 \
				--embed_type bienc \
				--max_nbrs 20 \
				--beamsize 5 \
				--top_k 100 \
				--comp_budget 500 \
				--graph_metric l2 \
				--graph_type knn_e2e \
				--e2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt/doctor_who/ent_to_ent_scores_n_e_40281x40281_topk_100_embed_bienc_m2e_.pkl \
				--misc mstart_$mstart
				done

				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name nsw_doctor_who_6000 bin/run.sh \
				python eval/run_cross_encoder_w_nsw_eval.py \
				--data_name doctor_who \
				--n_ment_start 6000 \
				--n_ment 2334 \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last_copy.ckpt \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--batch_size 150 \
				--embed_type bienc \
				--max_nbrs 20 \
				--beamsize 5 \
				--top_k 100 \
				--comp_budget 500 \
				--graph_metric l2 \
				--graph_type knn_e2e \
				--e2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt/doctor_who/ent_to_ent_scores_n_e_40281x40281_topk_100_embed_bienc_m2e_.pkl \
				--misc mstart_6000


				# Chunking for america_football
				# sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name nsw_american_football bin/run.sh \
				# python eval/run_cross_encoder_w_nsw_eval.py \
				# --data_name american_football \
				# --n_ment -1 \
				# --res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval \
				# --cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last_copy.ckpt \
				# --bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				# --batch_size 150 \
				# --embed_type bienc \
				# --max_nbrs 20 \
				# --beamsize 5 \
				# --top_k 100 \
				# --comp_budget 500 \
				# --graph_metric l2 \
				# --graph_type knn_e2e \
				# --e2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt/american_football/ent_to_ent_scores_n_e_31929x31929_topk_100_embed_bienc_m2e_.pkl

				for mstart in 0 500 1000 1500 2000 2500 3000 
				do
				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name nsw_american_football_${mstart} bin/run.sh \
				python eval/run_cross_encoder_w_nsw_eval.py \
				--data_name american_football \
				--n_ment_start ${mstart} \
				--n_ment 500 \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last_copy.ckpt \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--batch_size 150 \
				--embed_type bienc \
				--max_nbrs 20 \
				--beamsize 5 \
				--top_k 100 \
				--comp_budget 500 \
				--graph_metric l2 \
				--graph_type knn_e2e \
				--e2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt/american_football/ent_to_ent_scores_n_e_31929x31929_topk_100_embed_bienc_m2e_.pkl \
				--misc mstart_${mstart}
				done

				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name nsw_american_football_3500 bin/run.sh \
				python eval/run_cross_encoder_w_nsw_eval.py \
				--data_name american_football \
				--n_ment_start 3500 \
				--n_ment 398 \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last_copy.ckpt \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--batch_size 150 \
				--embed_type bienc \
				--max_nbrs 20 \
				--beamsize 5 \
				--top_k 100 \
				--comp_budget 500 \
				--graph_metric l2 \
				--graph_type knn_e2e \
				--e2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt/american_football/ent_to_ent_scores_n_e_31929x31929_topk_100_embed_bienc_m2e_.pkl \
				--misc mstart_3500

				# Chunking final_fantasy
				# sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name nsw_final_fantasy bin/run.sh \
				# python eval/run_cross_encoder_w_nsw_eval.py \
				# --data_name final_fantasy \
				# --n_ment -1 \
				# --res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval \
				# --cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last_copy.ckpt \
				# --bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				# --batch_size 150 \
				# --embed_type bienc \
				# --max_nbrs 20 \
				# --beamsize 5 \
				# --top_k 100 \
				# --comp_budget 500 \
				# --graph_metric l2 \
				# --graph_type knn_e2e \
				# --e2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt/final_fantasy/ent_to_ent_scores_n_e_14044x14044_topk_100_embed_bienc_m2e_.pkl


				for mstart in 0 2000
				do
				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name nsw_final_fantasy-$mstart bin/run.sh \
				python eval/run_cross_encoder_w_nsw_eval.py \
				--data_name final_fantasy \
				--n_ment_start $mstart \
				--n_ment 2000 \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last_copy.ckpt \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--batch_size 150 \
				--embed_type bienc \
				--max_nbrs 20 \
				--beamsize 5 \
				--top_k 100 \
				--comp_budget 500 \
				--graph_metric l2 \
				--graph_type knn_e2e \
				--e2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt/final_fantasy/ent_to_ent_scores_n_e_14044x14044_topk_100_embed_bienc_m2e_.pkl \
				--misc mstart_$mstart
				done

				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name nsw_final_fantasy-last bin/run.sh \
				python eval/run_cross_encoder_w_nsw_eval.py \
				--data_name final_fantasy \
				--n_ment_start 4000 \
				--n_ment 2041 \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last_copy.ckpt \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--batch_size 150 \
				--embed_type bienc \
				--max_nbrs 20 \
				--beamsize 5 \
				--top_k 100 \
				--comp_budget 500 \
				--graph_metric l2 \
				--graph_type knn_e2e \
				--e2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt/final_fantasy/ent_to_ent_scores_n_e_14044x14044_topk_100_embed_bienc_m2e_.pkl \
				--misc mstart_4000


				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name nsw_fallout bin/run.sh \
				python eval/run_cross_encoder_w_nsw_eval.py \
				--data_name fallout \
				--n_ment -1 \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last_copy.ckpt \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--batch_size 150 \
				--embed_type bienc \
				--max_nbrs 20 \
				--beamsize 5 \
				--top_k 100 \
				--comp_budget 500 \
				--graph_metric l2 \
				--graph_type knn_e2e \
				--e2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt/fallout/ent_to_ent_scores_n_e_16992x16992_topk_100_embed_bienc_m2e_.pkl


				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name nsw_coronation_street bin/run.sh \
				python eval/run_cross_encoder_w_nsw_eval.py \
				--data_name coronation_street \
				--n_ment -1 \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last_copy.ckpt \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--batch_size 150 \
				--embed_type bienc \
				--max_nbrs 20 \
				--beamsize 5 \
				--top_k 100 \
				--comp_budget 500 \
				--graph_metric l2 \
				--graph_type knn_e2e \
				--e2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt/coronation_street/ent_to_ent_scores_n_e_17809x17809_topk_100_embed_bienc_m2e_.pkl

				# Chunking for elder_scrolls
				# sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name nsw_elder_scrolls bin/run.sh \
				# python eval/run_cross_encoder_w_nsw_eval.py \
				# --data_name elder_scrolls \
				# --n_ment -1 \
				# --res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval \
				# --cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last_copy.ckpt \
				# --bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				# --batch_size 150 \
				# --embed_type bienc \
				# --max_nbrs 20 \
				# --beamsize 5 \
				# --top_k 100 \
				# --comp_budget 500 \
				# --graph_metric l2 \
				# --graph_type knn_e2e \
				# --e2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt/elder_scrolls/ent_to_ent_scores_n_e_21712x21712_topk_100_embed_bienc_m2e_.pkl

				for mstart in 0 500 1000 1500 2000 2500 3000 3500 
				do
				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name nsw_elder_scrolls_${mstart} bin/run.sh \
				python eval/run_cross_encoder_w_nsw_eval.py \
				--data_name elder_scrolls \
				--n_ment_start ${mstart} \
				--n_ment 500 \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last_copy.ckpt \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--batch_size 150 \
				--embed_type bienc \
				--max_nbrs 20 \
				--beamsize 5 \
				--top_k 100 \
				--comp_budget 500 \
				--graph_metric l2 \
				--graph_type knn_e2e \
				--e2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt/elder_scrolls/ent_to_ent_scores_n_e_21712x21712_topk_100_embed_bienc_m2e_.pkl \
				--misc mstart_$mstart
				done

				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name nsw_elder_scrolls_4000 bin/run.sh \
				python eval/run_cross_encoder_w_nsw_eval.py \
				--data_name elder_scrolls \
				--n_ment_start 4000 \
				--n_ment 275 \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last_copy.ckpt \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--batch_size 150 \
				--embed_type bienc \
				--max_nbrs 20 \
				--beamsize 5 \
				--top_k 100 \
				--comp_budget 500 \
				--graph_metric l2 \
				--graph_type knn_e2e \
				--e2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt/elder_scrolls/ent_to_ent_scores_n_e_21712x21712_topk_100_embed_bienc_m2e_.pkl \
				--misc mstart_4000


				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name nsw_ice_hockey bin/run.sh \
				python eval/run_cross_encoder_w_nsw_eval.py \
				--data_name ice_hockey \
				--n_ment -1 \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last_copy.ckpt \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--batch_size 150 \
				--embed_type bienc \
				--max_nbrs 20 \
				--beamsize 5 \
				--top_k 100 \
				--comp_budget 500 \
				--graph_metric l2 \
				--graph_type knn_e2e \
				--e2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt/ice_hockey/ent_to_ent_scores_n_e_28684x28684_topk_100_embed_bienc_m2e_.pkl

				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name nsw_muppets bin/run.sh \
				python eval/run_cross_encoder_w_nsw_eval.py \
				--data_name muppets \
				--n_ment -1 \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last_copy.ckpt \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--batch_size 150 \
				--embed_type bienc \
				--max_nbrs 20 \
				--beamsize 5 \
				--top_k 100 \
				--comp_budget 500 \
				--graph_metric l2 \
				--graph_type knn_e2e \
				--e2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt/muppets/ent_to_ent_scores_n_e_21344x21344_topk_100_embed_bienc_m2e_.pkl

				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name nsw_pro_wrestling bin/run.sh \
				python eval/run_cross_encoder_w_nsw_eval.py \
				--data_name pro_wrestling \
				--n_ment -1 \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last_copy.ckpt \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--batch_size 150 \
				--embed_type bienc \
				--max_nbrs 20 \
				--beamsize 5 \
				--top_k 100 \
				--comp_budget 500 \
				--graph_metric l2 \
				--graph_type knn_e2e \
				--e2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt/pro_wrestling/ent_to_ent_scores_n_e_10133x10133_topk_100_embed_bienc_m2e_.pkl


				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name nsw_world_of_warcraft bin/run.sh \
				python eval/run_cross_encoder_w_nsw_eval.py \
				--data_name world_of_warcraft \
				--n_ment -1 \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last_copy.ckpt \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--batch_size 150 \
				--embed_type bienc \
				--max_nbrs 20 \
				--beamsize 5 \
				--top_k 100 \
				--comp_budget 500 \
				--graph_metric l2 \
				--graph_type knn_e2e \
				--e2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt/world_of_warcraft/ent_to_ent_scores_n_e_27677x27677_topk_100_embed_bienc_m2e_.pkl



				# Chunking jobs for starwars

				for mstart in 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000
				do
				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name nsw_sw_$mstart bin/run.sh \
				python eval/run_cross_encoder_w_nsw_eval.py \
				--data_name starwars \
				--n_ment_start $mstart \
				--n_ment 1000 \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last_copy.ckpt \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--batch_size 150 \
				--embed_type bienc \
				--max_nbrs 20 \
				--beamsize 5 \
				--top_k 100 \
				--comp_budget 500 \
				--graph_metric l2 \
				--graph_type knn_e2e \
				--e2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt/starwars/ent_to_ent_scores_n_e_87056x87056_topk_100_embed_bienc_m2e_.pkl \
				--misc mstart_$mstart
				done

				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name nsw_sw_10K bin/run.sh \
				python eval/run_cross_encoder_w_nsw_eval.py \
				--data_name starwars \
				--n_ment_start 11000 \
				--n_ment 824 \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last_copy.ckpt \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--batch_size 150 \
				--embed_type bienc \
				--max_nbrs 20 \
				--beamsize 5 \
				--top_k 100 \
				--comp_budget 500 \
				--graph_metric l2 \
				--graph_type knn_e2e \
				--e2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt/starwars/ent_to_ent_scores_n_e_87056x87056_topk_100_embed_bienc_m2e_.pkl \
				--misc mstart_11K


				# Chunking jobs for military
				for mstart in 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 11000 
				do
				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name nsw_mil_$mstart bin/run.sh \
				python eval/run_cross_encoder_w_nsw_eval.py \
				--data_name military \
				--n_ment_start $mstart \
				--n_ment 1000 \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last_copy.ckpt \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--batch_size 150 \
				--embed_type bienc \
				--max_nbrs 20 \
				--beamsize 5 \
				--top_k 100 \
				--comp_budget 500 \
				--graph_metric l2 \
				--graph_type knn_e2e \
				--e2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt/military/ent_to_ent_scores_n_e_104520x104520_topk_100_embed_bienc_m2e_.pkl \
				--misc mstart_$mstart
				done

				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name nsw_mil_12K bin/run.sh \
				python eval/run_cross_encoder_w_nsw_eval.py \
				--data_name military \
				--n_ment_start 12000 \
				--n_ment 1063 \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last_copy.ckpt \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--batch_size 150 \
				--embed_type bienc \
				--max_nbrs 20 \
				--beamsize 5 \
				--top_k 100 \
				--comp_budget 500 \
				--graph_metric l2 \
				--graph_type knn_e2e \
				--e2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt/military/ent_to_ent_scores_n_e_104520x104520_topk_100_embed_bienc_m2e_.pkl \
				--misc mstart_12K


			Launching graph eval for crossencoder w/ E2E graph 5-20-1000

				# Chunking for doctor_who
				# sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name nsw_doctor_who bin/run.sh \
				# python eval/run_cross_encoder_w_nsw_eval.py \
				# --data_name doctor_who \
				# --n_ment -1 \
				# --res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval \
				# --cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
				# --bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				# --batch_size 150 \
				# --embed_type bienc \
				# --max_nbrs 20 \
				# --beamsize 5 \
				# --top_k 100 \
				# --comp_budget 1000 \
				# --graph_metric l2 \
				# --graph_type knn_e2e \
				# --e2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt/doctor_who/ent_to_ent_scores_n_e_40281x40281_topk_100_embed_bienc_m2e_.pkl

				for mstart in 0 2000 4000
				do
				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name nsw_doctor_who_${mstart} bin/run.sh \
				python eval/run_cross_encoder_w_nsw_eval.py \
				--data_name doctor_who \
				--n_ment_start $mstart \
				--n_ment 2000 \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--batch_size 150 \
				--embed_type bienc \
				--max_nbrs 20 \
				--beamsize 5 \
				--top_k 100 \
				--comp_budget 1000 \
				--graph_metric l2 \
				--graph_type knn_e2e \
				--e2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt/doctor_who/ent_to_ent_scores_n_e_40281x40281_topk_100_embed_bienc_m2e_.pkl \
				--misc mstart_$mstart
				done

				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name nsw_doctor_who_6000 bin/run.sh \
				python eval/run_cross_encoder_w_nsw_eval.py \
				--data_name doctor_who \
				--n_ment_start 6000 \
				--n_ment 2334 \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--batch_size 150 \
				--embed_type bienc \
				--max_nbrs 20 \
				--beamsize 5 \
				--top_k 100 \
				--comp_budget 1000 \
				--graph_metric l2 \
				--graph_type knn_e2e \
				--e2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt/doctor_who/ent_to_ent_scores_n_e_40281x40281_topk_100_embed_bienc_m2e_.pkl \
				--misc mstart_6000


				# Chunking for america_football
				# sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name nsw_american_football bin/run.sh \
				# python eval/run_cross_encoder_w_nsw_eval.py \
				# --data_name american_football \
				# --n_ment -1 \
				# --res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval \
				# --cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
				# --bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				# --batch_size 150 \
				# --embed_type bienc \
				# --max_nbrs 20 \
				# --beamsize 5 \
				# --top_k 100 \
				# --comp_budget 1000 \
				# --graph_metric l2 \
				# --graph_type knn_e2e \
				# --e2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt/american_football/ent_to_ent_scores_n_e_31929x31929_topk_100_embed_bienc_m2e_.pkl

				for mstart in 0 1000 2000 
				do
				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name nsw_american_football_${mstart} bin/run.sh \
				python eval/run_cross_encoder_w_nsw_eval.py \
				--data_name american_football \
				--n_ment_start ${mstart} \
				--n_ment 1000 \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--batch_size 150 \
				--embed_type bienc \
				--max_nbrs 20 \
				--beamsize 5 \
				--top_k 100 \
				--comp_budget 1000 \
				--graph_metric l2 \
				--graph_type knn_e2e \
				--e2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt/american_football/ent_to_ent_scores_n_e_31929x31929_topk_100_embed_bienc_m2e_.pkl \
				--misc mstart_${mstart}
				done

				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name nsw_american_football_3500 bin/run.sh \
				python eval/run_cross_encoder_w_nsw_eval.py \
				--data_name american_football \
				--n_ment_start 3000 \
				--n_ment 898 \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--batch_size 150 \
				--embed_type bienc \
				--max_nbrs 20 \
				--beamsize 5 \
				--top_k 100 \
				--comp_budget 1000 \
				--graph_metric l2 \
				--graph_type knn_e2e \
				--e2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt/american_football/ent_to_ent_scores_n_e_31929x31929_topk_100_embed_bienc_m2e_.pkl \
				--misc mstart_3000

				# Chunking final_fantasy
				# sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name nsw_final_fantasy bin/run.sh \
				# python eval/run_cross_encoder_w_nsw_eval.py \
				# --data_name final_fantasy \
				# --n_ment -1 \
				# --res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval \
				# --cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
				# --bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				# --batch_size 150 \
				# --embed_type bienc \
				# --max_nbrs 20 \
				# --beamsize 5 \
				# --top_k 100 \
				# --comp_budget 1000 \
				# --graph_metric l2 \
				# --graph_type knn_e2e \
				# --e2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt/final_fantasy/ent_to_ent_scores_n_e_14044x14044_topk_100_embed_bienc_m2e_.pkl


				for mstart in 0 2000
				do
				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name nsw_final_fantasy-$mstart bin/run.sh \
				python eval/run_cross_encoder_w_nsw_eval.py \
				--data_name final_fantasy \
				--n_ment_start $mstart \
				--n_ment 2000 \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--batch_size 150 \
				--embed_type bienc \
				--max_nbrs 20 \
				--beamsize 5 \
				--top_k 100 \
				--comp_budget 1000 \
				--graph_metric l2 \
				--graph_type knn_e2e \
				--e2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt/final_fantasy/ent_to_ent_scores_n_e_14044x14044_topk_100_embed_bienc_m2e_.pkl \
				--misc mstart_$mstart
				done

				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name nsw_final_fantasy-last bin/run.sh \
				python eval/run_cross_encoder_w_nsw_eval.py \
				--data_name final_fantasy \
				--n_ment_start 4000 \
				--n_ment 2041 \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--batch_size 150 \
				--embed_type bienc \
				--max_nbrs 20 \
				--beamsize 5 \
				--top_k 100 \
				--comp_budget 1000 \
				--graph_metric l2 \
				--graph_type knn_e2e \
				--e2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt/final_fantasy/ent_to_ent_scores_n_e_14044x14044_topk_100_embed_bienc_m2e_.pkl \
				--misc mstart_4000


				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name nsw_fallout bin/run.sh \
				python eval/run_cross_encoder_w_nsw_eval.py \
				--data_name fallout \
				--n_ment -1 \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--batch_size 150 \
				--embed_type bienc \
				--max_nbrs 20 \
				--beamsize 5 \
				--top_k 100 \
				--comp_budget 1000 \
				--graph_metric l2 \
				--graph_type knn_e2e \
				--e2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt/fallout/ent_to_ent_scores_n_e_16992x16992_topk_100_embed_bienc_m2e_.pkl


				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name nsw_coronation_street bin/run.sh \
				python eval/run_cross_encoder_w_nsw_eval.py \
				--data_name coronation_street \
				--n_ment -1 \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--batch_size 150 \
				--embed_type bienc \
				--max_nbrs 20 \
				--beamsize 5 \
				--top_k 100 \
				--comp_budget 1000 \
				--graph_metric l2 \
				--graph_type knn_e2e \
				--e2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt/coronation_street/ent_to_ent_scores_n_e_17809x17809_topk_100_embed_bienc_m2e_.pkl

				# Chunking for elder_scrolls
				# sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name nsw_elder_scrolls bin/run.sh \
				# python eval/run_cross_encoder_w_nsw_eval.py \
				# --data_name elder_scrolls \
				# --n_ment -1 \
				# --res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval \
				# --cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
				# --bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				# --batch_size 150 \
				# --embed_type bienc \
				# --max_nbrs 20 \
				# --beamsize 5 \
				# --top_k 100 \
				# --comp_budget 1000 \
				# --graph_metric l2 \
				# --graph_type knn_e2e \
				# --e2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt/elder_scrolls/ent_to_ent_scores_n_e_21712x21712_topk_100_embed_bienc_m2e_.pkl

				for mstart in 0 1000 2000 3000 
				do
				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name nsw_elder_scrolls_${mstart} bin/run.sh \
				python eval/run_cross_encoder_w_nsw_eval.py \
				--data_name elder_scrolls \
				--n_ment_start ${mstart} \
				--n_ment 1000 \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--batch_size 150 \
				--embed_type bienc \
				--max_nbrs 20 \
				--beamsize 5 \
				--top_k 100 \
				--comp_budget 1000 \
				--graph_metric l2 \
				--graph_type knn_e2e \
				--e2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt/elder_scrolls/ent_to_ent_scores_n_e_21712x21712_topk_100_embed_bienc_m2e_.pkl \
				--misc mstart_$mstart
				done

				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name nsw_elder_scrolls_4000 bin/run.sh \
				python eval/run_cross_encoder_w_nsw_eval.py \
				--data_name elder_scrolls \
				--n_ment_start 4000 \
				--n_ment 275 \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--batch_size 150 \
				--embed_type bienc \
				--max_nbrs 20 \
				--beamsize 5 \
				--top_k 100 \
				--comp_budget 1000 \
				--graph_metric l2 \
				--graph_type knn_e2e \
				--e2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt/elder_scrolls/ent_to_ent_scores_n_e_21712x21712_topk_100_embed_bienc_m2e_.pkl \
				--misc mstart_4000


				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name nsw_ice_hockey bin/run.sh \
				python eval/run_cross_encoder_w_nsw_eval.py \
				--data_name ice_hockey \
				--n_ment -1 \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--batch_size 150 \
				--embed_type bienc \
				--max_nbrs 20 \
				--beamsize 5 \
				--top_k 100 \
				--comp_budget 1000 \
				--graph_metric l2 \
				--graph_type knn_e2e \
				--e2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt/ice_hockey/ent_to_ent_scores_n_e_28684x28684_topk_100_embed_bienc_m2e_.pkl

				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name nsw_muppets bin/run.sh \
				python eval/run_cross_encoder_w_nsw_eval.py \
				--data_name muppets \
				--n_ment -1 \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--batch_size 150 \
				--embed_type bienc \
				--max_nbrs 20 \
				--beamsize 5 \
				--top_k 100 \
				--comp_budget 1000 \
				--graph_metric l2 \
				--graph_type knn_e2e \
				--e2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt/muppets/ent_to_ent_scores_n_e_21344x21344_topk_100_embed_bienc_m2e_.pkl

				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name nsw_pro_wrestling bin/run.sh \
				python eval/run_cross_encoder_w_nsw_eval.py \
				--data_name pro_wrestling \
				--n_ment -1 \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--batch_size 150 \
				--embed_type bienc \
				--max_nbrs 20 \
				--beamsize 5 \
				--top_k 100 \
				--comp_budget 1000 \
				--graph_metric l2 \
				--graph_type knn_e2e \
				--e2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt/pro_wrestling/ent_to_ent_scores_n_e_10133x10133_topk_100_embed_bienc_m2e_.pkl


				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name nsw_world_of_warcraft bin/run.sh \
				python eval/run_cross_encoder_w_nsw_eval.py \
				--data_name world_of_warcraft \
				--n_ment -1 \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--batch_size 150 \
				--embed_type bienc \
				--max_nbrs 20 \
				--beamsize 5 \
				--top_k 100 \
				--comp_budget 1000 \
				--graph_metric l2 \
				--graph_type knn_e2e \
				--e2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt/world_of_warcraft/ent_to_ent_scores_n_e_27677x27677_topk_100_embed_bienc_m2e_.pkl



				# Chunking jobs for starwars

				for mstart in 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000
				do
				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name nsw_sw_$mstart bin/run.sh \
				python eval/run_cross_encoder_w_nsw_eval.py \
				--data_name starwars \
				--n_ment_start $mstart \
				--n_ment 1000 \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--batch_size 150 \
				--embed_type bienc \
				--max_nbrs 20 \
				--beamsize 5 \
				--top_k 100 \
				--comp_budget 1000 \
				--graph_metric l2 \
				--graph_type knn_e2e \
				--e2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt/starwars/ent_to_ent_scores_n_e_87056x87056_topk_100_embed_bienc_m2e_.pkl \
				--misc mstart_$mstart
				done

				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name nsw_sw_10K bin/run.sh \
				python eval/run_cross_encoder_w_nsw_eval.py \
				--data_name starwars \
				--n_ment_start 11000 \
				--n_ment 824 \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--batch_size 150 \
				--embed_type bienc \
				--max_nbrs 20 \
				--beamsize 5 \
				--top_k 100 \
				--comp_budget 1000 \
				--graph_metric l2 \
				--graph_type knn_e2e \
				--e2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt/starwars/ent_to_ent_scores_n_e_87056x87056_topk_100_embed_bienc_m2e_.pkl \
				--misc mstart_11K


				# Chunking jobs for military
				for mstart in 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 11000 
				do
				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name nsw_mil_$mstart bin/run.sh \
				python eval/run_cross_encoder_w_nsw_eval.py \
				--data_name military \
				--n_ment_start $mstart \
				--n_ment 1000 \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--batch_size 150 \
				--embed_type bienc \
				--max_nbrs 20 \
				--beamsize 5 \
				--top_k 100 \
				--comp_budget 1000 \
				--graph_metric l2 \
				--graph_type knn_e2e \
				--e2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt/military/ent_to_ent_scores_n_e_104520x104520_topk_100_embed_bienc_m2e_.pkl \
				--misc mstart_$mstart
				done

				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name nsw_mil_12K bin/run.sh \
				python eval/run_cross_encoder_w_nsw_eval.py \
				--data_name military \
				--n_ment_start 12000 \
				--n_ment 1063 \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--batch_size 150 \
				--embed_type bienc \
				--max_nbrs 20 \
				--beamsize 5 \
				--top_k 100 \
				--comp_budget 1000 \
				--graph_metric l2 \
				--graph_type knn_e2e \
				--e2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt/military/ent_to_ent_scores_n_e_104520x104520_topk_100_embed_bienc_m2e_.pkl \
				--misc mstart_12K


		Training w/ cls_w_lin crossencoder w/ 63/500 biencoder hard negatives second epoch

			sbatch -p rtx8000-long --gres gpu:2 --mem 64GB --job-name bi_hard_negs_epoch_1_w_reload bin/run.sh  \
			python models/train.py \
			--config config/el_zeshel_cross_enc.json \
			--exp_id 6_ReprCrossEnc \
			--loss ce \
			--pooling_type cls_w_lin \
			--neg_strategy precomp \
			--num_negs 63 \
			--train_batch_size 4 \
			--ent_w_score_file_template ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval/{}/m=-1_k=500_1_eoe-0-last.ckpt/crossenc_topk_preds_w_bienc_retrvr.txt \
			--ckpt_path             	../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
			--reload_dataloaders_every_n_epochs 1 \
			--eval_batch_size 32 \
			--ckpt_metric mrr \
			--warmup_proportion 0.01 \
			--strategy ddp \
			--num_gpus 2 \
			--eval_interval 0.2 \
			--num_epochs 2 \
			--misc w_cls_w_lin_6_387/63_of_500_reranked_bienc_negs/epoch_1_w_reload

			sbatch -p rtx8000-long --gres gpu:2 --mem 64GB --job-name bi_hard_negs_epoch_1_2_500 bin/run.sh  \
			python models/train.py \
			--config config/el_zeshel_cross_enc.json \
			--exp_id 6_ReprCrossEnc \
			--loss ce \
			--pooling_type cls_w_lin \
			--neg_strategy precomp \
			--num_negs 63 \
			--train_batch_size 4 \
			--ent_w_score_file_template ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval/{}/m=-1_k=500_1_eoe-0-last.ckpt/crossenc_topk_preds_w_bienc_retrvr.txt \
			--ckpt_path             	../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
			--reload_dataloaders_every_n_epochs 1 \
			--eval_batch_size 32 \
			--ckpt_metric mrr \
			--warmup_proportion 0.01 \
			--strategy ddp \
			--num_gpus 2 \
			--eval_interval 0.2 \
			--num_epochs 3 \
			--misc w_cls_w_lin_6_387/63_of_500_reranked_bienc_negs/epoch_1_2

			Launching bi+cross eval w/ top-500 bienc entities
						
				for data in american_football fallout final_fantasy pro_wrestling world_of_warcraft coronation_street elder_scrolls ice_hockey muppets
				do
				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name m1cross_w_bi_$data bin/run.sh \
				python eval/run_cross_encoder_w_binenc_retriever_zeshel.py \
				--data_name $data \
				--n_ment -1 \
				--top_k 500 \
				--batch_size 1 \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=precomp_s=1234_w_cls_w_lin_6_387/63_of_500_reranked_bienc_negs/epoch_1/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=precomp_s=1234_w_cls_w_lin_6_387/63_of_500_reranked_bienc_negs/epoch_1/model/eoe-1-last.ckpt \
				--misc eoe-1-last.ckpt
				done

				# For doctor_who
				for mstart in 0 3000 
				do
				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name m1cross_w_bi_doctor_who-$mstart bin/run.sh \
				python eval/run_cross_encoder_w_binenc_retriever_zeshel.py \
				--data_name doctor_who \
				--n_ment_start $mstart \
				--n_ment 3000 \
				--top_k 500 \
				--batch_size 1 \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=precomp_s=1234_w_cls_w_lin_6_387/63_of_500_reranked_bienc_negs/epoch_1/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=precomp_s=1234_w_cls_w_lin_6_387/63_of_500_reranked_bienc_negs/epoch_1/model/eoe-1-last.ckpt \
				--misc eoe-1-last.ckpt_mstart_${mstart}
				done

				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name m1cross_w_bi_doctor_who-6K bin/run.sh \
				python eval/run_cross_encoder_w_binenc_retriever_zeshel.py \
				--data_name doctor_who \
				--n_ment_start 6000 \
				--n_ment 2334 \
				--top_k 500 \
				--batch_size 1 \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=precomp_s=1234_w_cls_w_lin_6_387/63_of_500_reranked_bienc_negs/epoch_1/eval \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=precomp_s=1234_w_cls_w_lin_6_387/63_of_500_reranked_bienc_negs/epoch_1/model/eoe-1-last.ckpt \
				--misc eoe-1-last.ckpt_mstart_6000

				# For military

					for mstart in 0 2000 4000 6000 8000 10000  
					do
					sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name m1cross_w_bi_military-$mstart bin/run.sh \
					python eval/run_cross_encoder_w_binenc_retriever_zeshel.py \
					--data_name military \
					--n_ment_start $mstart \
					--n_ment 2000 \
					--top_k 500 \
					--batch_size 1 \
					--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
					--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=precomp_s=1234_w_cls_w_lin_6_387/63_of_500_reranked_bienc_negs/epoch_1/eval \
					--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=precomp_s=1234_w_cls_w_lin_6_387/63_of_500_reranked_bienc_negs/epoch_1/model/eoe-1-last.ckpt \
					--misc eoe-1-last.ckpt_mstart_${mstart}
					done

					sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name m1cross_w_bi_military-12K bin/run.sh \
					python eval/run_cross_encoder_w_binenc_retriever_zeshel.py \
					--data_name military \
					--n_ment_start 12000 \
					--n_ment 1063 \
					--top_k 500 \
					--batch_size 1 \
					--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
					--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=precomp_s=1234_w_cls_w_lin_6_387/63_of_500_reranked_bienc_negs/epoch_1/eval \
					--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=precomp_s=1234_w_cls_w_lin_6_387/63_of_500_reranked_bienc_negs/epoch_1/model/eoe-1-last.ckpt \
					--misc eoe-1-last.ckpt_mstart_12000


					# For starwars

					for mstart in 0 2000 4000 6000 8000  
					do
					sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name m1cross_w_bi_starwars-$mstart bin/run.sh \
					python eval/run_cross_encoder_w_binenc_retriever_zeshel.py \
					--data_name starwars \
					--n_ment_start $mstart \
					--n_ment 2000 \
					--top_k 500 \
					--batch_size 1 \
					--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
					--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=precomp_s=1234_w_cls_w_lin_6_387/63_of_500_reranked_bienc_negs/epoch_1/eval \
					--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=precomp_s=1234_w_cls_w_lin_6_387/63_of_500_reranked_bienc_negs/epoch_1/model/eoe-1-last.ckpt \
					--misc eoe-1-last.ckpt_mstart_${mstart}
					done

					sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name m1cross_w_bi_starwars-10K bin/run.sh \
					python eval/run_cross_encoder_w_binenc_retriever_zeshel.py \
					--data_name starwars \
					--n_ment_start 10000 \
					--n_ment 1824 \
					--top_k 500 \
					--batch_size 1 \
					--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
					--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=precomp_s=1234_w_cls_w_lin_6_387/63_of_500_reranked_bienc_negs/epoch_1/eval \
					--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=precomp_s=1234_w_cls_w_lin_6_387/63_of_500_reranked_bienc_negs/epoch_1/model/eoe-1-last.ckpt \
					--misc eoe-1-last.ckpt_mstart_10000



			Training w/ cls_w_lin crossencoder w/ 63/500 biencoder hard negatives epoch=2
				sbatch -p rtx8000-long --gres gpu:2 --mem 64GB --job-name bi_hard_negs_epoch_2_w_reload bin/run.sh  \
				python models/train.py \
				--config config/el_zeshel_cross_enc.json \
				--exp_id 6_ReprCrossEnc \
				--loss ce \
				--pooling_type cls_w_lin \
				--neg_strategy precomp \
				--num_negs 63 \
				--train_batch_size 4 \
				--ent_w_score_file_template ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval/{}/m=-1_k=500_1_eoe-0-last.ckpt/crossenc_topk_preds_w_bienc_retrvr.txt \
				--ckpt_path             	../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=precomp_s=1234_w_cls_w_lin_6_387/63_of_500_reranked_bienc_negs/epoch_1/model/eoe-1-last.ckpt \
				--reload_dataloaders_every_n_epochs 1 \
				--eval_batch_size 32 \
				--ckpt_metric mrr \
				--warmup_proportion 0.01 \
				--strategy ddp \
				--num_gpus 2 \
				--eval_interval 0.2 \
				--num_epochs 3 \
				--misc w_cls_w_lin_6_387/63_of_500_reranked_bienc_negs/epoch_2


		Training w/ cls_w_lin crossencoder w/ 63/1000 biencoder hard negatives second epoch

			sbatch -p rtx8000-long --gres gpu:2 --mem 64GB --job-name bi_hard_negs_epoch_1_1K bin/run.sh  \
			python models/train.py \
			--config config/el_zeshel_cross_enc.json \
			--exp_id 6_ReprCrossEnc \
			--loss ce \
			--pooling_type cls_w_lin \
			--neg_strategy precomp \
			--num_negs 63 \
			--train_batch_size 4 \
			--ent_w_score_file_template ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval/{}/m=-1_k=1000_1_eoe-0-last.ckpt/crossenc_topk_preds_w_bienc_retrvr.txt \
			--ckpt_path             	../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
			--reload_dataloaders_every_n_epochs 1 \
			--eval_batch_size 32 \
			--ckpt_metric mrr \
			--warmup_proportion 0.01 \
			--strategy ddp \
			--num_gpus 2 \
			--eval_interval 0.2 \
			--num_epochs 2 \
			--misc w_cls_w_lin_6_387/63_of_1000_reranked_bienc_negs/epoch_1


			sbatch -p rtx8000-long --gres gpu:2 --mem 64GB --job-name bi_hard_negs_epoch_1_2_1K bin/run.sh  \
			python models/train.py \
			--config config/el_zeshel_cross_enc.json \
			--exp_id 6_ReprCrossEnc \
			--loss ce \
			--pooling_type cls_w_lin \
			--neg_strategy precomp \
			--num_negs 63 \
			--train_batch_size 4 \
			--ent_w_score_file_template ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval/{}/m=-1_k=1000_1_eoe-0-last.ckpt/crossenc_topk_preds_w_bienc_retrvr.txt \
			--ckpt_path             	../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
			--reload_dataloaders_every_n_epochs 1 \
			--eval_batch_size 32 \
			--ckpt_metric mrr \
			--warmup_proportion 0.01 \
			--strategy ddp \
			--num_gpus 2 \
			--eval_interval 0.2 \
			--num_epochs 3 \
			--misc w_cls_w_lin_6_387/63_of_1000_reranked_bienc_negs/epoch_1_2


		Training w/ cls_w_lin crossencoder w/ 63/500 Graph_Search 5-20-500 hard negatives second epoch

			sbatch -p rtx8000-long --gres gpu:2 --mem 64GB --job-name graph_negs_epoch_1_w_reload bin/run.sh  \
			python models/train.py \
			--config config/el_zeshel_cross_enc.json \
			--exp_id 6_ReprCrossEnc \
			--loss ce \
			--pooling_type cls_w_lin \
			--neg_strategy precomp \
			--num_negs 63 \
			--train_batch_size 4 \
			--ent_w_score_file_template ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval/{}/m=-1_k=100_g=knn_e2e_e=bienc_20_5_500_/crossenc_topk_preds_w_graph.txt \
			--ckpt_path             	../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
			--reload_dataloaders_every_n_epochs 1 \
			--eval_batch_size 32 \
			--ckpt_metric mrr \
			--warmup_proportion 0.01 \
			--strategy ddp \
			--num_gpus 2 \
			--eval_interval 0.2 \
			--num_epochs 2 \
			--misc w_cls_w_lin_6_387/63_of_500_5_20_500_e2e_graph/epoch_1


			Launching e2e score computation.

				for data in fallout final_fantasy pro_wrestling world_of_warcraft  coronation_street elder_scrolls ice_hockey muppets
				do
				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name e2e-ece0-$data bin/run.sh  \
				python eval/run_cross_encoder_for_ent_ent_matrix.py \
				--data_name $data \
				--n_ent_x -1 \
				--n_ent_y -1 \
				--topk 100 \
				--embed_type bienc \
				--token_opt m2e \
				--batch_size 150 \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=precomp_s=1234_w_cls_w_lin_6_387/63_of_500_5_20_500_e2e_graph/epoch_1/score_mats_eoe-1-last.ckpt \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=precomp_s=1234_w_cls_w_lin_6_387/63_of_500_5_20_500_e2e_graph/epoch_1/model/eoe-1-last.ckpt \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt 
				done


				# Chunking computation for domain=american_football
				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name e2e-ece0-american_football-0 bin/run.sh  \
				python eval/run_cross_encoder_for_ent_ent_matrix.py \
				--data_name american_football \
				--n_ent_x_start 0 \
				--n_ent_x  20000 \
				--n_ent_y -1 \
				--topk 100 \
				--embed_type bienc \
				--token_opt m2e \
				--batch_size 150 \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=precomp_s=1234_w_cls_w_lin_6_387/63_of_500_5_20_500_e2e_graph/epoch_1/score_mats_eoe-1-last.ckpt \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=precomp_s=1234_w_cls_w_lin_6_387/63_of_500_5_20_500_e2e_graph/epoch_1/model/eoe-1-last.ckpt \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
				--misc x_start_0
				done

				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name e2e-ece0-american_football-2K bin/run.sh  \
				python eval/run_cross_encoder_for_ent_ent_matrix.py \
				--data_name american_football \
				--n_ent_x_start 20000 \
				--n_ent_x  11929 \
				--n_ent_y -1 \
				--topk 100 \
				--embed_type bienc \
				--token_opt m2e \
				--batch_size 150 \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=precomp_s=1234_w_cls_w_lin_6_387/63_of_500_5_20_500_e2e_graph/epoch_1/score_mats_eoe-1-last.ckpt \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=precomp_s=1234_w_cls_w_lin_6_387/63_of_500_5_20_500_e2e_graph/epoch_1/model/eoe-1-last.ckpt \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
				--misc x_start_20000




				# Chunking computation for domain=doctor_who
				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name e2e-ece0-doctor_who-0 bin/run.sh  \
				python eval/run_cross_encoder_for_ent_ent_matrix.py \
				--data_name doctor_who \
				--n_ent_x_start 0 \
				--n_ent_x  20000 \
				--n_ent_y -1 \
				--topk 100 \
				--embed_type bienc \
				--token_opt m2e \
				--batch_size 150 \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=precomp_s=1234_w_cls_w_lin_6_387/63_of_500_5_20_500_e2e_graph/epoch_1/score_mats_eoe-1-last.ckpt \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=precomp_s=1234_w_cls_w_lin_6_387/63_of_500_5_20_500_e2e_graph/epoch_1/model/eoe-1-last.ckpt \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
				--misc x_start_0
				done

				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name e2e-ece0-doctor_who-2K bin/run.sh  \
				python eval/run_cross_encoder_for_ent_ent_matrix.py \
				--data_name doctor_who \
				--n_ent_x_start 20000 \
				--n_ent_x  20281 \
				--n_ent_y -1 \
				--topk 100 \
				--embed_type bienc \
				--token_opt m2e \
				--batch_size 150 \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=precomp_s=1234_w_cls_w_lin_6_387/63_of_500_5_20_500_e2e_graph/epoch_1/score_mats_eoe-1-last.ckpt \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=precomp_s=1234_w_cls_w_lin_6_387/63_of_500_5_20_500_e2e_graph/epoch_1/model/eoe-1-last.ckpt \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
				--misc x_start_20000


				# Chunking computation for domain=military
				for xstart in 0 20000 40000 60000 80000
				do
				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name e2e-ece0-miliary-$xstart bin/run.sh  \
				python eval/run_cross_encoder_for_ent_ent_matrix.py \
				--data_name military \
				--n_ent_x_start $xstart \
				--n_ent_x  20000 \
				--n_ent_y -1 \
				--topk 100 \
				--embed_type bienc \
				--token_opt m2e \
				--batch_size 150 \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=precomp_s=1234_w_cls_w_lin_6_387/63_of_500_5_20_500_e2e_graph/epoch_1/score_mats_eoe-1-last.ckpt \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=precomp_s=1234_w_cls_w_lin_6_387/63_of_500_5_20_500_e2e_graph/epoch_1/model/eoe-1-last.ckpt \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
				--misc x_start_$xstart
				done

				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name e2e-ece0-miliary-10K bin/run.sh  \
				python eval/run_cross_encoder_for_ent_ent_matrix.py \
				--data_name military \
				--n_ent_x_start 100000 \
				--n_ent_x  4520 \
				--n_ent_y -1 \
				--topk 100 \
				--embed_type bienc \
				--token_opt m2e \
				--batch_size 150 \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=precomp_s=1234_w_cls_w_lin_6_387/63_of_500_5_20_500_e2e_graph/epoch_1/score_mats_eoe-1-last.ckpt \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=precomp_s=1234_w_cls_w_lin_6_387/63_of_500_5_20_500_e2e_graph/epoch_1/model/eoe-1-last.ckpt \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
				--misc x_start_100000

				# Chunking computation for domain=starwars
				for xstart in 0 20000 40000 60000
				do
				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name e2e-ece0-startwars-$xstart bin/run.sh  \
				python eval/run_cross_encoder_for_ent_ent_matrix.py \
				--data_name starwars \
				--n_ent_x_start $xstart \
				--n_ent_x  20000 \
				--n_ent_y -1 \
				--topk 100 \
				--embed_type bienc \
				--token_opt m2e \
				--batch_size 150 \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=precomp_s=1234_w_cls_w_lin_6_387/63_of_500_5_20_500_e2e_graph/epoch_1/score_mats_eoe-1-last.ckpt \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=precomp_s=1234_w_cls_w_lin_6_387/63_of_500_5_20_500_e2e_graph/epoch_1/model/eoe-1-last.ckpt \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
				--misc x_start_$xstart
				done

				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name e2e-ece0-startwars-60K bin/run.sh  \
				python eval/run_cross_encoder_for_ent_ent_matrix.py \
				--data_name starwars \
				--n_ent_x_start 80000 \
				--n_ent_x  7056 \
				--n_ent_y -1 \
				--topk 100 \
				--embed_type bienc \
				--token_opt m2e \
				--batch_size 150 \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=precomp_s=1234_w_cls_w_lin_6_387/63_of_500_5_20_500_e2e_graph/epoch_1/score_mats_eoe-1-last.ckpt \
				--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=precomp_s=1234_w_cls_w_lin_6_387/63_of_500_5_20_500_e2e_graph/epoch_1/model/eoe-1-last.ckpt \
				--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
				--misc x_start_80000


		Training w/ cls_w_lin crossencoder w/ 63/500 Graph_Search 5-20-1000 hard negatives second epoch

			sbatch -p rtx8000-long --gres gpu:2 --mem 64GB --job-name graph_negs_epoch_1_w_reload bin/run.sh  \
			python models/train.py \
			--config config/el_zeshel_cross_enc.json \
			--exp_id 6_ReprCrossEnc \
			--loss ce \
			--pooling_type cls_w_lin \
			--neg_strategy precomp \
			--num_negs 63 \
			--train_batch_size 4 \
			--ent_w_score_file_template ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval/{}/m=-1_k=100_g=knn_e2e_e=bienc_20_5_1000_/crossenc_topk_preds_w_graph.txt \
			--ckpt_path             	../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
			--reload_dataloaders_every_n_epochs 1 \
			--eval_batch_size 32 \
			--ckpt_metric mrr \
			--warmup_proportion 0.01 \
			--strategy ddp \
			--num_gpus 2 \
			--eval_interval 0.2 \
			--num_epochs 2 \
			--misc w_cls_w_lin_6_387/63_of_1000_5_20_1000_e2e_graph/epoch_1



	26 May

		Debugging filtering nbrs in e2e graph to reduce number of cross-encoder calls while maintaining good recall
			python eval/nsw_eval_zeshel.py \
			--data_name pro_wrestling \
			--embed_type none \
			--res_dir          ../../results/6_ReprCrossEnc/_del \
			--graph_type knn_e2e  \
			--entry_method tfidf \
			--graph_metric l2 \
			--debug_mode 1 \
			--misc try_2 \
			--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
			--e2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_w_bienc_w_ddp_w_best_wrt_dev_mrr_cls_w_lin/score_mats_model-1-11359.0--80.19.ckpt/pro_wrestling/ent_to_ent_scores_n_e_10133x10133_topk_100_embed_bienc_m2e_.pkl  \
			--score_mat_dir ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_w_bienc_w_ddp_w_best_wrt_dev_mrr_cls_w_lin/score_mats_model-1-11359.0--80.19.ckpt



			python eval/nsw_eval_zeshel.py \
			--project_name Graph_Search \
			--data_name pro_wrestling \
			--n_ment 100 \
			--embed_type none \
			--entry_method bienc \
			--graph_type knn_e2e \
			--graph_metric l2 \
			--masked_node_frac 0.0 \
			--misc c=02_CE_6_49_b=00_HardNegs_6_20_n_m=100_mnf=0.0 \
			--bi_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
			--e2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_w_bienc_w_ddp_w_best_wrt_dev_mrr_cls_w_lin/score_mats_model-1-11359.0--80.19.ckpt/pro_wrestling/ent_to_ent_scores_n_e_10133x10133_topk_100_embed_bienc_m2e_.pkl \
			--a2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_w_bienc_w_ddp_w_best_wrt_dev_mrr_cls_w_lin/score_mats_model-1-11359.0--80.19.ckpt/pro_wrestling/ent_to_ent_scores_n_e_10133x10133_topk_1000_embed_bienc_m2e_.pkl \
			--score_mat_dir ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_w_bienc_w_ddp_w_best_wrt_dev_mrr_cls_w_lin/score_mats_model-1-11359.0--80.19.ckpt \
			--res_dir ../../results/6_ReprCrossEnc/Graph_Search/pro_wrestling_masked_nodes \
			--force_exact_init_search 1


	27 May
		Trying fixed set of anchor embeddings for cls-crossenc model

			for data in lego pro_wrestling doctor_who yugioh 
			do
			sbatch -p gpu --gres gpu:1 --mem 32GB --job-name anchor_ents_bienc_${data} --exclude gpu-0-0 bin/run.sh  \
			python eval/find_anchor_entities.py \
			--data $data \
			--embed_type bienc  \
			--bi_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
			--res_dir ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/entity_clusters \
			--misc method=alt
			done


			for data in lego pro_wrestling doctor_who yugioh 
			do
			sbatch -p cpu --mem 32GB --job-name anchor_ents_tfidf_${data} bin/run.sh  \
			python eval/find_anchor_entities.py \
			--data $data \
			--embed_type tfidf  \
			--bi_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
			--res_dir ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/entity_clusters \
			--misc method=alt
			done


		Launching e2e score computation based on anchor enitites for CE cls-crossenc 6-49

			# for data in doctor_who
			for topk in 500
			do
			for embmethod in bienc
			do
			for xstart in 0 1000 2000 3000 4000 5000 6000 7000 8000 9000  
			do
			sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name e2e-anchor-${embmethod}-pro_wrestling bin/run.sh  \
			python eval/run_cross_encoder_for_ent_ent_matrix.py \
			--data_name pro_wrestling \
			--n_ent_x_start ${xstart} \
			--n_ent_x 1000 \
			--n_ent_y -1 \
			--topk $topk \
			--topk_ents_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/entity_clusters/pro_wrestling/anchor_ents_emb=${embmethod}_k=${topk}__method=alt.json \
			--embed_type ${embmethod} \
			--token_opt m2e \
			--batch_size 150 \
			--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_w_bienc_w_ddp_w_best_wrt_dev_mrr_cls_w_lin/score_mats_model-1-11359.0--80.19.ckpt \
			--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_w_bienc_w_ddp_w_best_wrt_dev_mrr_cls_w_lin/model/model-1-11359.0--80.19.ckpt \
			--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
			--misc kmed_cluster_alt_xstart_${xstart}
			done

			sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name e2e-anchor-${embmethod}-pro_wrestling bin/run.sh  \
			python eval/run_cross_encoder_for_ent_ent_matrix.py \
			--data_name pro_wrestling \
			--n_ent_x_start 10000 \
			--n_ent_x 133 \
			--n_ent_y -1 \
			--topk $topk \
			--topk_ents_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/entity_clusters/pro_wrestling/anchor_ents_emb=${embmethod}_k=${topk}__method=alt.json \
			--embed_type ${embmethod} \
			--token_opt m2e \
			--batch_size 150 \
			--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_w_bienc_w_ddp_w_best_wrt_dev_mrr_cls_w_lin/score_mats_model-1-11359.0--80.19.ckpt \
			--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_w_bienc_w_ddp_w_best_wrt_dev_mrr_cls_w_lin/model/model-1-11359.0--80.19.ckpt \
			--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
			--misc kmed_cluster_alt_xstart_10000
			done
			done


		Debug
			python eval/find_anchor_entities.py \
			--data lego \
			--embed_type tfidf  \
			--res_dir ../../results/6_ReprCrossEnc/_del \
			--misc test \
			--disable_wandb 1 


	28-30 May
		
		Training entity embeddings (through distillation or contrastive learning)

			First computing mention-entity scores

				sbatch -p rtx8000-long --gres gpu:1 --mem 64GB --job-name pro_m2e bin/run.sh  \
				python eval/run_cross_encoder_for_ment_ent_matrix_zeshel.py \
				--n_ment 100 \
				--n_ent -1 \
				--batch_size 500 \
				--data_name pro_wrestling \
				--layers final \
				--cross_model_ckpt ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt

				sbatch -p rtx8000-long --gres gpu:1 --mem 64GB --job-name wow_m2e bin/run.sh  \
				python eval/run_cross_encoder_for_ment_ent_matrix_zeshel.py \
				--n_ment 1437 \
				--n_ent -1 \
				--batch_size 500 \
				--data_name world_of_warcraft \
				--layers final \
				--cross_model_ckpt ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt
			

			Training w/ standard distillation objective from 6-20 checkpoint
				sbatch -p rtx8000-long --gres gpu:2 --mem 64GB --job-name pro_distill_6_20 bin/run.sh  \
				python models/train.py \
				--config config/el_zeshel_bi_enc_pro_only.json \
				--exp_id 7_EntModel \
				--neg_strategy distill \
				--distill_n_labels 64 \
				--train_batch_size 8 \
				--path_to_model ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
				--ent_w_score_file_template ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt/{}/ment_to_ent_scores_n_m_1392_n_e_10133_all_layers_False.json \
				--reload_dataloaders_every_n_epochs 0 \
				--eval_batch_size 32 \
				--warmup_proportion 0.01 \
				--strategy ddp \
				--num_gpus 2 \
				--eval_interval 0.2 \
				--num_epochs 20 \
				--misc distill_w_64_crossenc_negs_wrt_6_400_pro_from_6_20

			
			Training w/ standard distillation objective from scratch
				sbatch -p rtx8000-long --gres gpu:2 --mem 64GB --job-name pro_distill_scratch bin/run.sh  \
				python models/train.py \
				--config config/el_zeshel_bi_enc_pro_only.json \
				--exp_id 7_EntModel \
				--neg_strategy distill \
				--distill_n_labels 64 \
				--train_batch_size 8 \
				--ent_w_score_file_template ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt/{}/ment_to_ent_scores_n_m_1392_n_e_10133_all_layers_False.json \
				--reload_dataloaders_every_n_epochs 0 \
				--eval_batch_size 32 \
				--warmup_proportion 0.01 \
				--strategy ddp \
				--num_gpus 2 \
				--num_epochs 20 \
				--eval_interval 0.2 \
				--misc distill_w_64_crossenc_negs_wrt_6_400_pro_from_scratch


			Training by taking top-k cross-encoder entities as positive and doing hard negative mining otherwise (from 6-20 checkpoint)

				sbatch -p rtx8000-long --gres gpu:2 --mem 64GB --job-name pro_distill_6_20 bin/run.sh  \
				python models/train.py \
				--config config/el_zeshel_bi_enc_pro_only.json \
				--exp_id 7_EntModel \
				--neg_strategy top_ce_as_pos_w_bienc_hard_negs \
				--distill_n_labels 64 \
				--num_negs 64 \
				--train_batch_size 8 \
				--path_to_model ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
				--ent_w_score_file_template ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt/{}/ment_to_ent_scores_n_m_1392_n_e_10133_all_layers_False.json \
				--reload_dataloaders_every_n_epochs 1 \
				--eval_batch_size 32 \
				--warmup_proportion 0.01 \
				--strategy ddp \
				--num_gpus 2 \
				--eval_interval 1.0 \
				--num_epochs 20 \
				--misc distill_w_64_crossenc_negs_wrt_6_400_pro_from_6_20

						[To-Run] Training w/ standard distillation objective from scratch
				sbatch -p rtx8000-long --gres gpu:2 --mem 64GB --job-name pro_distill_scratch bin/run.sh  \
				python models/train.py \
				--config config/el_zeshel_bi_enc_pro_only.json \
				--exp_id 7_EntModel \
				--neg_strategy top_ce_as_pos_w_bienc_hard_negs \
				--distill_n_labels 64 \
				--num_negs 64 \
				--train_batch_size 8 \
				--ent_w_score_file_template ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt/{}/ment_to_ent_scores_n_m_1392_n_e_10133_all_layers_False.json \
				--reload_dataloaders_every_n_epochs 1 \
				--eval_batch_size 32 \
				--warmup_proportion 0.01 \
				--strategy ddp \
				--num_gpus 2 \
				--num_epochs 20 \
				--eval_interval 1.0 \
				--misc distill_w_64_crossenc_negs_wrt_6_400_pro_from_scratch


				Debug

				python models/train.py \
				--config config/el_zeshel_bi_enc_debug.json \
				--exp_id _0_Debug \
				--neg_strategy top_ce_as_pos_w_bienc_hard_negs \
				--distill_n_labels 5 \
				--num_negs 10 \
				--train_batch_size 8 \
				--ent_w_score_file_template ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt/{}/ment_to_ent_scores_n_m_1392_n_e_10133_all_layers_False.json \
				--reload_dataloaders_every_n_epochs 0 \
				--eval_batch_size 32 \
				--warmup_proportion 0.01 \
				--num_epochs 20 \
				--eval_interval 0.9 \
				--misc debug_multiple_pos_labels

				sbatch -p gpu --gres gpu:1 --mem 64GB --job-name pro_distill_6_20 --exclude gpu-0-0 bin/run.sh  \
				python models/train.py \
				--config config/el_zeshel_bi_enc_debug.json \
				--exp_id _0_Debug \
				--neg_strategy top_ce_as_pos_w_bienc_hard_negs \
				--distill_n_labels 5 \
				--num_negs 10 \
				--train_batch_size 8 \
				--path_to_model ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
				--ent_w_score_file_template ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt/{}/ment_to_ent_scores_n_m_1392_n_e_10133_all_layers_False.json \
				--reload_dataloaders_every_n_epochs 0 \
				--eval_batch_size 32 \
				--warmup_proportion 0.01 \
				--num_epochs 20 \
				--eval_interval 0.9 \
				--misc debug_multiple_pos_labels_w_6_20_init



1-2 June


	CUR Decomposition for e-crossenc model 

		sbatch -p gpu --gres gpu:1 --mem 64GB --job-name cur_lego_ece --exclude gpu-0-0 bin/run.sh  \
		python eval/run_retrieval_eval_wrt_exact_crossenc.py \
		--data lego \
		--res_dir ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt \
		--bi_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
		--n_seeds 10 \
		--n_ment 1199

		sbatch -p gpu --gres gpu:1 --mem 64GB --job-name cur_pro_ece --exclude gpu-0-0 bin/run.sh  \
		python eval/run_retrieval_eval_wrt_exact_crossenc.py \
		--data pro_wrestling \
		--res_dir ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt \
		--bi_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
		--n_seeds 10 \
		--n_ment 1392

		sbatch -p gpu --gres gpu:1 --mem 64GB --job-name cur_yug_ece --exclude gpu-0-0 bin/run.sh  \
		python eval/run_retrieval_eval_wrt_exact_crossenc.py \
		--data yugioh \
		--res_dir ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt \
		--bi_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
		--n_seeds 10 \
		--n_ment 3374


		sbatch -p gpu --gres gpu:1 --mem 64GB --job-name cur_dw_ece --exclude gpu-0-0 bin/run.sh  \
		python eval/run_retrieval_eval_wrt_exact_crossenc.py \
		--data doctor_who \
		--res_dir ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt \
		--bi_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
		--n_seeds 10 \
		--n_ment 2100		

		sbatch -p gpu --gres gpu:1 --mem 64GB --job-name cur_lego_cls --exclude gpu-0-0 bin/run.sh  \
		python eval/run_retrieval_eval_wrt_exact_crossenc.py \
		--data lego \
		--res_dir ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_w_bienc_w_ddp_w_best_wrt_dev_mrr_cls_w_lin/score_mats_model-1-11359.0--80.19.ckpt \
		--bi_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
		--n_seeds 10 \
		--n_ment 1199

		sbatch -p gpu --gres gpu:1 --mem 64GB --job-name cur_pro_cls --exclude gpu-0-0  bin/run.sh  \
		python eval/run_retrieval_eval_wrt_exact_crossenc.py \
		--data pro_wrestling \
		--res_dir ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_w_bienc_w_ddp_w_best_wrt_dev_mrr_cls_w_lin/score_mats_model-1-11359.0--80.19.ckpt \
		--bi_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
		--n_seeds 10 \
		--n_ment 1392


		Debug
			python eval/run_retrieval_eval_wrt_exact_crossenc.py \
			--data pro_wrestling \
			--res_dir ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt \
			--bi_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
			--n_seeds 10 \
			--n_ment 1392 \
			--misc debug


	Launching ment-ent matrix computation for yugioh and doctor_who for e-crossenc model  (6-256)
		for mstart in {3000..3325..25}
		do
		#sbatch -p gpu --gres gpu:1 --mem 32GB --job-name mat1-ce-$data --exclude gpu-0-0 bin/run.sh \
		sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name mat1-ce-yugioh-$mstart  bin/run.sh  \
		python eval/run_cross_encoder_for_ment_ent_matrix_zeshel.py \
		--n_ment_start $mstart \
		--n_ment 25 \
		--n_ent -1 \
		--batch_size 300 \
		--data_name yugioh \
		--layers final \
		--cross_model_ckpt ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/model/model-2-15999.0--79.46.ckpt \
		--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt \
		--misc mstart_${mstart}
		done

		sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name mat1-ce-yugioh-3350  bin/run.sh  \
		python eval/run_cross_encoder_for_ment_ent_matrix_zeshel.py \
		--n_ment_start  3350 \
		--n_ment 24 \
		--n_ent -1 \
		--batch_size 300 \
		--data_name yugioh \
		--layers final \
		--cross_model_ckpt ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/model/model-2-15999.0--79.46.ckpt \
		--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt \
		--misc mstart_3350


		for mstart in {0..2000..100}
		do
		#sbatch -p gpu --gres gpu:1 --mem 32GB --job-name mat1-ce-$data --exclude gpu-0-0 bin/run.sh \
		sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name mat1-ce-dw-$mstart  bin/run.sh  \
		python eval/run_cross_encoder_for_ment_ent_matrix_zeshel.py \
		--n_ment_start $mstart \
		--n_ment 100 \
		--n_ent -1 \
		--batch_size 300 \
		--data_name doctor_who \
		--layers final \
		--cross_model_ckpt ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/model/model-2-15999.0--79.46.ckpt \
		--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt \
		--misc mstart_${mstart}
		done


	Cross-Encoder w/ random in-batch negs

		sbatch -p rtx8000-long --gres gpu:2 --mem 64GB --job-name cls_train bin/run.sh  \
		python models/train.py \
		--config config/el_zeshel_cross_enc.json \
		--exp_id 6_ReprCrossEnc \
		--loss ce \
		--pooling_type cls_w_lin \
		--neg_strategy random \
		--num_negs 63 \
		--train_batch_size 4 \
		--reload_dataloaders_every_n_epochs 1 \
		--eval_batch_size 32 \
		--ckpt_metric mrr \
		--warmup_proportion 0.01 \
		--strategy ddp \
		--num_gpus 2 \
		--eval_interval 0.2 \
		--num_epochs 4 \
		--misc 63_negs_w_cls_w_lin


		sbatch -p rtx8000-long --gres gpu:2 --mem 64GB --job-name ece_random bin/run.sh  \
		python models/train.py \
		--config config/el_zeshel_cross_enc.json \
		--exp_id 6_ReprCrossEnc \
		--loss ce \
		--cross_enc_type w_embeds \
		--neg_strategy random \
		--num_negs 63 \
		--train_batch_size 4 \
		--reload_dataloaders_every_n_epochs 1 \
		--eval_batch_size 32 \
		--ckpt_metric mrr \
		--warmup_proportion 0.01 \
		--strategy ddp \
		--num_gpus 2 \
		--eval_interval 0.2 \
		--num_epochs 4 \
		--misc 63_negs_w_crossenc_w_embeds



		python models/train.py \
		--config config/el_zeshel_cross_enc_debug.json \
		--exp_id _0_Debug \
		--loss ce \
		--pooling_type cls_w_lin \
		--neg_strategy random \
		--num_negs 5 \
		--train_batch_size 4 \
		--reload_dataloaders_every_n_epochs 1 \
		--eval_batch_size 32 \
		--ckpt_metric mrr \
		--warmup_proportion 0.01 \
		--eval_interval 0.2 \
		--num_epochs 4 \
		--misc 63_negs_w_cls_w_lin_debug



2 June


	Running graph eval w/ cur-based anchor embeds of entities

		python eval/nsw_eval_zeshel.py  \
		--project_name _0_Debug  \
		--data_name pro_wrestling  \
		--n_ment -1  \
		--embed_type cur-anchor  \
		--entry_method bienc  \
		--graph_type nsw  \
		--graph_metric l2  \
		--masked_node_frac 0.0  \
		--misc c=05_E-CrossEnc_6_256_b=00_HardNegs_6_20_n_m=-1  \
		--bi_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
		--e2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt/pro_wrestling/ent_to_ent_scores_n_e_10133x10133_topk_100_embed_bienc_m2e_.pkl  \
		--a2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt/pro_wrestling/ent_to_ent_scores_n_e_10133x10133_.pkl  \
		--score_mat_dir ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt  \
		--res_dir ../../results/6_ReprCrossEnc/Graph_Search/pro_wrestling_debug_cur  \
		--force_exact_init_search 1 


	Running some more ment-ent matrix computation things

		For pro_wrestling

			For joint training with mutual distillation
				for mstart in {0..1000..250}
				do
				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name mat1-ce-pro_wrestling-${mstart}  bin/run.sh  \
				python eval/run_cross_encoder_for_ment_ent_matrix_zeshel.py \
				--n_ment_start $mstart \
				--n_ment 250 \
				--n_ent -1 \
				--batch_size 300 \
				--data_name pro_wrestling \
				--layers final \
				--cross_model_ckpt ../../results/6_ReprCrossEnc/d=ent_link/joint_train/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_w_0.5_bi_cross_loss_0.5_mutual_distill_from_scratch/model/model-3-24599.0--75.46.ckpt \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/joint_train/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_w_0.5_bi_cross_loss_0.5_mutual_distill_from_scratch/score_mats_model-3-24599.0--75.46.ckpt \
				--misc mstart_${mstart}
				done

				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name mat1-ce-pro_wrestling-1250  bin/run.sh  \
				python eval/run_cross_encoder_for_ment_ent_matrix_zeshel.py \
				--n_ment_start $mstart \
				--n_ment 142 \
				--n_ent -1 \
				--batch_size 300 \
				--data_name pro_wrestling \
				--layers final \
				--cross_model_ckpt ../../results/6_ReprCrossEnc/d=ent_link/joint_train/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_w_0.5_bi_cross_loss_0.5_mutual_distill_from_scratch/model/model-3-24599.0--75.46.ckpt \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/joint_train/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_w_0.5_bi_cross_loss_0.5_mutual_distill_from_scratch/score_mats_model-3-24599.0--75.46.ckpt \
				--misc mstart_1250


			For joint training 

				for mstart in {0..1000..250}
				do
				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name mat2-ce-pro_wrestling-${mstart}  bin/run.sh  \
				python eval/run_cross_encoder_for_ment_ent_matrix_zeshel.py \
				--n_ment_start $mstart \
				--n_ment 250 \
				--n_ent -1 \
				--batch_size 300 \
				--data_name pro_wrestling \
				--layers final \
				--cross_model_ckpt ../../results/6_ReprCrossEnc/d=ent_link/joint_train/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_w_0.5_bi_cross_loss_from_scratch/model/model-3-19679.0--77.23.ckpt \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/joint_train/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_w_0.5_bi_cross_loss_from_scratch/score_mats_model-3-19679.0--77.23.ckpt \
				--misc mstart_${mstart}
				done

				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name mat2-ce-pro_wrestling-1250  bin/run.sh  \
				python eval/run_cross_encoder_for_ment_ent_matrix_zeshel.py \
				--n_ment_start $mstart \
				--n_ment 142 \
				--n_ent -1 \
				--batch_size 300 \
				--data_name pro_wrestling \
				--layers final \
				--cross_model_ckpt ../../results/6_ReprCrossEnc/d=ent_link/joint_train/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_w_0.5_bi_cross_loss_from_scratch/model/model-3-19679.0--77.23.ckpt \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/joint_train/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_w_0.5_bi_cross_loss_from_scratch/score_mats_model-3-19679.0--77.23.ckpt \
				--misc mstart_1250


			For bi->cross distillation

				for mstart in {0..1000..250}
				do
				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name mat3-ce-pro_wrestling-${mstart}  bin/run.sh  \
				python eval/run_cross_encoder_for_ment_ent_matrix_zeshel.py \
				--n_ment_start $mstart \
				--n_ment 250 \
				--n_ent -1 \
				--batch_size 300 \
				--data_name pro_wrestling \
				--layers final \
				--cross_model_ckpt ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_distill_s=1234_crossenc_w_embeds/model/model-1-12279.0-1.91.ckpt \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_distill_s=1234_crossenc_w_embeds/score_mats_model-1-12279.0-1.91.ckpt \
				--misc mstart_${mstart}
				done

				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name mat3-ce-pro_wrestling-1250  bin/run.sh  \
				python eval/run_cross_encoder_for_ment_ent_matrix_zeshel.py \
				--n_ment_start $mstart \
				--n_ment 142 \
				--n_ent -1 \
				--batch_size 300 \
				--data_name pro_wrestling \
				--layers final \
				--cross_model_ckpt ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_distill_s=1234_crossenc_w_embeds/model/model-1-12279.0-1.91.ckpt \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_distill_s=1234_crossenc_w_embeds/score_mats_model-1-12279.0-1.91.ckpt \
				--misc mstart_1250

				
		For yugioh

			For joint training with mutual distillation

				for mstart in {0..2500..500}
				do
				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name mat1-ce-yugioh-$mstart  bin/run.sh  \
				python eval/run_cross_encoder_for_ment_ent_matrix_zeshel.py \
				--n_ment_start $mstart \
				--n_ment 500 \
				--n_ent -1 \
				--batch_size 300 \
				--data_name yugioh \
				--layers final \
				--cross_model_ckpt ../../results/6_ReprCrossEnc/d=ent_link/joint_train/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_w_0.5_bi_cross_loss_from_scratch/model/model-3-19679.0--77.23.ckpt \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/joint_train/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_w_0.5_bi_cross_loss_from_scratch/score_mats_model-3-19679.0--77.23.ckpt \
				--misc mstart_${mstart}
				done

				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name mat1-ce-yugioh-3000  bin/run.sh  \
				python eval/run_cross_encoder_for_ment_ent_matrix_zeshel.py \
				--n_ment_start 3000 \
				--n_ment 374 \
				--n_ent -1 \
				--batch_size 300 \
				--data_name yugioh \
				--layers final \
				--cross_model_ckpt ../../results/6_ReprCrossEnc/d=ent_link/joint_train/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_w_0.5_bi_cross_loss_from_scratch/model/model-3-19679.0--77.23.ckpt \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/joint_train/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_w_0.5_bi_cross_loss_from_scratch/score_mats_model-3-19679.0--77.23.ckpt \
				--misc mstart_3000

			For joint training
				for mstart in {0..2500..500}
				do
				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name mat2-ce-yugioh-$mstart  bin/run.sh  \
				python eval/run_cross_encoder_for_ment_ent_matrix_zeshel.py \
				--n_ment_start $mstart \
				--n_ment 500 \
				--n_ent -1 \
				--batch_size 300 \
				--data_name yugioh \
				--layers final \
				--cross_model_ckpt ../../results/6_ReprCrossEnc/d=ent_link/joint_train/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_w_0.5_bi_cross_loss_0.5_mutual_distill_from_scratch/model/model-3-24599.0--75.46.ckpt \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/joint_train/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_w_0.5_bi_cross_loss_0.5_mutual_distill_from_scratch/score_mats_model-3-24599.0--75.46.ckpt \
				--misc mstart_${mstart}
				done

				sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name mat2-ce-yugioh-3000  bin/run.sh  \
				python eval/run_cross_encoder_for_ment_ent_matrix_zeshel.py \
				--n_ment_start 3000 \
				--n_ment 374 \
				--n_ent -1 \
				--batch_size 300 \
				--data_name yugioh \
				--layers final \
				--cross_model_ckpt ../../results/6_ReprCrossEnc/d=ent_link/joint_train/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_w_0.5_bi_cross_loss_0.5_mutual_distill_from_scratch/model/model-3-24599.0--75.46.ckpt \
				--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/joint_train/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_w_0.5_bi_cross_loss_0.5_mutual_distill_from_scratch/score_mats_model-3-24599.0--75.46.ckpt \
				--misc mstart_3000


			For bi->cross distillation
				


4 June 

	CUR Decomposition for e-crossenc model 

					
		for dir in "joint_train/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_w_0.5_bi_cross_loss_0.5_mutual_distill_from_scratch/score_mats_model-3-24599.0--75.46.ckpt" "joint_train/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_w_0.5_bi_cross_loss_from_scratch/score_mats_model-3-19679.0--77.23.ckpt" "m=cross_enc_l=ce_neg=bienc_distill_s=1234_crossenc_w_embeds/score_mats_model-1-12279.0-1.91.ckpt"
		do
		# sbatch -p gpu --gres gpu:1 --mem 32GB --job-name cur_pro_ece --exclude gpu-0-0 bin/run.sh  \
		# python eval/run_retrieval_eval_wrt_exact_crossenc.py \
		# --data pro_wrestling \
		# --res_dir ../../results/6_ReprCrossEnc/d=ent_link/${dir} \
		# --bi_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
		# --n_seeds 10 \
		# --n_ment 1392 \
		# --misc w_exact_cost_bienc

		sleep 1
		sbatch -p gpu --gres gpu:1 --mem 32GB --job-name cur_yug_ece --exclude gpu-0-0 bin/run.sh  \
		python eval/run_retrieval_eval_wrt_exact_crossenc.py \
		--data yugioh \
		--res_dir ../../results/6_ReprCrossEnc/d=ent_link/${dir} \
		--bi_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
		--n_seeds 10 \
		--n_ment 3374 \
		--misc w_exact_cost_bienc

		sleep 1
		done

		# With self-retrieval models
		# For joint train w/ mutual distillation
			sbatch -p gpu --gres gpu:1 --mem 32GB --job-name cur_pro_ece_self_1 --exclude gpu-0-0 bin/run.sh  \
			python eval/run_retrieval_eval_wrt_exact_crossenc.py \
			--data pro_wrestling \
			--res_dir ../../results/6_ReprCrossEnc/d=ent_link/joint_train/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_w_0.5_bi_cross_loss_0.5_mutual_distill_from_scratch/score_mats_model-3-24599.0--75.46.ckpt \
			--bi_model_file ../../results/6_ReprCrossEnc/d=ent_link/joint_train/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_w_0.5_bi_cross_loss_0.5_mutual_distill_from_scratch/model/model-3-24599.0--75.46.ckpt \
			--n_seeds 10 \
			--n_ment 1392 \
			--misc w_exact_cost_bienc_self_rtvr

			sbatch -p gpu --gres gpu:1 --mem 32GB --job-name cur_yug_ece_self_1 --exclude gpu-0-0 bin/run.sh  \
			python eval/run_retrieval_eval_wrt_exact_crossenc.py \
			--data yugioh \
			--res_dir ../../results/6_ReprCrossEnc/d=ent_link/joint_train/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_w_0.5_bi_cross_loss_0.5_mutual_distill_from_scratch/score_mats_model-3-24599.0--75.46.ckpt \
			--bi_model_file ../../results/6_ReprCrossEnc/d=ent_link/joint_train/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_w_0.5_bi_cross_loss_0.5_mutual_distill_from_scratch/model/model-3-24599.0--75.46.ckpt \
			--n_seeds 10 \
			--n_ment 3374 \
			--misc w_exact_cost_bienc_self_rtvr

				
		# For joint train
			sbatch -p gpu --gres gpu:1 --mem 32GB --job-name cur_pro_ece_self_2 --exclude gpu-0-0 bin/run.sh  \
			python eval/run_retrieval_eval_wrt_exact_crossenc.py \
			--data pro_wrestling \
			--res_dir ../../results/6_ReprCrossEnc/d=ent_link/joint_train/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_w_0.5_bi_cross_loss_from_scratch/score_mats_model-3-19679.0--77.23.ckpt \
			--bi_model_file ../../results/6_ReprCrossEnc/d=ent_link/joint_train/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_w_0.5_bi_cross_loss_from_scratch/model/model-3-19679.0--77.23.ckpt \
			--n_seeds 10 \
			--n_ment 1392 \
			--misc w_exact_cost_bienc_self_rtvr

			sbatch -p gpu --gres gpu:1 --mem 32GB --job-name cur_yug_ece_self_2 --exclude gpu-0-0 bin/run.sh  \
			python eval/run_retrieval_eval_wrt_exact_crossenc.py \
			--data yugioh \
			--res_dir ../../results/6_ReprCrossEnc/d=ent_link/joint_train/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_w_0.5_bi_cross_loss_from_scratch/score_mats_model-3-19679.0--77.23.ckpt \
			--bi_model_file ../../results/6_ReprCrossEnc/d=ent_link/joint_train/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds_w_0.5_bi_cross_loss_from_scratch/model/model-3-19679.0--77.23.ckpt \
			--n_seeds 10 \
			--n_ment 3374 \
			--misc w_exact_cost_bienc_self_rtvr

				

	Cross-encoder w/ tfidf hard negs
		sbatch -p rtx800

		sbatch -p rtx8000-long --gres gpu:2 --mem 256GB --job-name cls_tfidf bin/run.sh  \
		python models/train.py \
		--config config/el_zeshel_cross_enc.json \
		--exp_id 6_ReprCrossEnc \
		--loss ce \
		--pooling_type cls_w_lin \
		--neg_strategy tfidf_hard_negs \
		--num_negs 63 \
		--train_batch_size 4 \
		--reload_dataloaders_every_n_epochs 0 \
		--eval_batch_size 32 \
		--ckpt_metric mrr \
		--warmup_proportion 0.01 \
		--strategy ddp \
		--num_gpus 2 \
		--eval_interval 0.2 \
		--num_epochs 4 \
		--misc 63_negs_w_cls_w_lin



	Debug CUR

		python eval/run_retrieval_eval_wrt_exact_crossenc.py \
		--data pro_wrestling \
		--res_dir ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt \
		--bi_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
		--n_seeds 1 \
		--n_ment 1392 \
		--misc debug_w_svd_inv \
		--disable_wandb 1

		python eval/run_retrieval_eval_wrt_exact_crossenc.py \
		--data pro_wrestling \
		--res_dir ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt \
		--bi_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
		--n_seeds 1 \
		--n_ment 1392 \
		--misc debug \
		--disable_wandb 1



6 June 

	Launching ment-ent matrix computation for military and star_trek for e-crossenc model (6-256)
		for mstart in {0..2500..50}
		for mstart in {2330..2490..10}
		do
		#sbatch -p gpu --gres gpu:1 --mem 32GB --job-name mat1-ce-$data --exclude gpu-0-0 bin/run.sh \
		sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name mat1-ce-mil-$mstart  bin/run.sh  \
		python eval/run_cross_encoder_for_ment_ent_matrix_zeshel.py \
		--n_ment_start $mstart \
		--n_ment 10 \
		--n_ent -1 \
		--batch_size 300 \
		--data_name military \
		--layers final \
		--cross_model_ckpt ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/model/model-2-15999.0--79.46.ckpt \
		--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt \
		--misc mstart_${mstart}
		done

		for mstart in {0..2500..50}
		for mstart in {3400..4200..50}
		do
		#sbatch -p gpu --gres gpu:1 --mem 32GB --job-name mat1-ce-$data --exclude gpu-0-0 bin/run.sh \
		sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name mat1-ce-st-$mstart  bin/run.sh  \
		python eval/run_cross_encoder_for_ment_ent_matrix_zeshel.py \
		--n_ment_start $mstart \
		--n_ment 50 \
		--n_ent -1 \
		--batch_size 300 \
		--data_name star_trek \
		--layers final \
		--cross_model_ckpt ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/model/model-2-15999.0--79.46.ckpt \
		--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt \
		--misc mstart_${mstart}
		done

		sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name mat1-ce-st-4200  bin/run.sh  \
		python eval/run_cross_encoder_for_ment_ent_matrix_zeshel.py \
		--n_ment_start 4200 \
		--n_ment 27 \
		--n_ent -1 \
		--batch_size 300 \
		--data_name star_trek \
		--layers final \
		--cross_model_ckpt ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/model/model-2-15999.0--79.46.ckpt \
		--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt \
		--misc mstart_4200


7 June 2022

	Training cross-encoder on NQ dataset
		Debug
		python models/train.py \
		--config config/nq_cross_enc_debug.json \
		--exp_id _0_Debug \
		--loss ce \
		--pooling_type cls_w_lin \
		--neg_strategy precomp \
		--num_negs 5 \
		--use_top_negs 0 \
		--train_batch_size 4 \
		--reload_dataloaders_every_n_epochs 0 \
		--eval_batch_size 32 \
		--ckpt_metric mrr \
		--warmup_proportion 0.01 \
		--eval_interval 1.0 \
		--num_epochs 4 \
		--misc debug_2




		sbatch -p rtx8000-long --gres gpu:2 --mem 128GB --job-name cls_nq_top_negs_4_40 bin/run.sh  \
		python models/train.py \
		--config config/nq_cross_enc.json \
		--exp_id 8_CUR_EMNLP \
		--loss ce \
		--pooling_type cls_w_lin \
		--neg_strategy precomp \
		--num_negs 4 \
		--use_top_negs 1 \
		--train_batch_size 64 \
		--reload_dataloaders_every_n_epochs 0 \
		--eval_batch_size 128 \
		--warmup_proportion 0.0001 \
		--eval_interval 0.5 \
		--num_epochs 40 \
		--strategy ddp \
		--num_gpus 2 \
		--misc cls_w_lin_w_4_top_negs_w_40_epochs


		sbatch -p rtx8000-long --gres gpu:2 --mem 128GB --job-name cls_nq_top_rng_negs_4_40 bin/run.sh  \
		python models/train.py \
		--config config/nq_cross_enc.json \
		--exp_id 8_CUR_EMNLP \
		--loss ce \
		--pooling_type cls_w_lin \
		--neg_strategy precomp \
		--num_negs 4 \
		--use_top_negs 0 \
		--train_batch_size 64 \
		--reload_dataloaders_every_n_epochs 0 \
		--eval_batch_size 128 \
		--warmup_proportion 0.0001 \
		--eval_interval 0.5 \
		--num_epochs 40 \
		--strategy ddp \
		--num_gpus 2 \
		--misc cls_w_lin_w_4_sampled_hard_negs_w_40_epochs


		sbatch -p rtx8000-long --gres gpu:2 --mem 128GB --job-name ece_nq_top_negs_4_40 bin/run.sh  \
		python models/train.py \
		--config config/nq_cross_enc.json \
		--exp_id 8_CUR_EMNLP \
		--loss ce \
		--pooling_type cls_w_lin \
		--cross_enc_type w_embeds \
		--neg_strategy precomp \
		--num_negs 4 \
		--use_top_negs 1 \
		--train_batch_size 64 \
		--reload_dataloaders_every_n_epochs 0 \
		--eval_batch_size 128 \
		--warmup_proportion 0.0001 \
		--eval_interval 0.5 \
		--num_epochs 40 \
		--strategy ddp \
		--num_gpus 2 \
		--misc crossenc_w_embeds_w_4_top_negs_w_40_epochs


	CUR on military dataset for e-crossenc 256
		# sbatch -p gpu --gres gpu:1 --mem 32GB --job-name cur_mil_256 --exclude gpu-0-0 bin/run.sh  \
		sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name cur_mil_256_10 bin/run.sh  \
		python eval/run_retrieval_eval_wrt_exact_crossenc.py \
		--data military \
		--res_dir ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt \
		--bi_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
		--n_seeds 10 \
		--n_ment 1000 


		# sbatch -p gpu --gres gpu:1 --mem 32GB --job-name cur_mil_256 --exclude gpu-0-0 bin/run.sh  \
		sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name cur_mil_256_1 bin/run.sh  \
		python eval/run_retrieval_eval_wrt_exact_crossenc.py \
		--data military \
		--res_dir ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt \
		--bi_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
		--n_seeds 1 \
		--n_ment 1000 


		sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name cur_star_trek_256_10 bin/run.sh  \
		python eval/run_retrieval_eval_wrt_exact_crossenc.py \
		--data star_trek \
		--res_dir ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt \
		--bi_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
		--n_seeds 10 \
		--n_ment 2500 


		# sbatch -p gpu --gres gpu:1 --mem 32GB --job-name cur_star_trek_256 --exclude gpu-0-0 bin/run.sh  \
		sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name cur_star_trek_256_1 bin/run.sh  \
		python eval/run_retrieval_eval_wrt_exact_crossenc.py \
		--data star_trek \
		--res_dir ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt \
		--bi_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
		--n_seeds 1 \
		--n_ment 2500 


8 June 2022


	Training for distillation exps for CUR paper

		python models/train.py \
		--config config/ce_distill/zeshel_bi_enc_distill_debug.json \
		--exp_id _0_Debug \
		--debug_w_small_data 0 \
		--neg_strategy top_ce_match \
		--distill_n_labels 4 \
		--num_negs 7 \
		--train_batch_size 4 \
		--grad_acc_steps 1 \
		--reload_dataloaders_every_n_epochs 0 \
		--eval_batch_size 32 \
		--warmup_proportion 0.01 \
		--num_epochs 20 \
		--eval_interval 1.0 \
		--misc debug_distill


		Debug
		python models/train.py \
		--config config/ce_distill/zeshel_bi_enc_distill.json \
		--exp_id _0_Debug \
		--res_dir_prefix yugioh/nm_train=500/split_idx=0 \
		--neg_strategy top_ce_w_bienc_hard_negs_trp \
		--path_to_model ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
		--distill_n_labels 10 \
		--train_batch_size 5 \
		--train_ent_w_score_file_template ../../results/8_CUR_EMNLP/d=ent_link_ce/6_256_e_crossenc/score_mats_model-2-15999.0--79.46.ckpt/{}/m2e_splits/nm_train=500/split_idx=0/train_train.pkl \
		--dev_ent_w_score_file_template   ../../results/8_CUR_EMNLP/d=ent_link_ce/6_256_e_crossenc/score_mats_model-2-15999.0--79.46.ckpt/{}/m2e_splits/nm_train=500/split_idx=0/train_dev.pkl \
		--reload_dataloaders_every_n_epochs 1 \
		--eval_batch_size 20 \
		--num_epochs 10 \
		--eval_interval 0.2 \
		--misc debug



		sbatch -p 2080ti-long --gres gpu:2 --mem 64GB --job-name bienc_distill1 bin/run.sh  \
		python models/train.py \
		--config config/ce_distill/zeshel_bi_enc_distill.json \
		--exp_id _0_Debug \
		--res_dir_prefix yugioh/nm_train=500/split_idx=0 \
		--neg_strategy top_ce_w_bienc_hard_negs_trp \
		--distill_n_labels 100 \
		--train_batch_size 32 \
		--train_ent_w_score_file_template ../../results/8_CUR_EMNLP/d=ent_link_ce/6_256_e_crossenc/score_mats_model-2-15999.0--79.46.ckpt/{}/m2e_splits/nm_train=500/split_idx=0/train_train.pkl \
		--dev_ent_w_score_file_template   ../../results/8_CUR_EMNLP/d=ent_link_ce/6_256_e_crossenc/score_mats_model-2-15999.0--79.46.ckpt/{}/m2e_splits/nm_train=500/split_idx=0/train_dev.pkl \
		--reload_dataloaders_every_n_epochs 1 \
		--eval_batch_size 256 \
		--num_epochs 20 \
		--eval_interval 0.2 \
		--strategy ddp \
		--num_gpus 2 \
		--misc top_100

		sbatch -p 2080ti-long --gres gpu:2 --mem 64GB --job-name bienc_distill50_4_16 bin/run.sh  \
		python models/train.py \
		--config config/ce_distill/zeshel_bi_enc_distill.json \
		--exp_id _0_Debug \
		--res_dir_prefix yugioh/nm_train=500/split_idx=0 \
		--neg_strategy top_ce_match \
		--distill_n_labels 25 \
		--train_batch_size 4 \
		--grad_acc_steps 4 \
		--train_ent_w_score_file_template ../../results/8_CUR_EMNLP/d=ent_link_ce/6_256_e_crossenc/score_mats_model-2-15999.0--79.46.ckpt/{}/m2e_splits/nm_train=500/split_idx=0/train_train.pkl \
		--dev_ent_w_score_file_template   ../../results/8_CUR_EMNLP/d=ent_link_ce/6_256_e_crossenc/score_mats_model-2-15999.0--79.46.ckpt/{}/m2e_splits/nm_train=500/split_idx=0/train_dev.pkl \
		--reload_dataloaders_every_n_epochs 0 \
		--eval_batch_size 16 \
		--warmup_proportion 0.00 \
		--num_epochs 20 \
		--eval_interval 0.2 \
		--strategy ddp \
		--num_gpus 2 \
		--misc top_25


12 June 2022


	Some commands for launching eval jobs

		python utils/launch_bienc_distill_jobs.py cur 0 0 0
		python utils/launch_bienc_distill_jobs.py cur 0 0 1

		python utils/launch_bienc_distill_jobs.py bienc_6_20 0 0 0

		python utils/launch_bienc_distill_jobs.py bienc~eoe 0 0 0
		python utils/launch_bienc_distill_jobs.py bienc~eoe 0 0 1

		python utils/launch_bienc_distill_jobs.py bienc~best_wrt_dev 0 0 0
		python utils/launch_bienc_distill_jobs.py bienc~best_wrt_dev 0 0 1



		python utils/launch_bienc_distill_jobs.py cur 1 0 0
		python utils/launch_bienc_distill_jobs.py cur 1 0 1

		python utils/launch_bienc_distill_jobs.py bienc_6_20 1 0 0

		python utils/launch_bienc_distill_jobs.py bienc~eoe 1 0 0
		python utils/launch_bienc_distill_jobs.py bienc~eoe 1 0 1

		python utils/launch_bienc_distill_jobs.py bienc~best_wrt_dev 1 0 0
		python utils/launch_bienc_distill_jobs.py bienc~best_wrt_dev 1 0 1


	Debugging using only score against a fixed set of entities for eval


		sbatch -p gpu --gres gpu:1 --mem 32GB --job-name anc_ent_pro --exclude gpu-0-0 bin/run.sh  \
		python eval/run_retrieval_eval_wrt_exact_crossenc.py \
		--data pro_wrestling \
		--bi_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
		--res_dir ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt \
		--n_seeds 2 \
		--n_ment 1392 \
		--misc w_anchor_ent_eval


		sbatch -p gpu --gres gpu:1 --mem 32GB --job-name anc_ent_pro --exclude gpu-0-0 bin/run.sh  \
		python eval/run_retrieval_eval_wrt_exact_crossenc.py \
		--data lego \
		--bi_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
		--res_dir ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt \
		--n_seeds 2 \
		--n_ment 1199 \
		--misc w_anchor_ent_eval




		python eval/run_retrieval_eval_wrt_exact_crossenc.py \
		--data pro_wrestling \
		--res_dir ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt \
		--n_seeds 1 \
		--n_ment 1392 \
		--misc debug_anchor_ent_only \
		--disable_wandb 1


	Launching e2e jobs for CUR_w_Anchor_Ent2Ent scores 

		Things to double check
		1. n_ent_start should match in misc and arg
		2. step size should be appropriate
		3. Check loop params
		4. final x_start = prev_end + step_size

		# Chunking computation for domain=yugioh, Total = 10031
			for xstart in {0..8000..2000}
			do
			sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name e2e-yugioh-${xstart} bin/run.sh  \
			python eval/run_cross_encoder_for_ent_ent_matrix.py \
			--data_name yugioh \
			--n_ent_x_start ${xstart} \
			--n_ent_x  2000 \
			--n_ent_y -1 \
			--topk 1000 \
			--embed_type none \
			--token_opt m2e \
			--batch_size 150 \
			--topk_ents_file   ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt/yugioh/anchor_ents.json \
			--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt \
			--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/model/model-2-15999.0--79.46.ckpt \
			--misc x_start_${xstart}
			done

			sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name e2e-yugioh-final bin/run.sh  \
			python eval/run_cross_encoder_for_ent_ent_matrix.py \
			--data_name yugioh \
			--n_ent_x_start 10000 \
			--n_ent_x  31 \
			--n_ent_y -1 \
			--topk 1000 \
			--embed_type none \
			--token_opt m2e \
			--batch_size 150 \
			--topk_ents_file   ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt/yugioh/anchor_ents.json \
			--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt \
			--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/model/model-2-15999.0--79.46.ckpt \
			--misc x_start_10000

		# Chunking computation for domain=star_trek, Total = 34430
			for xstart in {0..32000..2000}
			do
			sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name e2e-star_trek-${xstart} bin/run.sh  \
			python eval/run_cross_encoder_for_ent_ent_matrix.py \
			--data_name star_trek \
			--n_ent_x_start ${xstart} \
			--n_ent_x  2000 \
			--n_ent_y -1 \
			--topk 1000 \
			--embed_type none \
			--token_opt m2e \
			--batch_size 150 \
			--topk_ents_file   ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt/star_trek/anchor_ents.json \
			--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt \
			--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/model/model-2-15999.0--79.46.ckpt \
			--misc x_start_${xstart}

			sleep 1
			done

			sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name e2e-star_trek-final bin/run.sh  \
			python eval/run_cross_encoder_for_ent_ent_matrix.py \
			--data_name star_trek \
			--n_ent_x_start 34000 \
			--n_ent_x  430 \
			--n_ent_y -1 \
			--topk 1000 \
			--embed_type none \
			--token_opt m2e \
			--batch_size 150 \
			--topk_ents_file   ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt/star_trek/anchor_ents.json \
			--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt \
			--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/model/model-2-15999.0--79.46.ckpt \
			--misc x_start_34000

		# Chunking computation for domain=military, Total = 104520
			# for xstart in {0..104000..2000}

			for xstart in {0..18000..2000}
			for xstart in {20000..38000..2000}
			for xstart in {40000..58000..2000}
			for xstart in {60000..78000..2000}
			for xstart in {80000..102000..2000}
			do
			sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name e2e-military-${xstart} bin/run.sh  \
			python eval/run_cross_encoder_for_ent_ent_matrix.py \
			--data_name military \
			--n_ent_x_start ${xstart} \
			--n_ent_x  2000 \
			--n_ent_y -1 \
			--topk 1000 \
			--embed_type none \
			--token_opt m2e \
			--batch_size 150 \
			--topk_ents_file   ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt/military/anchor_ents.json \
			--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt \
			--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/model/model-2-15999.0--79.46.ckpt \
			--misc x_start_${xstart}
			done

			sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name e2e-military-final bin/run.sh  \
			python eval/run_cross_encoder_for_ent_ent_matrix.py \
			--data_name military \
			--n_ent_x_start 104000 \
			--n_ent_x  520 \
			--n_ent_y -1 \
			--topk 1000 \
			--embed_type none \
			--token_opt m2e \
			--batch_size 150 \
			--topk_ents_file   ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt/military/anchor_ents.json \
			--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt \
			--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/model/model-2-15999.0--79.46.ckpt \
			--misc x_start_104000

		# Chunking computation for domain=pro_wrestling, Total = 10133
			for xstart in {0..8000..2000}
			do
			sbatch -p 1080ti-long --gres gpu:1 --mem 32GB --job-name e2e-pro_wrestling-${xstart} bin/run.sh  \
			python eval/run_cross_encoder_for_ent_ent_matrix.py \
			--data_name pro_wrestling \
			--n_ent_x_start ${xstart} \
			--n_ent_x  2000 \
			--n_ent_y -1 \
			--topk 1000 \
			--embed_type none \
			--token_opt m2e \
			--batch_size 150 \
			--topk_ents_file   ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt/pro_wrestling/anchor_ents.json \
			--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt \
			--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/model/model-2-15999.0--79.46.ckpt \
			--misc x_start_${xstart}
			done

			sbatch -p 1080ti-long --gres gpu:1 --mem 32GB --job-name e2e-pro_wrestling-final bin/run.sh  \
			python eval/run_cross_encoder_for_ent_ent_matrix.py \
			--data_name pro_wrestling \
			--n_ent_x_start 10000 \
			--n_ent_x  133 \
			--n_ent_y -1 \
			--topk 1000 \
			--embed_type none \
			--token_opt m2e \
			--batch_size 150 \
			--topk_ents_file   ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt/pro_wrestling/anchor_ents.json \
			--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt \
			--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/model/model-2-15999.0--79.46.ckpt \
			--misc x_start_10000

		# Chunking computation for domain=doctor_who, Total = 40281


			for xstart in {0..18000..2000}
			for xstart in {20000..38000..2000}
			do
			sbatch -p 1080ti-long --gres gpu:1 --mem 32GB --job-name e2e-doctor_who-${xstart} bin/run.sh  \
			python eval/run_cross_encoder_for_ent_ent_matrix.py \
			--data_name doctor_who \
			--n_ent_x_start ${xstart} \
			--n_ent_x  2000 \
			--n_ent_y -1 \
			--topk 1000 \
			--embed_type none \
			--token_opt m2e \
			--batch_size 150 \
			--topk_ents_file   ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt/doctor_who/anchor_ents.json \
			--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt \
			--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/model/model-2-15999.0--79.46.ckpt \
			--misc x_start_${xstart}
			done

			sbatch -p 1080ti-long --gres gpu:1 --mem 32GB --job-name e2e-doctor_who-final bin/run.sh  \
			python eval/run_cross_encoder_for_ent_ent_matrix.py \
			--data_name doctor_who \
			--n_ent_x_start 40000 \
			--n_ent_x  281 \
			--n_ent_y -1 \
			--topk 1000 \
			--embed_type none \
			--token_opt m2e \
			--batch_size 150 \
			--topk_ents_file   ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt/doctor_who/anchor_ents.json \
			--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt \
			--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/model/model-2-15999.0--79.46.ckpt \
			--misc x_start_40000


		# First create top-k file by randomly sampling entities idxs from correct range
		# Make sure the output file is a json with 1000 as key in that file, maybe also create file w/ 100 and 10 as key in that file.
		# Then launch these exps online and set Zeshel_Ent2Ent wandb paramters by filtering by appropriate timestamp

		# Debug
			python eval/run_cross_encoder_for_ent_ent_matrix.py \
			--data_name pro_wrestling \
			--n_ent_x_start 3 \
			--n_ent_x  4 \
			--n_ent_y -1 \
			--topk 101 \
			--embed_type none \
			--token_opt m2e \
			--batch_size 150 \
			--topk_ents_file   ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt/pro_wrestling/anchor_ents.json \
			--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt \
			--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/model/model-2-15999.0--79.46.ckpt \
			--misc debug_x_3 \
			--disable_wandb 1 




13 June 2022

	Training e-crossenc and cls-crossenc on random negatives and TFIDF hard negatives

		sbatch -p rtx8000-long --gres gpu:2 --mem 32GB --job-name ece-random bin/run.sh  \
		python models/train.py \
		--config config/el_zeshel_cross_enc.json \
		--exp_id 8_CUR_EMNLP \
		--cross_enc_type w_embeds \
		--loss ce \
		--neg_strategy random \
		--num_negs 63 \
		--train_batch_size 4 \
		--reload_dataloaders_every_n_epochs 1 \
		--eval_batch_size 32 \
		--ckpt_metric mrr \
		--warmup_proportion 0.01 \
		--strategy ddp \
		--num_gpus 2 \
		--eval_interval 0.2 \
		--misc 63_negs_w_crossenc_w_embeds


		sbatch -p rtx8000-long --gres gpu:2 --mem 32GB --job-name cls-random bin/run.sh  \
		python models/train.py \
		--config config/el_zeshel_cross_enc.json \
		--exp_id 8_CUR_EMNLP \
		--loss ce \
		--pooling_type cls_w_lin \
		--neg_strategy random \
		--num_negs 63 \
		--train_batch_size 4 \
		--reload_dataloaders_every_n_epochs 1 \
		--eval_batch_size 32 \
		--ckpt_metric mrr \
		--warmup_proportion 0.01 \
		--strategy ddp \
		--num_gpus 2 \
		--eval_interval 0.2 \
		--num_epochs 4 \
		--misc 63_negs_w_cls_w_lin

	TFIDF Hard negatives - Make sure precomp negs = num_negs + 1 -- just to get around an assert statement in my code
		sbatch -p rtx8000-long --gres gpu:2 --mem 32GB --job-name ece-tfidf bin/run.sh  \
		python models/train.py \
		--config config/el_zeshel_cross_enc.json \
		--exp_id 8_CUR_EMNLP \
		--cross_enc_type w_embeds \
		--loss ce \
		--neg_strategy precomp \
		--ent_w_score_file_template ../../results/8_CUR_EMNLP/d=ent_link_ce/precomputed_tfidf_negs/{}/tfidf_hard_negs_n=64.json \
		--num_negs 63 \
		--train_batch_size 4 \
		--reload_dataloaders_every_n_epochs 0 \
		--eval_batch_size 32 \
		--ckpt_metric mrr \
		--warmup_proportion 0.01 \
		--strategy ddp \
		--num_gpus 2 \
		--eval_interval 0.2 \
		--misc 63_negs_w_crossenc_w_embeds_tfidf_hard_negs


		sbatch -p rtx8000-long --gres gpu:2 --mem 32GB --job-name cls-tfidf bin/run.sh  \
		python models/train.py \
		--config config/el_zeshel_cross_enc.json \
		--exp_id 8_CUR_EMNLP \
		--loss ce \
		--pooling_type cls_w_lin \
		--neg_strategy precomp \
		--ent_w_score_file_template ../../results/8_CUR_EMNLP/d=ent_link_ce/precomputed_tfidf_negs/{}/tfidf_hard_negs_n=64.json \
		--num_negs 63 \
		--train_batch_size 4 \
		--reload_dataloaders_every_n_epochs 0 \
		--eval_batch_size 32 \
		--ckpt_metric mrr \
		--warmup_proportion 0.01 \
		--strategy ddp \
		--num_gpus 2 \
		--eval_interval 0.2 \
		--num_epochs 4 \
		--misc 63_negs_w_cls_w_lin_tfidf_hard_negs

	Computing TF-IDF Hard negs
		for data in american_football doctor_who fallout final_fantasy military pro_wrestling starwars world_of_warcraft coronation_street elder_scrolls ice_hockey muppets forgotten_realms lego star_trek yugioh  world_of_warcraft coronation_street elder_scrolls ice_hockey muppets forgotten_realms lego star_trek yugioh 
		for data in military 
		do
		sbatch -p 2080ti-short --mem 256GB --job-name tfidf-$data bin/run.sh python eval/compute_tfidf_hard_negs.py --data_name $data --num_negs 64
		done

	Run ment2ent score computation for yugioh for e-crossenc and cls-crossenc models

		For 6-49 cls-crossenc model (6-405)
			for mstart in {0..3200..100}
			for mstart in {0..1300..100}
			for mstart in {2100..3200..100}
			for mstart in {0..1300..100}
			do
			# sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name mat1-ce-yugioh-$mstart  bin/run.sh  \
			sbatch -p gpu --gres gpu:1 --mem 32GB --job-name mat2-ce-yugioh-$mstart   bin/run.sh  \
			python eval/run_cross_encoder_for_ment_ent_matrix_zeshel.py \
			--n_ment_start $mstart \
			--n_ment 100 \
			--n_ent -1 \
			--batch_size 300 \
			--data_name yugioh \
			--layers final \
			--cross_model_ckpt ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_4_epochs_reproduce_6_49/model/model-1-12279.0--80.14.ckpt \
			--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_4_epochs_reproduce_6_49/score_mats_model-1-12279.0--80.14.ckpt \
			--misc mstart_${mstart}
			done

			sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name mat1-ce-yugioh-3350  bin/run.sh  \
			python eval/run_cross_encoder_for_ment_ent_matrix_zeshel.py \
			--n_ment_start  3300 \
			--n_ment 74 \
			--n_ent -1 \
			--batch_size 300 \
			--data_name yugioh \
			--layers final \
			--cross_model_ckpt ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_4_epochs_reproduce_6_49/model/model-1-12279.0--80.14.ckpt \
			--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_4_epochs_reproduce_6_49/score_mats_model-1-12279.0--80.14.ckpt \
			--misc mstart_3350

		For random neg models - e-crossenc

			for mstart in {1700..3200..100}
			for mstart in {0..1600..100}
			for mstart in {1700..3200..100}
			do
			sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name mat1-ce-yugioh-$mstart  bin/run.sh  \
			python eval/run_cross_encoder_for_ment_ent_matrix_zeshel.py \
			--n_ment_start $mstart \
			--n_ment 100 \
			--n_ent -1 \
			--batch_size 300 \
			--data_name yugioh \
			--layers final \
			--cross_model_ckpt ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=random_s=1234_63_negs_w_crossenc_w_embeds/model/model-3-23399.0--98.07.ckpt  \
			--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=random_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-3-23399.0--98.07.ckpt  \
			--misc mstart_${mstart}
			done

			sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name mat1-ce-yugioh-3350  bin/run.sh  \
			python eval/run_cross_encoder_for_ment_ent_matrix_zeshel.py \
			--n_ment_start  3300 \
			--n_ment 74 \
			--n_ent -1 \
			--batch_size 300 \
			--data_name yugioh \
			--layers final \
			--cross_model_ckpt ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=random_s=1234_63_negs_w_crossenc_w_embeds/model/model-3-23399.0--98.07.ckpt  \
			--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=random_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-3-23399.0--98.07.ckpt  \
			--misc mstart_3350

		For random neg models - cls-crossenc

			for mstart in {0..3200..100}


			for mstart in {0..1600..100}
			for mstart in {1700..3200..100}
			do
			sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name mat2-ce-yugioh-$mstart  bin/run.sh  \
			python eval/run_cross_encoder_for_ment_ent_matrix_zeshel.py \
			--n_ment_start $mstart \
			--n_ment 100 \
			--n_ent -1 \
			--batch_size 300 \
			--data_name yugioh \
			--layers final \
			--cross_model_ckpt ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=random_s=1234_63_negs_w_cls_w_lin/model/model-3-23399.0--98.18.ckpt   \
			--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=random_s=1234_63_negs_w_cls_w_lin/score_mats_model-3-23399.0--98.18.ckpt   \
			--misc mstart_${mstart}
			done

			sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name mat2-ce-yugioh-3350  bin/run.sh  \
			python eval/run_cross_encoder_for_ment_ent_matrix_zeshel.py \
			--n_ment_start  3300 \
			--n_ment 74 \
			--n_ent -1 \
			--batch_size 300 \
			--data_name yugioh \
			--layers final \
			--cross_model_ckpt ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=random_s=1234_63_negs_w_cls_w_lin/model/model-3-23399.0--98.18.ckpt   \
			--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=random_s=1234_63_negs_w_cls_w_lin/score_mats_model-3-23399.0--98.18.ckpt   \
			--misc mstart_3350

		For tfidf negs models - E-CrossEnc


			for mstart in {0..1600..100}
			for mstart in {1700..3200..100}
			for mstart in {0..1600..100}
			for mstart in {1700..3200..100}
			do
			sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name mat2-ce-yugioh-$mstart  bin/run.sh  \
			python eval/run_cross_encoder_for_ment_ent_matrix_zeshel.py \
			--n_ment_start $mstart \
			--n_ment 100 \
			--n_ent -1 \
			--batch_size 300 \
			--data_name yugioh \
			--layers final \
			--cross_model_ckpt ../../results/8_CUR_EMNLP/d=ent_link/m=cross_enc_l=ce_neg=precomp_s=1234_63_negs_w_cls_w_lin_tfidf_hard_negs/model/model-1-12279.0--90.95.ckpt \
			--res_dir          ../../results/8_CUR_EMNLP/d=ent_link/m=cross_enc_l=ce_neg=precomp_s=1234_63_negs_w_cls_w_lin_tfidf_hard_negs/score_mats_model-1-12279.0--90.95.ckpt \
			--misc mstart_${mstart}
			done

			sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name mat2-ce-yugioh-3300  bin/run.sh  \
			python eval/run_cross_encoder_for_ment_ent_matrix_zeshel.py \
			--n_ment_start  3300 \
			--n_ment 74 \
			--n_ent -1 \
			--batch_size 300 \
			--data_name yugioh \
			--layers final \
			--cross_model_ckpt ../../results/8_CUR_EMNLP/d=ent_link/m=cross_enc_l=ce_neg=precomp_s=1234_63_negs_w_cls_w_lin_tfidf_hard_negs/model/model-1-12279.0--90.95.ckpt \
			--res_dir          ../../results/8_CUR_EMNLP/d=ent_link/m=cross_enc_l=ce_neg=precomp_s=1234_63_negs_w_cls_w_lin_tfidf_hard_negs/score_mats_model-1-12279.0--90.95.ckpt \
			--misc mstart_3300

		For tfidf negs model - cls-crossenc


			for mstart in {0..1600..100}

			for mstart in {0..1600..100}
			for mstart in {1700..3200..100}
			do
			sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name mat4-ce-yugioh-$mstart  bin/run.sh  \
			python eval/run_cross_encoder_for_ment_ent_matrix_zeshel.py \
			--n_ment_start $mstart \
			--n_ment 100 \
			--n_ent -1 \
			--batch_size 300 \
			--data_name yugioh \
			--layers final \
			--cross_model_ckpt ../../results/8_CUR_EMNLP/d=ent_link/m=cross_enc_l=ce_neg=precomp_s=1234_63_negs_w_crossenc_w_embeds_tfidf_hard_negs/model/model-1-11079.0--91.17.ckpt \
			--res_dir          ../../results/8_CUR_EMNLP/d=ent_link/m=cross_enc_l=ce_neg=precomp_s=1234_63_negs_w_crossenc_w_embeds_tfidf_hard_negs/score_mats_model-1-11079.0--91.17.ckpt \
			--misc mstart_${mstart}
			done

			sbatch -p 2080ti-long --gres gpu:1 --mem 32GB --job-name mat4-ce-yugioh-3300  bin/run.sh  \
			python eval/run_cross_encoder_for_ment_ent_matrix_zeshel.py \
			--n_ment_start  3300 \
			--n_ment 74 \
			--n_ent -1 \
			--batch_size 300 \
			--data_name yugioh \
			--layers final \
			--cross_model_ckpt ../../results/8_CUR_EMNLP/d=ent_link/m=cross_enc_l=ce_neg=precomp_s=1234_63_negs_w_crossenc_w_embeds_tfidf_hard_negs/model/model-1-11079.0--91.17.ckpt \
			--res_dir          ../../results/8_CUR_EMNLP/d=ent_link/m=cross_enc_l=ce_neg=precomp_s=1234_63_negs_w_crossenc_w_embeds_tfidf_hard_negs/score_mats_model-1-11079.0--91.17.ckpt \
			--misc mstart_3300



14 June

	Trying idea from Archan paper for the case when we have a square U matrix in CUR
		python eval/run_retrieval_eval_wrt_exact_crossenc.py \
		--data pro_wrestling \
		--res_dir ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt \
		--bi_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt \
		--n_seeds 1 \
		--n_ment 1392 \
		--misc eigen_val_idea \
		--disable_wandb 1





Commands for plotting


	python eval/run_retrieval_eval_wrt_exact_crossenc.py \
	--data yugioh \
	--res_dir       ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt \
	--n_seeds 1 \
	--n_ment 3374 \
	--disable_wandb 1 \
	--misc for_approx_error

	python eval/run_retrieval_eval_wrt_exact_crossenc.py \
	--data yugioh \
	--res_dir       ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_4_epochs_reproduce_6_49/score_mats_model-1-12279.0--80.14.ckpt \
	--n_seeds 1 \
	--n_ment 3374 \
	--disable_wandb 1 \
	--misc for_approx_error


	python eval/run_retrieval_eval_wrt_exact_crossenc.py \
	--data yugioh \
	--res_dir       ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt \
	--n_seeds 1 \
	--n_ment 3374 \
	--disable_wandb 1 \
	--misc for_plots


	python eval/run_retrieval_eval_wrt_exact_crossenc.py \
	--data yugioh \
	--res_dir       ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_4_epochs_reproduce_6_49/score_mats_model-1-12279.0--80.14.ckpt \
	--n_seeds 1 \
	--n_ment 3374 \
	--disable_wandb 1 \
	--misc for_plots

	python  eval/run_emnlp_retrieval_eval_wrt_exact_crossenc.py  \
	--data_name pro_wrestling  \
	--eval_method graph  \
	--res_dir ../../results/8_CUR_EMNLP/d=ent_link_ce/models/pro_wrestling/nm_train=1000/split_idx=0/graph_search../../results/8_CUR_EMNLP/d=ent_link_ce/models/pro_wrestling/nm_train=1000/split_idx=0/graph_search/emb=bienc_nbr=5_b=1_entry=bienc  \
	--test_data_file ../../results/8_CUR_EMNLP/d=ent_link_ce/6_256_e_crossenc/score_mats_model-2-15999.0--79.46.ckpt/pro_wrestling/m2e_splits/nm_train=1000/split_idx=0/test.pkl  \
	--train_data_file ../../results/8_CUR_EMNLP/d=ent_link_ce/6_256_e_crossenc/score_mats_model-2-15999.0--79.46.ckpt/pro_wrestling/m2e_splits/nm_train=1000/split_idx=0/train.pkl  \
	--mention_file ../../data/zeshel/processed/train_worlds/pro_wrestling_mentions.jsonl  \
	--entity_file ../../data/zeshel/documents/pro_wrestling.json  \
	--bi_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
	--embed_type bienc  \
	--max_nbrs 5  \
	--beamsize 1  \
	--entry_method bienc  \
	--misc d=test_debug  \
	--use_wandb 0  \
	--mode eval 


	python  eval/run_emnlp_retrieval_eval_wrt_exact_crossenc.py  \
	--data_name yugioh  \
	--eval_method graph  \
	--res_dir ../../results/8_CUR_EMNLP/d=ent_link_ce/models/yugioh/nm_train=100/split_idx=0/graph/emb=bienc_nbr=5_b=1_entry=bienc  \
	--test_data_file ../../results/8_CUR_EMNLP/d=ent_link_ce/6_256_e_crossenc/score_mats_model-2-15999.0--79.46.ckpt/yugioh/m2e_splits/nm_train=100/split_idx=0/test.pkl  \
	--train_data_file ../../results/8_CUR_EMNLP/d=ent_link_ce/6_256_e_crossenc/score_mats_model-2-15999.0--79.46.ckpt/yugioh/m2e_splits/nm_train=100/split_idx=0/train.pkl  \
	--mention_file ../../data/zeshel/processed/test_worlds/yugioh_mentions.jsonl  \
	--entity_file ../../data/zeshel/documents/yugioh.json  \
	--bi_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
	--embed_type bienc  \
	--max_nbrs 5  \
	--beamsize 1  \
	--entry_method bienc  \
	--misc d=test  \
	--use_wandb 1  \
	--mode eval 
	\
	-- --




July
7 July 2022

	Using CUR for filtering entities and/or choosing entry points in graph for search
		python eval/nsw_eval_zeshel.py  \
		--project_name _0_Debug  \
		--data_name yugioh  \
		--n_ment 100  \
		--embed_type cur  \
		--entry_method cur  \
		--graph_type nsw  \
		--graph_metric l2  \
		--masking_type adapt_soft \
		--masked_node_frac 0.9  \
		--masked_node_method cur \
		--misc c=05_E-CrossEnc_6_256_b=00_HardNegs_6_20_n_m=100_mask_t=hard_method=cur_frac=0.9  \
		--bi_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
		--train_m2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt/yugioh/chunked_compute/ment_to_ent_scores_n_m_500_n_e_10031_all_layers_Falsemstart_500.pkl \
		--score_mat_dir ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt  \
		--res_dir ../../results/6_ReprCrossEnc/Graph_Search/temp_debug  \
		--force_exact_init_search 1 \
		--debug_mode 1
			
		# Domain = YuGiOh
		for mnf in $(seq 0.0 0.3 0.9)
		do
		for masking_type in adapt_soft
		do
		for masked_node_method in cur 
		do
		sbatch -p gpu --gres gpu:1 --mem 32GB --job-name gs1_${masking_type}_${masked_node_method}_${mnf} --exclude gpu-0-[0-1] bin/run.sh \
		python eval/nsw_eval_zeshel.py  \
		--project_name Graph_Search  \
		--data_name yugioh  \
		--n_ment 100  \
		--embed_type bienc  \
		--entry_method bienc  \
		--graph_type nsw  \
		--graph_metric l2  \
		--masking_type ${masking_type} \
		--masked_node_method ${masked_node_method} \
		--masked_node_frac ${mnf}  \
		--misc c=05_E-CrossEnc_6_256_b=00_HardNegs_6_20_n_m=100_mask_type=${masking_type}_method=${masked_node_method}_local_frac=${mnf}  \
		--bi_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
		--train_m2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt/yugioh/chunked_compute/ment_to_ent_scores_n_m_500_n_e_10031_all_layers_Falsemstart_500.pkl \
		--score_mat_dir ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt  \
		--res_dir ../../results/6_ReprCrossEnc/Graph_Search/yugioh/node_masking  \
		--force_exact_init_search 1 
		done
		done
		done

		# Domain = Pro Wrestling
		for mnf in $(seq 0.0 0.3 0.9)
		do
		for masking_type in adapt_soft
		do
		for masked_node_method in cur 
		do
		sbatch -p gpu --gres gpu:1 --mem 32GB --job-name gs2_${masking_type}_${masked_node_method}_${mnf} --exclude gpu-0-[0-1] bin/run.sh \
		python eval/nsw_eval_zeshel.py  \
		--project_name Graph_Search  \
		--data_name pro_wrestling  \
		--n_ment 100  \
		--embed_type bienc  \
		--entry_method bienc  \
		--graph_type nsw  \
		--graph_metric l2  \
		--masking_type ${masking_type} \
		--masked_node_method ${masked_node_method} \
		--masked_node_frac ${mnf}  \
		--misc c=05_E-CrossEnc_6_256_b=00_HardNegs_6_20_n_m=100_mask_type=${masking_type}_method=${masked_node_method}_frac=${mnf}  \
		--bi_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
		--score_mat_dir ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt  \
		--train_m2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt/pro_wrestling/ment_to_ent_scores_n_m_1392_n_e_10133_all_layers_False.pkl \
		--res_dir ../../results/6_ReprCrossEnc/Graph_Search/pro_wrestling/node_masking  \
		--force_exact_init_search 1 
		done
		done
		done

		# Domain = Lego
		for mnf in $(seq 0.0 0.3 0.9)
		do
		for masking_type in adapt_soft
		do
		for masked_node_method in cur 
		do
		sbatch -p gpu --gres gpu:1 --mem 32GB --job-name gs3_${masking_type}_${masked_node_method}_${mnf} --exclude gpu-0-[0-1] bin/run.sh \
		python eval/nsw_eval_zeshel.py  \
		--project_name Graph_Search  \
		--data_name lego  \
		--n_ment 100  \
		--embed_type bienc  \
		--entry_method bienc  \
		--graph_type nsw  \
		--graph_metric l2  \
		--masking_type ${masking_type} \
		--masked_node_method ${masked_node_method} \
		--masked_node_frac ${mnf}  \
		--misc c=05_E-CrossEnc_6_256_b=00_HardNegs_6_20_n_m=100_mask_type=${masking_type}_method=${masked_node_method}_frac=${mnf}  \
		--bi_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
		--score_mat_dir ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt  \
		--train_m2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt/lego/ment_to_ent_scores_n_m_1199_n_e_10076_all_layers_False.pkl \
		--res_dir ../../results/6_ReprCrossEnc/Graph_Search/lego/node_masking  \
		--force_exact_init_search 1 
		done
		done
		done


		# Domain = Star Trek
		for mnf in $(seq 0.0 0.3 0.9)
		do
		for masking_type in soft hard
		do
		for masked_node_method in cur 
		do
		sbatch -p gpu --gres gpu:1 --mem 32GB --job-name gs4_${masking_type}_${masked_node_method}_${mnf} --exclude gpu-0-[0-1] bin/run.sh \
		python eval/nsw_eval_zeshel.py  \
		--project_name Graph_Search  \
		--data_name star_trek  \
		--n_ment 2500  \
		--embed_type bienc  \
		--entry_method bienc  \
		--graph_type nsw  \
		--graph_metric l2  \
		--masking_type ${masking_type} \
		--masked_node_method ${masked_node_method} \
		--masked_node_frac ${mnf}  \
		--misc c=05_E-CrossEnc_6_256_b=00_HardNegs_6_20_n_m=100_mask_type=${masking_type}_method=${masked_node_method}_frac=${mnf}  \
		--bi_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
		--score_mat_dir ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt  \
		--train_m2e_score_filename ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt/star_trek/ment_to_ent_scores_n_m_4227_n_e_34430_all_layers_False.pkl \
		--res_dir ../../results/6_ReprCrossEnc/Graph_Search/star_trek/node_masking  \
		--force_exact_init_search 1 
		done
		done
		done


		python eval/run_retrieval_eval_wrt_exact_crossenc.py \
		--data yugioh \
		--res_dir  ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt \
		--n_seeds 1 \
		--n_ment 100 \
		--disable_wandb 1 \
		--misc debug_i_cur





20 July

export CUDA_VISIBLE_DEVICES=$1
python runner.py SiameseXML ../../.. test_debug_1 configs/SiameseXML/LF-AmazonTitles-131K.json 0


22 July 


Biased sampling of anchor entities using exact cross-encoder scores

	python eval/run_retrieval_eval_wrt_exact_crossenc.py \
	--data yugioh \
	--res_dir  ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt \
	--n_seeds 5 \
	--n_ment 100 \
	--disable_wandb 1 \
	--shortlist_method exact \
	--sampling_method exact_softmax_cumul \
	--i_cur_n_steps 1 \
	--misc debug_i_cur_uniform_anchor_split/


	for shortlist_method in approx exact
	do
	for sampling_method in random_cumul approx_cumul approx_softmax_cumul exact_cumul exact_softmax_cumul random_diff approx_diff approx_softmax_diff exact_diff exact_softmax_diff
	do
	for i_cur_n_steps in 1 2 5
	do
	# sbatch -p cpu --gres gpu:1 --mem 32GB --job-name icur_${shortlist_method}_${sampling_method}_${i_cur_n_steps} --exclude gpu-0-[0-1] bin/run.sh \
	sbatch -p cpu --mem 32GB --job-name icur_${shortlist_method}_${sampling_method}_${i_cur_n_steps} --exclude gpu-0-[0-1] bin/run.sh \
	python eval/run_retrieval_eval_wrt_exact_crossenc.py \
	--data yugioh \
	--res_dir  ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt \
	--n_seeds 5 \
	--n_ment 100 \
	--disable_wandb 0 \
	--shortlist_method ${shortlist_method} \
	--sampling_method ${sampling_method} \
	--i_cur_n_steps ${i_cur_n_steps} \
	--misc i_cur_uniform_anchor_split/
	done
	done
	done





30 July

	Gradient Based Inference

		python eval/run_gradient_based_search_w_cross_enc.py \
		--quant_method bienc \
		--lr 0.1 \
		--n_ment 20 \
		--bi_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
		--cross_model_ckpt ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=random_s=1234_63_negs_w_cls_w_lin/model/model-3-23399.0--98.18.ckpt   \
		--data_name lego \
		--res_dir ../../results/9_GradientInf/debug


		python eval/run_gradient_based_search_w_cross_enc.py \
		--quant_method viterbi \
		--lr 0.1 \
		--n_ment 2 \
		--bi_model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json \
		--cross_model_config ../../results/4_Zeshel/d\=ent_link/m\=cross_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_16_w_hard_bienc/model/best_wrt_dev/wrapper_config.json \
		--data_name lego \
		--res_dir ../../results/9_GradientInf/debug


		python eval/run_gradient_based_search_w_cross_enc.py \
		--quant_method concat \
		--lr 0.1 \
		--n_ment 2 \
		--bi_model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json \
		--cross_model_config ../../results/4_Zeshel/d\=ent_link/m\=cross_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_16_w_hard_bienc/model/best_wrt_dev/wrapper_config.json \
		--data_name lego \
		--res_dir ../../results/9_GradientInf/debug_concat_wo_exc_explored_nodes_during_quant_w_gt_node_init_w_20_steps

		python eval/run_gradient_based_search_w_cross_enc.py \
		--quant_method concat \
		--lr 0.1 \
		--n_ment 2 \
		--bi_model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json \
		--cross_model_config ../../results/4_Zeshel/d\=ent_link/m\=cross_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_16_w_hard_bienc/model/best_wrt_dev/wrapper_config.json \
		--data_name lego \
		--num_search_steps 100 \
		--res_dir ../../results/9_GradientInf/debug_concat_wo_exc_explored_nodes_during_quant_w_gt_node_init_w_100_steps


		python eval/run_gradient_based_search_w_cross_enc.py \
		--quant_method unigram_greedy \
		--lr 0.1 \
		--n_ment 2 \
		--bi_model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json \
		--cross_model_config ../../results/4_Zeshel/d\=ent_link/m\=cross_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_16_w_hard_bienc/model/best_wrt_dev/wrapper_config.json \
		--data_name lego \
		--res_dir ../../results/9_GradientInf/debug_unigram_greedy_wo_exc_explored_nodes_during_quant_w_0_node_init_w_20_steps


		python eval/run_gradient_based_search_w_cross_enc.py \
		--quant_method unigram_greedy \
		--lr 0.1 \
		--n_ment 10 \
		--bi_model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json \
		--cross_model_config ../../results/4_Zeshel/d\=ent_link/m\=cross_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_16_w_hard_bienc/model/best_wrt_dev/wrapper_config.json \
		--data_name lego \
		--res_dir ../../results/9_GradientInf/debug_unigram_greedy_wo_exc_explored_nodes_during_quant_w_0_node_init_w_20_steps_10_ments



		python eval/run_gradient_based_search_w_cross_enc.py \
		--quant_method unigram_greedy \
		--lr 0.1 \
		--n_ment 2 \
		--bi_model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json \
		--cross_model_config ../../results/4_Zeshel/d\=ent_link/m\=cross_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_16_w_hard_bienc/model/best_wrt_dev/wrapper_config.json \
		--data_name lego \
		--num_search_steps 100 \
		--res_dir ../../results/9_GradientInf/debug_unigram_greedy_wo_exc_explored_nodes_during_quant_w_0_node_init_w_100_steps





August 2


	Gradient Based inference with soft label inputs instead of having free parameters for label embeddings

		python eval/run_gradient_based_search_w_cross_enc.py \
		--quant_method concat \
		--lr 0.1 \
		--n_ment 1 \
		--bi_model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json \
		--cross_model_config ../../results/4_Zeshel/d\=ent_link/m\=cross_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_16_w_hard_bienc/model/best_wrt_dev/wrapper_config.json \
		--data_name lego \
		--num_search_steps 100 \
		--res_dir ../../results/9_GradientInf/debug_soft_token/concat_100_steps_lr_0.1


		python eval/run_gradient_based_search_w_cross_enc.py \
		--quant_method concat \
		--n_ment 1 \
		--bi_model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json \
		--cross_model_config ../../results/4_Zeshel/d\=ent_link/m\=cross_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_16_w_hard_bienc/model/best_wrt_dev/wrapper_config.json \
		--data_name lego \
		--lr 1000 \
		--num_search_steps 100 \
		--res_dir ../../results/9_GradientInf/debug_soft_token/concat_100_steps_lr_1000



		python eval/run_gradient_based_search_w_cross_enc.py \
		--quant_method unigram_greedy \
		--lr 0.1 \
		--n_ment 1 \
		--bi_model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json \
		--cross_model_config ../../results/4_Zeshel/d\=ent_link/m\=cross_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_16_w_hard_bienc/model/best_wrt_dev/wrapper_config.json \
		--data_name lego \
		--res_dir ../../results/9_GradientInf/debug_soft_token/unigram_greedy


		python eval/run_gradient_based_search_w_cross_enc.py \
		--quant_method label_greedy \
		--lr 0.1 \
		--n_ment 1 \
		--bi_model_config ../../results/4_Zeshel/d\=ent_link/m\=bi_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_32/model/best_wrt_dev_9/wrapper_config.json \
		--cross_model_config ../../results/4_Zeshel/d\=ent_link/m\=cross_enc_l\=ce_s\=1234_hard_negs_wo_dp_bs_16_w_hard_bienc/model/best_wrt_dev/wrapper_config.json \
		--data_name lego \
		--res_dir ../../results/9_GradientInf/debug_soft_label/label_greedy





		python eval/run_gradient_based_search_w_cross_enc.py \
		--config config/gbi/gbi_debug.json \
		--misc debug_refactoring


August 5
	
	Gradient Based search
		python eval/run_gradient_based_search_w_cross_enc.py \
		--config config/gbi/gbi_6_256_ece.json \
		--smooth_alpha 0.99 \
		--init_method gt \
		--lr 0.1 \
		--num_search_steps 100


		python eval/run_gradient_based_search_w_cross_enc.py \
		--config config/gbi/gbi_6_256_ece.json \
		--smooth_alpha 0.1 \
		--init_method gt \
		--lr 0.1 \
		--num_search_steps 100


		python eval/run_gradient_based_search_w_cross_enc.py \
		--config config/gbi/gbi_6_256_ece.json \
		--smooth_alpha 0.1 \
		--init_method gt \
		--param_type per_pos_weights \
		--quant_method unigram_greedy \
		--lr 0.1 \
		--num_search_steps 100



August 18
Hard negative training with CUR method

	Computing anchor mention-entity scores for 500 anchor mentions
		for data in ice_hockey muppets elder_scrolls coronation_street

		for data in military starwars doctor_who american_football world_of_warcraft fallout final_fantasy pro_wrestling
		for data in world_of_warcraft
		do
		for mstart in {200..250..50}
		do
		# sbatch -p gypsum-2080ti --gres gpu:1 --mem 32GB --job-name ce-mat-${data}-${mstart}  bin/run.sh  \
		sbatch -p gypsum-titanx-phd --gres gpu:1 --mem 32GB --job-name ce-mat-${data}-${mstart}  bin/run.sh  \
		python eval/run_cross_encoder_for_ment_ent_matrix_zeshel.py \
		--n_ment_start ${mstart} \
		--n_ment 50 \
		--n_ent -1 \
		--batch_size 300 \
		--data_name ${data} \
		--layers final \
		--cross_model_ckpt ../../results/6_ReprCrossEnc/d=ent_link/hard_neg_training/cls_ce/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
		--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/hard_neg_training/cls_ce/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt \
		--misc mstart_${mstart}
		done
		done

	Computing approx mention-entity scores using CUR with 200 anchor entities


			Debug
			python eval/run_cross_enc_hard_neg_mining_w_cur.py \
			--data_name pro_wrestling \
			--n_ment_start 0 \
			--n_ment 10 \
			--cur_k_retvr 100 \
			--num_anchor_ents 20 \
			--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/hard_neg_training/cls_ce/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
			--anchor_ment_to_ent_file ../../results/6_ReprCrossEnc/d=ent_link/hard_neg_training/cls_ce/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt/pro_wrestling/ment_to_ent_scores_n_m_50_n_e_10133_all_layers_Falsemstart_0.pkl \
			--res_dir ../../results/6_ReprCrossEnc/d=ent_link/hard_neg_training/cls_ce/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt \
			--misc debug_cur_retv_rerank \
			--disable_wandb 1


				
			# american_football
			sbatch -p gypsum-2080ti --gres gpu:1 --mem 32GB --job-name cur_ce_american_football bin/run.sh \
			python eval/run_cross_enc_hard_neg_mining_w_cur.py \
			--data_name american_football \
			--n_ment -1 \
			--cur_k_retvr 500 \
			--num_anchor_ents 200 \
			--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/hard_neg_training/cls_ce/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt \
			--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/hard_neg_training/cls_ce/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
			--anchor_ment_to_ent_file ../../results/6_ReprCrossEnc/d=ent_link/hard_neg_training/cls_ce/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt/american_football/ment_to_ent_scores_n_m_500_n_e_31929_all_layers_False.pkl 

			# fallout
			sbatch -p gypsum-2080ti --gres gpu:1 --mem 32GB --job-name cur_ce_fallout bin/run.sh \
			python eval/run_cross_enc_hard_neg_mining_w_cur.py \
			--data_name fallout \
			--n_ment -1 \
			--cur_k_retvr 500 \
			--num_anchor_ents 200 \
			--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/hard_neg_training/cls_ce/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt \
			--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/hard_neg_training/cls_ce/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
			--anchor_ment_to_ent_file ../../results/6_ReprCrossEnc/d=ent_link/hard_neg_training/cls_ce/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt/fallout/ment_to_ent_scores_n_m_500_n_e_16992_all_layers_False.pkl 

			# final_fantasy
			sbatch -p gypsum-2080ti --gres gpu:1 --mem 32GB --job-name cur_ce_final_fantasy bin/run.sh \
			python eval/run_cross_enc_hard_neg_mining_w_cur.py \
			--data_name final_fantasy \
			--n_ment -1 \
			--cur_k_retvr 500 \
			--num_anchor_ents 200 \
			--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/hard_neg_training/cls_ce/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt \
			--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/hard_neg_training/cls_ce/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
			--anchor_ment_to_ent_file ../../results/6_ReprCrossEnc/d=ent_link/hard_neg_training/cls_ce/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt/final_fantasy/ment_to_ent_scores_n_m_500_n_e_14044_all_layers_False.pkl 

			# pro_wrestling
			sbatch -p gypsum-2080ti --gres gpu:1 --mem 32GB --job-name cur_ce_pro_wrestling bin/run.sh \
			python eval/run_cross_enc_hard_neg_mining_w_cur.py \
			--data_name pro_wrestling \
			--n_ment -1 \
			--cur_k_retvr 500 \
			--num_anchor_ents 200 \
			--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/hard_neg_training/cls_ce/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt \
			--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/hard_neg_training/cls_ce/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
			--anchor_ment_to_ent_file ../../results/6_ReprCrossEnc/d=ent_link/hard_neg_training/cls_ce/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt/pro_wrestling/ment_to_ent_scores_n_m_500_n_e_10133_all_layers_False.pkl 

			# world_of_warcraft 
			sbatch -p gypsum-2080ti --gres gpu:1 --mem 32GB --job-name cur_ce_world_of_warcraft bin/run.sh \
			python eval/run_cross_enc_hard_neg_mining_w_cur.py \
			--data_name world_of_warcraft \
			--n_ment -1 \
			--cur_k_retvr 500 \
			--num_anchor_ents 200 \
			--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/hard_neg_training/cls_ce/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt \
			--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/hard_neg_training/cls_ce/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
			--anchor_ment_to_ent_file ../../results/6_ReprCrossEnc/d=ent_link/hard_neg_training/cls_ce/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt/world_of_warcraft/ment_to_ent_scores_n_m_500_n_e_27677_all_layers_False.pkl 

			# coronation_street 
			sbatch -p gypsum-2080ti --gres gpu:1 --mem 32GB --job-name cur_ce_coronation_street bin/run.sh \
			python eval/run_cross_enc_hard_neg_mining_w_cur.py \
			--data_name coronation_street \
			--n_ment -1 \
			--cur_k_retvr 500 \
			--num_anchor_ents 200 \
			--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/hard_neg_training/cls_ce/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt \
			--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/hard_neg_training/cls_ce/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
			--anchor_ment_to_ent_file ../../results/6_ReprCrossEnc/d=ent_link/hard_neg_training/cls_ce/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt/coronation_street/ment_to_ent_scores_n_m_500_n_e_17809_all_layers_False.pkl 

			# elder_scrolls 
			sbatch -p gypsum-2080ti --gres gpu:1 --mem 32GB --job-name cur_ce_elder_scrolls bin/run.sh \
			python eval/run_cross_enc_hard_neg_mining_w_cur.py \
			--data_name elder_scrolls \
			--n_ment -1 \
			--cur_k_retvr 500 \
			--num_anchor_ents 200 \
			--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/hard_neg_training/cls_ce/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt \
			--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/hard_neg_training/cls_ce/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
			--anchor_ment_to_ent_file ../../results/6_ReprCrossEnc/d=ent_link/hard_neg_training/cls_ce/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt/elder_scrolls/ment_to_ent_scores_n_m_500_n_e_21712_all_layers_False.pkl 

			# ice_hockey 
			sbatch -p gypsum-2080ti --gres gpu:1 --mem 32GB --job-name cur_ce_ice_hockey bin/run.sh \
			python eval/run_cross_enc_hard_neg_mining_w_cur.py \
			--data_name ice_hockey \
			--n_ment -1 \
			--cur_k_retvr 500 \
			--num_anchor_ents 200 \
			--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/hard_neg_training/cls_ce/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt \
			--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/hard_neg_training/cls_ce/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
			--anchor_ment_to_ent_file ../../results/6_ReprCrossEnc/d=ent_link/hard_neg_training/cls_ce/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt/ice_hockey/ment_to_ent_scores_n_m_500_n_e_28684_all_layers_False.pkl 

			# muppets
			sbatch -p gypsum-2080ti --gres gpu:1 --mem 32GB --job-name cur_ce_muppets bin/run.sh \
			python eval/run_cross_enc_hard_neg_mining_w_cur.py \
			--data_name muppets \
			--n_ment -1 \
			--cur_k_retvr 500 \
			--num_anchor_ents 200 \
			--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/hard_neg_training/cls_ce/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt \
			--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/hard_neg_training/cls_ce/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
			--anchor_ment_to_ent_file ../../results/6_ReprCrossEnc/d=ent_link/hard_neg_training/cls_ce/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt/muppets/ment_to_ent_scores_n_m_500_n_e_21344_all_layers_False.pkl 

			# For doctor_who
			for mstart in 0 3000 
			do
			sbatch -p gypsum-2080ti --gres gpu:1 --mem 32GB --job-name cur_ce_doctor_who-$mstart bin/run.sh \
			python eval/run_cross_enc_hard_neg_mining_w_cur.py \
			--data_name doctor_who \
			--n_ment_start $mstart \
			--n_ment 3000 \
			--cur_k_retvr 500 \
			--num_anchor_ents 200 \
			--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/hard_neg_training/cls_ce/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt \
			--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/hard_neg_training/cls_ce/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
			--anchor_ment_to_ent_file ../../results/6_ReprCrossEnc/d=ent_link/hard_neg_training/cls_ce/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt/doctor_who/ment_to_ent_scores_n_m_500_n_e_40281_all_layers_False.pkl \
			--misc mstart_${mstart}
			done

			sbatch -p gypsum-2080ti --gres gpu:1 --mem 32GB --job-name cur_ce_doctor_who-6K bin/run.sh \
			python eval/run_cross_enc_hard_neg_mining_w_cur.py \
			--data_name doctor_who \
			--n_ment_start 6000 \
			--n_ment 2334 \
			--cur_k_retvr 500 \
			--num_anchor_ents 200 \
			--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/hard_neg_training/cls_ce/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt \
			--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/hard_neg_training/cls_ce/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
			--anchor_ment_to_ent_file ../../results/6_ReprCrossEnc/d=ent_link/hard_neg_training/cls_ce/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt/doctor_who/ment_to_ent_scores_n_m_500_n_e_40281_all_layers_False.pkl \
			--misc mstart_6000

			# For military
			for mstart in 0 2000 4000 6000 8000 10000  
			do
			sbatch -p gypsum-2080ti --gres gpu:1 --mem 32GB --job-name cur_ce_military-$mstart bin/run.sh \
			python eval/run_cross_enc_hard_neg_mining_w_cur.py \
			--data_name military \
			--n_ment_start $mstart \
			--n_ment 2000 \
			--cur_k_retvr 500 \
			--num_anchor_ents 200 \
			--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/hard_neg_training/cls_ce/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt \
			--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/hard_neg_training/cls_ce/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
			--anchor_ment_to_ent_file ../../results/6_ReprCrossEnc/d=ent_link/hard_neg_training/cls_ce/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt/military/ment_to_ent_scores_n_m_500_n_e_104520_all_layers_False.pkl \
			--misc mstart_${mstart}
			done

			sbatch -p gypsum-2080ti --gres gpu:1 --mem 32GB --job-name cur_ce_military-12K bin/run.sh \
			python eval/run_cross_enc_hard_neg_mining_w_cur.py \
			--data_name military \
			--n_ment_start 12000 \
			--n_ment 1063 \
			--cur_k_retvr 500 \
			--num_anchor_ents 200 \
			--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/hard_neg_training/cls_ce/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt \
			--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/hard_neg_training/cls_ce/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
			--anchor_ment_to_ent_file ../../results/6_ReprCrossEnc/d=ent_link/hard_neg_training/cls_ce/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt/military/ment_to_ent_scores_n_m_500_n_e_104520_all_layers_False.pkl \
			--misc mstart_12000


			# For starwars
			for mstart in 0 2000 4000 6000 8000  
			do
			sbatch -p gypsum-rtx8000 --gres gpu:1 --mem 32GB --job-name cur_ce_starwars-$mstart bin/run.sh \
			python eval/run_cross_enc_hard_neg_mining_w_cur.py \
			--data_name starwars \
			--n_ment_start $mstart \
			--n_ment 2000 \
			--cur_k_retvr 500 \
			--num_anchor_ents 200 \
			--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/hard_neg_training/cls_ce/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt \
			--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/hard_neg_training/cls_ce/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
			--anchor_ment_to_ent_file ../../results/6_ReprCrossEnc/d=ent_link/hard_neg_training/cls_ce/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt/starwars/ment_to_ent_scores_n_m_500_n_e_87056_all_layers_False.pkl \
			--misc mstart_${mstart}
			done

			sbatch -p gypsum-rtx8000 --gres gpu:1 --mem 32GB --job-name cur_ce_starwars-10K bin/run.sh \
			python eval/run_cross_enc_hard_neg_mining_w_cur.py \
			--data_name starwars \
			--n_ment_start 10000 \
			--n_ment 1824 \
			--cur_k_retvr 500 \
			--num_anchor_ents 200 \
			--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/hard_neg_training/cls_ce/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt \
			--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/hard_neg_training/cls_ce/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
			--anchor_ment_to_ent_file ../../results/6_ReprCrossEnc/d=ent_link/hard_neg_training/cls_ce/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt/starwars/ment_to_ent_scores_n_m_500_n_e_87056_all_layers_False.pkl \
			--misc mstart_10000

	Computing cross-encoder scores for top-100 biencoder entities

		for data in ice_hockey muppets elder_scrolls coronation_street military starwars doctor_who american_football world_of_warcraft fallout final_fantasy pro_wrestling
		do
		sbatch -p gypsum-2080ti --gres gpu:1 --mem 32GB --job-name cross_w_bi_${data} bin/run.sh \
		python eval/run_cross_encoder_w_binenc_retriever_zeshel.py \
		--data_name ${data} \
		--n_ment_start 0 \
		--n_ment -1 \
		--top_k 100 \
		--batch_size 1 \
		--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
		--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/hard_neg_training/cls_ce/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval \
		--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/hard_neg_training/cls_ce/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
		--misc eoe-0-last.ckpt
		done

	Launching job for training w/ hard negs

		500 Negs from CUR (200 anchor ents, 500 anchor ments)
			sbatch -p gypsum-rtx8000 --gres gpu:2 --mem 64GB --job-name cur_hard_negs_epoch_1 bin/run.sh  \
			python models/train.py \
			--config config/el_zeshel_cross_enc.json \
			--exp_id 6_ReprCrossEnc \
			--loss ce \
			--pooling_type cls_w_lin \
			--neg_strategy precomp \
			--num_negs 63 \
			--train_batch_size 4 \
			--ent_w_score_file_template ../../results/6_ReprCrossEnc/d=ent_link/hard_neg_training/cls_ce/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/score_mats_eoe-0-last.ckpt/{}/crossenc_w_cur_retrvr_nm=-1_nm_start=0_k_retvr=500_.txt \
			--ckpt_path             	../../results/6_ReprCrossEnc/d=ent_link/hard_neg_training/cls_ce/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
			--reload_dataloaders_every_n_epochs 1 \
			--eval_batch_size 32 \
			--ckpt_metric mrr \
			--warmup_proportion 0.01 \
			--strategy ddp \
			--num_gpus 2 \
			--eval_interval 0.2 \
			--num_epochs 2 \
			--res_dir_prefix hard_neg_training/cls_ce/ \
			--misc w_cls_w_lin_6_387/63_of_500_cur_negs_nm_500_ne_200/epoch_1

		500 Negs from 500 CUR + 100 Bienc Negs
			sbatch -p gypsum-rtx8000 --gres gpu:2 --mem 128GB --exclude gypsum-gpu182,gypsum-gpu185,gypsum-gpu184  --job-name cur_n_bienc_100_hard_negs_epoch_1 bin/run.sh  \
			sbatch -p gypsum-rtx8000 --gres gpu:2 --mem 128GB  --job-name cur_n_bienc_100_hard_negs_epoch_1 bin/run.sh  \
			python models/train.py \
			--config config/el_zeshel_cross_enc.json \
			--exp_id 6_ReprCrossEnc \
			--loss ce \
			--pooling_type cls_w_lin \
			--neg_strategy precomp \
			--num_negs 63 \
			--train_batch_size 4 \
			--ent_w_score_file_template ../../results/6_ReprCrossEnc/d=ent_link/hard_neg_training/cls_ce/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval/{}/m=-1_k=500_CUR_500_negs_anc_nm_500_anc_ne_200_and_bienc_100_negs_comb_negs_eoe-0-last.ckpt.txt \
			--ckpt_path             	../../results/6_ReprCrossEnc/d=ent_link/hard_neg_training/cls_ce/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
			--reload_dataloaders_every_n_epochs 1 \
			--eval_batch_size 32 \
			--ckpt_metric mrr \
			--warmup_proportion 0.01 \
			--strategy ddp \
			--num_gpus 2 \
			--eval_interval 0.2 \
			--num_epochs 2 \
			--res_dir_prefix hard_neg_training/cls_ce/ \
			--misc w_cls_w_lin_6_387/63_of_500_cur_negs_nm_500_ne_200_plus_100_bienc_negs/epoch_1

		500 Negs from 500 CUR + 500 Negs
			sbatch -p gypsum-rtx8000 --gres gpu:2 --mem 128GB --exclude gypsum-gpu182,gypsum-gpu185,gypsum-gpu184  --job-name cur_n_bienc_500_hard_negs_epoch_1 bin/run.sh  \
			python models/train.py \
			--config config/el_zeshel_cross_enc.json \
			--exp_id 6_ReprCrossEnc \
			--loss ce \
			--pooling_type cls_w_lin \
			--neg_strategy precomp \
			--num_negs 63 \
			--train_batch_size 4 \
			--ent_w_score_file_template ../../results/6_ReprCrossEnc/d=ent_link/hard_neg_training/cls_ce/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/eval/{}/m=-1_k=500_CUR_500_negs_anc_nm_500_anc_ne_200_and_bienc_500_negs_comb_negs_eoe-0-last.ckpt.txt \
			--ckpt_path             	../../results/6_ReprCrossEnc/d=ent_link/hard_neg_training/cls_ce/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_cls_w_lin_for_hard_neg_training/model/eoe-0-last.ckpt \
			--reload_dataloaders_every_n_epochs 1 \
			--eval_batch_size 32 \
			--ckpt_metric mrr \
			--warmup_proportion 0.01 \
			--strategy ddp \
			--num_gpus 2 \
			--eval_interval 0.2 \
			--num_epochs 2 \
			--res_dir_prefix hard_neg_training/cls_ce/ \
			--misc w_cls_w_lin_6_387/63_of_500_cur_negs_nm_500_ne_200_plus_500_bienc_negs/epoch_1
	
	Evaluating trained model
			for data in yugioh star_trek lego forgotten_realms ice_hockey muppets elder_scrolls coronation_street
			do
			sbatch -p gypsum-2080ti --gres gpu:1 --mem 32GB --job-name cross_w_bi_${data} bin/run.sh \
			python eval/run_cross_encoder_w_binenc_retriever_zeshel.py \
			--data_name ${data} \
			--n_ment_start 0 \
			--n_ment -1 \
			--top_k 64 \
			--batch_size 1 \
			--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
			--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/hard_neg_training/cls_ce/m=cross_enc_l=ce_neg=precomp_s=1234_w_cls_w_lin_6_387/63_of_500_cur_negs_nm_500_ne_200/epoch_1/eval \
			--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/hard_neg_training/cls_ce/m=cross_enc_l=ce_neg=precomp_s=1234_w_cls_w_lin_6_387/63_of_500_cur_negs_nm_500_ne_200/epoch_1/model/eoe-1-last.ckpt \
			--misc eoe-1-last.ckpt
			done

			for data in yugioh star_trek lego forgotten_realms ice_hockey muppets elder_scrolls coronation_street
			do
			sbatch -p gypsum-2080ti --gres gpu:1 --mem 32GB --job-name cross_w_bi_${data} bin/run.sh \
			python eval/run_cross_encoder_w_binenc_retriever_zeshel.py \
			--data_name ${data} \
			--n_ment_start 0 \
			--n_ment -1 \
			--top_k 64 \
			--batch_size 1 \
			--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
			--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/hard_neg_training/cls_ce/m=cross_enc_l=ce_neg=precomp_s=1234_w_cls_w_lin_6_387/63_of_500_cur_negs_nm_500_ne_200_plus_500_bienc_negs/epoch_1/eval \
			--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/hard_neg_training/cls_ce/m=cross_enc_l=ce_neg=precomp_s=1234_w_cls_w_lin_6_387/63_of_500_cur_negs_nm_500_ne_200_plus_500_bienc_negs/epoch_1/model/eoe-1-last.ckpt \
			--misc eoe-1-last.ckpt
			done

			for data in yugioh star_trek lego forgotten_realms ice_hockey muppets elder_scrolls coronation_street
			do
			sbatch -p gypsum-2080ti --gres gpu:1 --mem 32GB --job-name cross_w_bi_${data} bin/run.sh \
			python eval/run_cross_encoder_w_binenc_retriever_zeshel.py \
			--data_name ${data} \
			--n_ment_start 0 \
			--n_ment -1 \
			--top_k 64 \
			--batch_size 1 \
			--bi_model_file    ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
			--res_dir          ../../results/6_ReprCrossEnc/d=ent_link/hard_neg_training/cls_ce/m=cross_enc_l=ce_neg=precomp_s=1234_w_cls_w_lin_6_387/63_of_500_cur_negs_nm_500_ne_200_plus_100_bienc_negs/epoch_1/eval \
			--cross_model_file ../../results/6_ReprCrossEnc/d=ent_link/hard_neg_training/cls_ce/m=cross_enc_l=ce_neg=precomp_s=1234_w_cls_w_lin_6_387/63_of_500_cur_negs_nm_500_ne_200_plus_100_bienc_negs/epoch_1/model/eoe-1-last.ckpt \
			--misc eoe-1-last.ckpt
			done



August 18-22
	Incremental CUR for cross-encoder scores

		for shortlist_method in none
		do
		for sampling_method in random_cumul approx_softmax_cumul exact_softmax_cumul approx_topk_cumul exact_topk_cumul exact_after_topk_cumul variance_cumul variance_topk_cumul
		do
		for i_cur_n_steps in 1 2 5 10 20 50 100
		do
		# sbatch -p cpu --gres gpu:1 --mem 32GB --job-name icur_${shortlist_method}_${sampling_method}_${i_cur_n_steps} --exclude gpu-0-[0-1] bin/run.sh \
		sbatch -p cpu --mem 32GB --job-name icur_${shortlist_method}_${sampling_method}_${i_cur_n_steps} --exclude gpu-0-[0-1] bin/run.sh \
		python eval/run_retrieval_eval_wrt_exact_crossenc.py \
		--data yugioh \
		--res_dir  ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt \
		--n_seeds 5 \
		--n_ment 100 \
		--disable_wandb 0 \
		--shortlist_method ${shortlist_method} \
		--sampling_method ${sampling_method} \
		--i_cur_n_steps ${i_cur_n_steps} \
		--misc i_cur_uniform_anchor_split_wo_shortlisting/
		done
		done
		done


			
		Run on 550 mentions with 500 anchor mentions
		for shortlist_method in none
		do
		for sampling_method in random_cumul approx_softmax_cumul exact_softmax_cumul approx_topk_cumul exact_topk_cumul exact_after_topk_cumul variance_cumul variance_topk_cumul
		do
		for i_cur_n_steps in 1 2 5 10 20 50 100
		do
		# sbatch -p cpu --gres gpu:1 --mem 32GB --job-name icur_${shortlist_method}_${sampling_method}_${i_cur_n_steps} --exclude gpu-0-[0-1] bin/run.sh \
		sbatch -p cpu --mem 32GB --job-name icur_${shortlist_method}_${sampling_method}_${i_cur_n_steps} --exclude gpu-0-[0-1] bin/run.sh \
		python eval/run_retrieval_eval_wrt_exact_crossenc.py \
		--data yugioh \
		--res_dir  ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt \
		--n_seeds 5 \
		--n_ment 3374 \
		--disable_wandb 0 \
		--shortlist_method ${shortlist_method} \
		--sampling_method ${sampling_method} \
		--i_cur_n_steps ${i_cur_n_steps} \
		--misc i_cur_uniform_anchor_split_wo_shortlisting_550_ments_anc_nm_500/
		done
		done
		done

		Run on 550 mentions with 500 anchor mentions
		for shortlist_method in none
		do
		for sampling_method in random_cumul approx_softmax_cumul exact_softmax_cumul approx_topk_cumul exact_topk_cumul exact_after_topk_cumul variance_cumul variance_topk_cumul
		do
		for i_cur_n_steps in 1 2 5 10 20 50 100
		do
		# sbatch -p cpu --gres gpu:1 --mem 32GB --job-name icur_${shortlist_method}_${sampling_method}_${i_cur_n_steps} --exclude gpu-0-[0-1] bin/run.sh \
		sbatch -p cpu --mem 32GB --job-name icur_${shortlist_method}_${sampling_method}_${i_cur_n_steps} --exclude gpu-0-[0-1] bin/run.sh \
		python eval/run_retrieval_eval_wrt_exact_crossenc.py \
		--data yugioh \
		--res_dir  ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt \
		--n_seeds 5 \
		--n_ment 3374 \
		--disable_wandb 0 \
		--shortlist_method ${shortlist_method} \
		--sampling_method ${sampling_method} \
		--i_cur_n_steps ${i_cur_n_steps} \
		--misc i_cur_uniform_anchor_split_wo_shortlisting_550_ments_anc_nm_500/
		done
		done
		done

	Debug
		python eval/run_retrieval_eval_wrt_exact_crossenc.py \
		--data yugioh \
		--res_dir  ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt \
		--n_seeds 5 \
		--n_ment 100 \
		--disable_wandb 0 \
		--shortlist_method none \
		--sampling_method approx_softmax_cumul \
		--i_cur_n_steps 10 \
		--misc debug_i_cur_uniform_anchor_split_wo_shortlisting/ \
		--disable_wandb  1

	Incremental CUR for bi-encoder scores
		# Need to package binecoder score matrix just like cross-encoder score matrix - 
		# at least need to pack mention tokens for re-using this code!
		Computing binencoder scores
		python eval/run_biencoder_for_ment_ent_matrix.py \
		--data_name yugioh \
		--n_ment 550 \
		--res_dir ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/score_mats_model-3-12039.0-2.17.ckpt \
		--bi_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt

		
		for shortlist_method in none
		do
		for sampling_method in random_cumul approx_softmax_cumul exact_softmax_cumul exact_topk_cumul exact_after_topk_cumul 
		do
		for i_cur_n_steps in 1 2 5 10
		do
		# sbatch -p cpu --gres gpu:1 --mem 32GB --job-name icur_${shortlist_method}_${sampling_method}_${i_cur_n_steps} --exclude gpu-0-[0-1] bin/run.sh \
		sbatch -p cpu --mem 32GB --job-name icur_${shortlist_method}_${sampling_method}_${i_cur_n_steps} --exclude gpu-0-[0-1] bin/run.sh \
		python eval/run_retrieval_eval_wrt_exact_crossenc.py \
		--data yugioh \
		--res_dir ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/score_mats_model-3-12039.0-2.17.ckpt \
		--n_seeds 5 \
		--n_ment 100 \
		--disable_wandb 0 \
		--shortlist_method ${shortlist_method} \
		--sampling_method ${sampling_method} \
		--i_cur_n_steps ${i_cur_n_steps} \
		--misc i_cur_uniform_anchor_split_wo_shortlisting_bienc/
		done
		done
		done

		for shortlist_method in none
		do
		for sampling_method in random_cumul approx_softmax_cumul exact_softmax_cumul exact_topk_cumul exact_after_topk_cumul 
		do
		for i_cur_n_steps in 1 2 5 10
		do
		# sbatch -p cpu --gres gpu:1 --mem 32GB --job-name icur_${shortlist_method}_${sampling_method}_${i_cur_n_steps} --exclude gpu-0-[0-1] bin/run.sh \
		sbatch -p cpu --mem 32GB --job-name icur_${shortlist_method}_${sampling_method}_${i_cur_n_steps} --exclude gpu-0-[0-1] bin/run.sh \
		python eval/run_retrieval_eval_wrt_exact_crossenc.py \
		--data yugioh \
		--res_dir ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/score_mats_model-3-12039.0-2.17.ckpt \
		--n_seeds 5 \
		--n_ment 550 \
		--disable_wandb 0 \
		--shortlist_method ${shortlist_method} \
		--sampling_method ${sampling_method} \
		--i_cur_n_steps ${i_cur_n_steps} \
		--misc i_cur_uniform_anchor_split_wo_shortlisting_bienc_anc_nm_500/
		done
		done
		done




August 24

	SVD for matrix factorization


		python eval/test_svd_for_matrix_factorization.py \
		--data_name yugioh \
		--res_dir  ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt \
		--n_ment 100 \
		--misc debug

		python eval/test_svd_for_matrix_factorization.py \
		--data_name yugioh \
		--res_dir  ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt \
		--n_ment 100 \
		--rank 99 \
		--n_sparse_per_row 10 \
		--misc debug

		python eval/test_svd_for_matrix_factorization.py \
		--data_name yugioh \
		--res_dir  ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt \
		--n_ment 100 \
		--misc debug


		python eval/test_svd_for_matrix_factorization.py \
		--data_name yugioh \
		--res_dir  ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt \
		--n_ment 3374 \
		--misc debug


		python eval/run_retrieval_eval_wrt_exact_crossenc.py \
		--data yugioh \
		--res_dir  ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt \
		--n_seeds 1 \
		--n_ment 100 \
		--disable_wandb 1 \
		--misc debug_svd/




		python eval/test_svd_for_matrix_factorization.py \
		--data_name yugioh \
		--res_dir  ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt \
		--n_ment 100 \
		--misc debug


August 27

Incremental CUR for cross-encoder scores

# Lego
for shortlist_method in none
do
for sampling_method in random_cumul approx_softmax_cumul exact_softmax_cumul approx_topk_cumul exact_topk_cumul exact_after_topk_cumul
do
for i_cur_n_steps in 1 2 5 10 
do
# sbatch -p cpu --gres gpu:1 --mem 32GB --job-name icur_${shortlist_method}_${sampling_method}_${i_cur_n_steps} --exclude gpu-0-[0-1] bin/run.sh \
sbatch -p cpu --mem 32GB --job-name icur_lego_${shortlist_method}_${sampling_method}_${i_cur_n_steps} --exclude gpu-0-[0-1] bin/run.sh \
python eval/run_retrieval_eval_wrt_exact_crossenc.py \
--data lego \
--res_dir  ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt \
--n_seeds 1 \
--n_ment 1199 \
--disable_wandb 0 \
--shortlist_method ${shortlist_method} \
--sampling_method ${sampling_method} \
--i_cur_n_steps ${i_cur_n_steps} \
--misc i_cur_uniform_anchor_split_wo_shortlisting/
done
done
done

# star_trek
for shortlist_method in none
do
for sampling_method in random_cumul approx_softmax_cumul exact_softmax_cumul approx_topk_cumul exact_topk_cumul
do
# for i_cur_n_steps in 1 2 5 10 
for i_cur_n_steps in 1 2 5 10 20 50
do
# sbatch -p cpu --gres gpu:1 --mem 32GB --job-name icur_${shortlist_method}_${sampling_method}_${i_cur_n_steps} --exclude gpu-0-[0-1] bin/run.sh \
sbatch -p cpu --mem 32GB --job-name icur_st_${shortlist_method}_${sampling_method}_${i_cur_n_steps} --exclude gpu-0-[0-1] bin/run.sh \
python eval/run_retrieval_eval_wrt_exact_crossenc.py \
--data star_trek \
--res_dir  ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt \
--n_seeds 1 \
--n_ment 2500 \
--disable_wandb 0 \
--shortlist_method ${shortlist_method} \
--sampling_method ${sampling_method} \
--i_cur_n_steps ${i_cur_n_steps} \
--misc i_cur_uniform_anchor_split_wo_shortlisting/
done
done
done



# Lego - Retrieve using bienc
sbatch -p gpu --gres gpu:1 --mem 32GB --job-name bienc_lego_1 --exclude gpu-0-[0-1] bin/run.sh \
python eval/run_retrieval_eval_wrt_exact_crossenc.py \
--data lego \
--res_dir  ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt \
--bi_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
--n_seeds 5 \
--n_ment 1199 \
--disable_wandb 0 \
--misc i_cur_uniform_anchor_split_wo_shortlisting/bienc

# star_trek
sbatch -p gpu --gres gpu:1 --mem 32GB --job-name bienc_st_1 --exclude gpu-0-[0-1] bin/run.sh \
python eval/run_retrieval_eval_wrt_exact_crossenc.py \
--data star_trek \
--res_dir  ../../results/6_ReprCrossEnc/d=ent_link/m=cross_enc_l=ce_neg=bienc_hard_negs_s=1234_63_negs_w_crossenc_w_embeds/score_mats_model-2-15999.0--79.46.ckpt \
--bi_model_file ../../results/6_ReprCrossEnc/d=ent_link/m=bi_enc_l=ce_neg=bienc_hard_negs_s=1234_63_hard_negs_4_epochs_wp_0.01_w_ddp/model/model-3-12039.0-2.17.ckpt  \
--n_seeds 5 \
--n_ment 2500 \
--disable_wandb 0 \
--misc i_cur_uniform_anchor_split_wo_shortlisting/bienc

